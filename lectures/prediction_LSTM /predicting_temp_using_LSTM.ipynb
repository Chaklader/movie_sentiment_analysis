{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Temperature Time Series with LSTM\n",
    "\n",
    "Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) that is commonly used for sequence modeling, particularly for processing time-series data. Unlike traditional RNNs, LSTMs have a memory cell that allows them to selectively remember or forget information over time, which makes them particularly useful for long-term dependencies.\n",
    "\n",
    "In this tutorial, we'll use Pytorch to build an LSTM model that can predict a time-series based on previous data. We'll use numpy and pandas to preprocess the data.\n",
    "\n",
    "#### Step 1: Import Libraries\n",
    "\n",
    "We'll start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Load Data\n",
    "For this tutorial, we'll use a sample dataset that contains temperature readings for a single sensor over time. We'll load the dataset into a pandas DataFrame and preprocess it so that it can be fed into our LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/21/25m9bdqj7pv3mhbdjmv2z1w40000gn/T/ipykernel_90151/1258845797.py:11: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df = df.resample('H').ffill()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1981-01-01 00:00:00</th>\n",
       "      <td>2.338143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-01 01:00:00</th>\n",
       "      <td>2.338143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-01 02:00:00</th>\n",
       "      <td>2.338143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-01 03:00:00</th>\n",
       "      <td>2.338143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-01 04:00:00</th>\n",
       "      <td>2.338143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-01 05:00:00</th>\n",
       "      <td>2.338143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-01 06:00:00</th>\n",
       "      <td>2.338143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-01 07:00:00</th>\n",
       "      <td>2.338143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-01 08:00:00</th>\n",
       "      <td>2.338143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-01 09:00:00</th>\n",
       "      <td>2.338143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Temp\n",
       "Date                         \n",
       "1981-01-01 00:00:00  2.338143\n",
       "1981-01-01 01:00:00  2.338143\n",
       "1981-01-01 02:00:00  2.338143\n",
       "1981-01-01 03:00:00  2.338143\n",
       "1981-01-01 04:00:00  2.338143\n",
       "1981-01-01 05:00:00  2.338143\n",
       "1981-01-01 06:00:00  2.338143\n",
       "1981-01-01 07:00:00  2.338143\n",
       "1981-01-01 08:00:00  2.338143\n",
       "1981-01-01 09:00:00  2.338143"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data into a pandas DataFrame\n",
    "df = pd.read_csv('data/temperature.csv')\n",
    "\n",
    "# Convert the 'datetime' column to a datetime object\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Set the 'datetime' column as the index\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Resample the data to hourly intervals and fill missing values with the previous value\n",
    "df = df.resample('H').ffill()\n",
    "\n",
    "# Normalize the data\n",
    "df = (df - df.mean()) / df.std()\n",
    "\n",
    "# Convert the DataFrame to a numpy array\n",
    "data = df.values\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Split Data\n",
    "\n",
    "Next, we'll split the data into training and testing sets. We'll use the first 70% of the data for training and the remaining 30% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_size = int(len(data) * 0.7)\n",
    "train_data, test_data = data[:train_size], data[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Create Data Sequences\n",
    "Before we can train our LSTM model, we need to create sequences of data that the model can learn from. We'll create sequences of length 24 (one day), and we'll use a sliding window approach to create overlapping sequences.\n",
    "\n",
    "\n",
    "# Understanding LSTMs and Sliding Window Technique for Time Series Analysis\n",
    "\n",
    "Time series analysis represents one of the most challenging domains in machine learning due to the temporal dependencies inherent in sequential data. The sliding window technique and Long Short-Term Memory (LSTM) networks form a powerful combination for addressing these challenges. Let me provide a comprehensive explanation of these concepts and their interrelationships.\n",
    "\n",
    "## The Sliding Window Technique\n",
    "\n",
    "The sliding window technique is a fundamental method for transforming sequential data into a format suitable for supervised learning. This approach is particularly valuable in time series forecasting where we aim to predict future values based on historical patterns.\n",
    "\n",
    "The function `create_sequences()` that you've shared implements this technique by:\n",
    "\n",
    "1. Taking a time series dataset and a sequence length parameter\n",
    "2. Creating pairs of input sequences (X) and target values (y)\n",
    "3. Sliding through the data one step at a time to generate these pairs\n",
    "\n",
    "To elaborate on the mechanics of the sliding window approach:\n",
    "\n",
    "For a time series with data points $[x_1, x_2, ..., x_n]$ and a chosen sequence length $k$, the sliding window creates:\n",
    "\n",
    "- Input sequence: $[x_i, x_{i+1}, ..., x_{i+k-1}]$\n",
    "- Target value: $x_{i+k}$\n",
    "\n",
    "Where $i$ ranges from 1 to $n-k$.\n",
    "\n",
    "The sequence length parameter (in your code, `seq_length = 24`) is crucial as it defines the temporal context the model will use for making predictions. This parameter determines how far back in time the model \"looks\" to predict the next value. Selecting an appropriate sequence length involves balancing:\n",
    "\n",
    "- Short windows: May miss longer-term patterns but train faster and require less data\n",
    "- Long windows: Can capture extended temporal dependencies but increase model complexity and computational requirements\n",
    "\n",
    "When working with time-based data like hourly temperature readings, a sequence length of 24 would correspond to using a full day's worth of readings to predict the next hour's temperature. This approach allows the model to learn daily cyclical patterns.\n",
    "\n",
    "## Long Short-Term Memory Networks\n",
    "\n",
    "LSTMs represent a specialized architecture of Recurrent Neural Networks (RNNs) designed to address the limitations of traditional RNNs, particularly the vanishing gradient problem that hampers learning of long-range dependencies.\n",
    "\n",
    "### The Vanishing Gradient Challenge\n",
    "\n",
    "Standard RNNs struggle with learning dependencies over long sequences because gradients tend to either vanish or explode during backpropagation through time. This limitation occurs because errors must propagate backward through many time steps, with multiplicative effects at each step that can cause gradients to become extremely small or large.\n",
    "\n",
    "### LSTM Architecture\n",
    "\n",
    "LSTMs address this challenge through a sophisticated cell structure with multiple gating mechanisms that regulate information flow. The LSTM cell architecture consists of:\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    Input[\"Input<br>x_t, h_t-1\"] --> Forget[\"Forget Gate<br>f_t = σ(W_f·[h_t-1,x_t]+b_f)\"]\n",
    "    Input --> Input_Gate[\"Input Gate<br>i_t = σ(W_i·[h_t-1,x_t]+b_i)\"]\n",
    "    Input --> Cell_State[\"Candidate Cell State<br>C̃_t = tanh(W_C·[h_t-1,x_t]+b_C)\"]\n",
    "    Input --> Output_Gate[\"Output Gate<br>o_t = σ(W_o·[h_t-1,x_t]+b_o)\"]\n",
    "    \n",
    "    Prev_Cell[\"Previous Cell State<br>C_t-1\"] --> Cell_Update[\"Cell State Update\"]\n",
    "    Forget --> Cell_Update\n",
    "    Input_Gate --> Cell_Update\n",
    "    Cell_State --> Cell_Update\n",
    "    \n",
    "    Cell_Update --> New_Cell[\"New Cell State<br>C_t = f_t * C_t-1 + i_t * C̃_t\"]\n",
    "    New_Cell --> Hidden_Calc[\"Hidden State Calculation\"]\n",
    "    Output_Gate --> Hidden_Calc\n",
    "    \n",
    "    Hidden_Calc --> New_Hidden[\"New Hidden State<br>h_t = o_t * tanh(C_t)\"]\n",
    "    \n",
    "    style Input fill:#BCFB89\n",
    "    style Forget fill:#FBF266\n",
    "    style Input_Gate fill:#FBF266\n",
    "    style Cell_State fill:#9AE4F5\n",
    "    style Output_Gate fill:#FBF266\n",
    "    style Prev_Cell fill:#9AE4F5\n",
    "    style Cell_Update fill:#FA756A\n",
    "    style New_Cell fill:#0096D9\n",
    "    style Hidden_Calc fill:#FA756A\n",
    "    style New_Hidden fill:#FCEB14\n",
    "```\n",
    "\n",
    "### The Three Gates\n",
    "\n",
    "1. **Forget Gate**: Determines what information from the previous cell state should be discarded\n",
    "   - $f_t = \\sigma(W_f \\cdot [h_t-1, x_t] + b_f)$\n",
    "\n",
    "2. **Input Gate**: Controls what new information will be stored in the cell state\n",
    "   - $i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$\n",
    "   - $\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$\n",
    "\n",
    "3. **Output Gate**: Filters the cell state to determine the next hidden state\n",
    "   - $o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$\n",
    "\n",
    "### Cell State and Hidden State\n",
    "\n",
    "The cell state ($C_t$) acts as the memory of the LSTM, allowing information to flow unchanged through the network or be selectively modified:\n",
    "\n",
    "$C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$\n",
    "\n",
    "The hidden state ($h_t$) represents the output for the current time step and input to the next time step:\n",
    "\n",
    "$h_t = o_t * \\tanh(C_t)$\n",
    "\n",
    "This architecture enables LSTMs to:\n",
    "- Retain important information over many time steps\n",
    "- Forget irrelevant information\n",
    "- Update the internal state with new inputs\n",
    "- Control what information is exposed as output\n",
    "\n",
    "# Explaining LSTM Gate Equations and Activation Functions\n",
    "\n",
    "Let me provide a detailed explanation of the LSTM gate equations, what each parameter represents, and the rationale behind the choice of activation functions.\n",
    "\n",
    "## LSTM Gate Equations with Parameter Explanations\n",
    "\n",
    "### Forget Gate\n",
    "\n",
    "$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$\n",
    "\n",
    "- $f_t$: Output of the forget gate at time step $t$ (a vector with values between 0 and 1)\n",
    "- $W_f$: Weight matrix for the forget gate (learned during training)\n",
    "- $h_{t-1}$: Hidden state from the previous time step (contains information from past sequence)\n",
    "- $x_t$: Input at the current time step\n",
    "- $[h_{t-1}, x_t]$: Concatenation of the previous hidden state and current input\n",
    "- $b_f$: Bias vector for the forget gate\n",
    "- $\\sigma$: Sigmoid activation function that outputs values between 0 and 1\n",
    "\n",
    "The forget gate determines which information from the previous cell state should be retained (values close to 1) or discarded (values close to 0). Each element in the output vector corresponds to a dimension in the cell state.\n",
    "\n",
    "### Input Gate\n",
    "\n",
    "$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$\n",
    "\n",
    "- $i_t$: Output of the input gate at time step $t$ (a vector with values between 0 and 1)\n",
    "- $W_i$: Weight matrix for the input gate\n",
    "- $b_i$: Bias vector for the input gate\n",
    "\n",
    "The input gate controls how much of the new candidate values should be added to the cell state.\n",
    "\n",
    "### Candidate Cell State\n",
    "\n",
    "$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$\n",
    "\n",
    "- $\\tilde{C}_t$: Candidate cell state at time step $t$ (a vector with values between -1 and 1)\n",
    "- $W_C$: Weight matrix for the candidate cell state\n",
    "- $b_C$: Bias vector for the candidate cell state\n",
    "- $\\tanh$: Hyperbolic tangent activation function that outputs values between -1 and 1\n",
    "\n",
    "The candidate cell state represents the new information that might be added to the cell state, regulated by the input gate.\n",
    "\n",
    "### Output Gate\n",
    "\n",
    "$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$\n",
    "\n",
    "- $o_t$: Output of the output gate at time step $t$ (a vector with values between 0 and 1)\n",
    "- $W_o$: Weight matrix for the output gate\n",
    "- $b_o$: Bias vector for the output gate\n",
    "\n",
    "The output gate determines what information from the cell state should be exposed to the next layer or time step.\n",
    "\n",
    "### Cell State Update\n",
    "\n",
    "$C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$\n",
    "\n",
    "- $C_t$: Cell state at time step $t$\n",
    "- $C_{t-1}$: Cell state from the previous time step\n",
    "- $*$: Element-wise multiplication (Hadamard product)\n",
    "\n",
    "The cell state update equation combines:\n",
    "1. What to keep from the previous cell state ($f_t * C_{t-1}$)\n",
    "2. What new information to add ($i_t * \\tilde{C}_t$)\n",
    "\n",
    "### Hidden State Update\n",
    "\n",
    "$h_t = o_t * \\tanh(C_t)$\n",
    "\n",
    "- $h_t$: Hidden state at time step $t$\n",
    "- $\\tanh(C_t)$: Cell state transformed to values between -1 and 1\n",
    "\n",
    "The hidden state is created by filtering the cell state through the output gate.\n",
    "\n",
    "## Why Choose tanh Activation Function?\n",
    "\n",
    "The choice of tanh for the candidate cell state ($\\tilde{C}_t$) and for transforming the cell state is deliberate and serves several important purposes:\n",
    "\n",
    "1. **Output Range**: The tanh function outputs values between -1 and 1, allowing the network to express both positive and negative influences. This is crucial for modeling complex patterns where some inputs may need to increase certain values while others decrease them.\n",
    "\n",
    "2. **Zero-Centered Output**: Unlike the sigmoid function which outputs values between 0 and 1 (with a mean of 0.5), tanh outputs are centered around zero. This property helps mitigate the vanishing gradient problem by allowing for both positive and negative gradients during backpropagation.\n",
    "\n",
    "3. **Steeper Gradient**: The tanh function has a steeper gradient compared to sigmoid, which means it can produce stronger signals and can learn more quickly in certain situations.\n",
    "\n",
    "4. **Normalization Effect**: Using tanh helps keep the cell state values normalized, preventing them from growing too large over many time steps. This contributes to numerical stability during training.\n",
    "\n",
    "5. **Complementary to Sigmoid Gates**: The combination of sigmoid for gates (determining what to keep/discard) and tanh for content (determining potential new values) creates a powerful mechanism for selective memory. Sigmoid outputs between 0-1 act as \"how much to keep,\" while tanh outputs between -1 and 1 represent \"what values to consider.\"\n",
    "\n",
    "When the tanh function is applied to the cell state in the hidden state calculation ($h_t = o_t * \\tanh(C_t)$), it serves to normalize the cell state values to the range [-1, 1] before being filtered by the output gate. This helps ensure that the hidden state remains within a consistent range, making it easier for subsequent layers to process.\n",
    "\n",
    "The sigmoid function, on the other hand, is used in all gates because they need to make binary decisions about what information to keep or discard. Values close to 0 mean \"discard,\" and values close to 1 mean \"keep,\" which aligns perfectly with the gate control mechanism.\n",
    "\n",
    "This carefully designed combination of activation functions allows LSTMs to effectively learn long-term dependencies in sequential data, addressing the limitations of traditional RNNs.\n",
    "\n",
    "## Application to Your LSTM Implementation\n",
    "\n",
    "In your code snippet:\n",
    "\n",
    "```python\n",
    "seq_length = 24  # Using 24 time steps\n",
    "X_train, y_train = create_sequences(train_data, seq_length)\n",
    "X_test, y_test = create_sequences(test_data, seq_length)\n",
    "```\n",
    "\n",
    "You're preparing data for an LSTM model by:\n",
    "\n",
    "1. Setting a sequence length of 24, which defines the temporal context\n",
    "2. Generating training sequences where each input is 24 consecutive time steps\n",
    "3. Setting each target as the value immediately following its input sequence\n",
    "\n",
    "After this preprocessing, your LSTM model will learn to recognize patterns within these 24-step windows to predict the next value. The LSTM's ability to selectively retain or forget information makes it particularly well-suited for this task, as it can:\n",
    "\n",
    "- Learn seasonal patterns (like daily temperature cycles)\n",
    "- Identify trends over multiple time steps\n",
    "- Remember important events from earlier in the sequence\n",
    "- Ignore irrelevant fluctuations\n",
    "\n",
    "When choosing parameters for your LSTM model, consider:\n",
    "\n",
    "1. **Number of LSTM units**: More units increase capacity but require more data and computational resources\n",
    "2. **Number of stacked LSTM layers**: Multiple layers can learn hierarchical temporal features\n",
    "3. **Dropout rate**: To prevent overfitting, especially with limited data\n",
    "4. **Learning rate**: Typically lower learning rates work better for complex time series\n",
    "\n",
    "## Advanced Considerations\n",
    "\n",
    "For sophisticated time series modeling, consider these extensions:\n",
    "\n",
    "1. **Bidirectional LSTMs**: Process sequences in both forward and backward directions to capture additional context\n",
    "2. **Attention mechanisms**: Allow the model to focus on different parts of the input sequence when making predictions\n",
    "3. **Stateful LSTMs**: Maintain state between batches for very long-range dependencies\n",
    "4. **Hybrid models**: Combine LSTMs with CNNs or traditional statistical methods\n",
    "\n",
    "The sliding window approach paired with LSTM networks provides a powerful framework for time series forecasting, capable of capturing complex temporal patterns while addressing the challenges inherent in sequential data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequences of data\n",
    "def create_sequences(data, seq_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences for training and testing data\n",
    "seq_length = 24\n",
    "X_train, y_train = create_sequences(train_data, seq_length)\n",
    "X_test, y_test = create_sequences(test_data, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create LSTM Model\n",
    "Now, we'll create our LSTM model using Pytorch. Our model will have one LSTM layer with 32 hidden units and one fully connected output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ``__init__`` method, we define an LSTM layer with hidden_size hidden units and a fully connected output layer with output_size output units. In the forward method, we pass the input `x` through the LSTM layer, take the output of the last time step, and pass it through the fully connected output layer.\n",
    "\n",
    "### Step 6: Instantiate Model and Define Loss Function and Optimizer\n",
    "Now, we'll instantiate our LSTM model, define our loss function (mean squared error), and define our optimizer (Adam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "input_size = X_train.shape[2]\n",
    "hidden_size = 32\n",
    "output_size = 1\n",
    "model = LSTM(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train the Model\n",
    "Next, we'll train our LSTM model on the training data. We'll use a batch size of 32 and train for 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Loss: 0.0007\n",
      "Epoch [2/25], Loss: 0.0005\n",
      "Epoch [3/25], Loss: 0.0008\n",
      "Epoch [4/25], Loss: 0.0001\n",
      "Epoch [5/25], Loss: 0.0000\n",
      "Epoch [6/25], Loss: 0.0002\n",
      "Epoch [7/25], Loss: 0.3496\n",
      "Epoch [8/25], Loss: 0.0000\n",
      "Epoch [9/25], Loss: 0.0001\n",
      "Epoch [10/25], Loss: 0.0000\n",
      "Epoch [11/25], Loss: 0.0009\n",
      "Epoch [12/25], Loss: 0.0008\n",
      "Epoch [13/25], Loss: 0.0003\n",
      "Epoch [14/25], Loss: 0.0006\n",
      "Epoch [15/25], Loss: 0.7127\n",
      "Epoch [16/25], Loss: 0.0000\n",
      "Epoch [17/25], Loss: 0.0003\n",
      "Epoch [18/25], Loss: 0.0000\n",
      "Epoch [19/25], Loss: 0.0000\n",
      "Epoch [20/25], Loss: 0.0000\n",
      "Epoch [21/25], Loss: 0.0002\n",
      "Epoch [22/25], Loss: 0.0000\n",
      "Epoch [23/25], Loss: 0.0001\n",
      "Epoch [24/25], Loss: 0.0007\n",
      "Epoch [25/25], Loss: 0.0004\n"
     ]
    }
   ],
   "source": [
    "# Convert numpy arrays to Pytorch tensors\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float()\n",
    "\n",
    "\n",
    "# Define the batch size and number of epochs\n",
    "batch_size = 32\n",
    "num_epochs = 25\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the training data\n",
    "    perm = torch.randperm(X_train.shape[0])\n",
    "    X_train = X_train[perm]\n",
    "    y_train = y_train[perm]\n",
    "\n",
    "    # Loop over batches\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        # Get batch\n",
    "        batch_X = X_train[i:i+batch_size]\n",
    "        batch_y = y_train[i:i+batch_size]\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss for this epoch\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Evaluate the Model\n",
    "Finally, we'll evaluate our LSTM model on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "\n",
    "# Calculate the test loss\n",
    "test_loss = criterion(y_pred, y_test)\n",
    "print('Test Loss: {:.4f}'.format(test_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert Pytorch tensors to numpy arrays\n",
    "y_test = y_test.numpy()\n",
    "y_pred = y_pred.numpy()\n",
    "\n",
    "# Plot predicted vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test[:500], label='Actual')\n",
    "plt.plot(y_pred[:500], label='Predicted')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Value')\n",
    "plt.title('LSTM Predictions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aipnd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
