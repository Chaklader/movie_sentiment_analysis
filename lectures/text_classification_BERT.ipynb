{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "#### Text classification using BERT\n",
    "\n",
    "In this notebook, we will utilize a pre-trained deep learning model to analyze some text. The model's output will be used to categorize the text, which is a collection of sentences extracted from movie reviews. Our goal is to determine whether each sentence conveys a positive or negative sentiment towards the subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### Objective\n",
    "\n",
    "Our objective is to develop a model that can analyze a given sentence and determine whether it expresses a positive sentiment, in which case it should produce a value of 1, or a negative sentiment.\n",
    "\n",
    "The model comprises two components: [DistilBERT](https://huggingface.co/transformers/model_doc/distilbert.html) and a basic [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model from scikit-learn.\n",
    "\n",
    "* DistilBERT processes the input sentence and passes on relevant information to the Logistic Regression model for sentiment classification. It is a lighter and faster version of BERT that performs comparably well.\n",
    "\n",
    "* The data shared between the two models is a vector of size 768. This is because DistilBERT represents each input sentence as a sequence of vectors, with each vector having a size of 768. This vector sequence is then fed to the Logistic Regression model for classification.\n",
    "\n",
    "#### Dataset - SST2\n",
    "\n",
    "The SST2 dataset is a widely-used benchmark dataset for sentiment analysis and text classification tasks. It consists of movie reviews from Rotten Tomatoes, with each review labeled as positive or negative. The dataset contains 11,855 training sentences and 2,210 testing sentences, each of which is parsed into a binary parse tree to capture its grammatical structure. The dataset has been used to evaluate the performance of various natural language processing models, including BERT and its variants. You can find the dataset [here](https://nlp.stanford.edu/sentiment/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  a stirring , funny and finally transporting re...  1\n",
       "1  apparently reassembled from the cutting room f...  0\n",
       "2  they presume their audience wo n't sit still f...  0\n",
       "3  this is a visually stunning rumination on love...  1\n",
       "4  jonathan parker 's bartleby should have been t...  1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv'\n",
    "df = pd.read_csv(url, delimiter='\\t', header=None, nrows=2500)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Load Pretrained model\n",
    "\n",
    "\n",
    "The code below demonstrates how to load a pre-trained DistilBERT model and tokenizer from the Transformers library by Hugging Face, which can be used for various natural language processing tasks.\n",
    "\n",
    "First, the `model_class`, `tokenizer_class`, and `pretrained_weights` variables are defined to hold the appropriate classes and weights required for the **DistilBERT** model.\n",
    "\n",
    "The `DistilBertTokenizer` class is used to tokenize raw text data and prepare it for input to the DistilBERT model. The `DistilBertModel` class is the implementation of the DistilBERT model itself. The `pretrained_weights` variable is set to `distilbert-base-uncased`, which indicates the specific pre-trained DistilBERT model to be used.\n",
    "\n",
    "Next, the `tokenizer` variable is initialized using the `from_pretrained()` method, which loads the pre-trained tokenizer for the specified DistilBERT model. This allows the raw text data to be tokenized and encoded in a way that can be understood by the model.\n",
    "\n",
    "Finally, the model variable is initialized using the `from_pretrained()` method, which loads the pre-trained DistilBERT model with the specified weights. This allows the model to be used for various NLP tasks, such as sentiment analysis or text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose:\n",
    "    Initializes pre-trained DistilBERT model and tokenizer components for text classification\n",
    "    tasks. Sets up the foundation for extracting contextualized embeddings from text input\n",
    "    using the lightweight DistilBERT architecture for downstream sentiment analysis.\n",
    "\n",
    "Parameters:\n",
    "    None - Uses predefined DistilBERT model configuration and weights\n",
    "\n",
    "Process Flow:\n",
    "    1. Define model class (DistilBertModel), tokenizer class (DistilBertTokenizer), and model name\n",
    "    2. Load pre-trained tokenizer from HuggingFace Hub using specified weights\n",
    "    3. Load pre-trained DistilBERT model with 'distilbert-base-uncased' configuration\n",
    "    4. Store initialized components for text processing and feature extraction pipeline\n",
    "\n",
    "Outputs:\n",
    "    tokenizer: DistilBertTokenizer instance for text preprocessing and encoding\n",
    "    model: DistilBertModel instance for generating contextualized embeddings (768-dim vectors)\n",
    "    Both components ready for inference-mode feature extraction\n",
    "\n",
    "Example:\n",
    "    >>> tokenizer = tokenizer_class.from_pretrained('distilbert-base-uncased')\n",
    "    >>> model = model_class.from_pretrained('distilbert-base-uncased')\n",
    "    >>> tokens = tokenizer.encode(\"This movie is great!\", add_special_tokens=True)\n",
    "    >>> embeddings = model(torch.tensor([tokens]))\n",
    "\"\"\"\n",
    "\n",
    "# For DistilBERT:\n",
    "model_class, tokenizer_class, pretrained_weights = (transformers.DistilBertModel,\n",
    "                                                    transformers.DistilBertTokenizer,\n",
    "                                                    'distilbert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "The code below tokenizes a column of reviews in a Pandas DataFrame using the pre-trained tokenizer from the DistilBERT model, which was previously loaded. The resulting tokenized reviews are stored in a new Pandas Series called `tokenized`.\n",
    "\n",
    "First, the `tokenizer.encode()` method is used to encode each review in the DataFrame. The `encode()` method converts the text into a sequence of integers that can be fed into the `DistilBERT` model. The `add_special_tokens=True` argument is passed to add special tokens like **[CLS]** (beginning of sequence) and **[SEP]** (end of sequence) to the beginning and end of each encoded review, respectively.\n",
    "\n",
    "The `apply()` method is used to apply the `tokenizer.encode()` function to each row in the DataFrame column containing the reviews. The resulting tokenized reviews are stored in a new Pandas Series called tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize all the reviews in column 0 of the dataframe \"df\"\n",
    "tokenized = df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualized_sentence_embedding(df: pd.DataFrame, tokenized: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        Creates a visualization DataFrame showing the mapping between original text tokens\n",
    "        and their corresponding numerical embeddings from DistilBERT tokenization. Helps\n",
    "        understand how text is converted to token IDs for model input.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Original DataFrame containing text reviews in column 0\n",
    "        tokenized (pd.Series): Series of tokenized sequences with numerical token IDs\n",
    "\n",
    "    Process Flow:\n",
    "        1. Extract first review text and split into individual word tokens\n",
    "        2. Add special tokens [CLS] at beginning and [SEP] at end to match BERT format\n",
    "        3. Validate token count matches tokenized sequence length\n",
    "        4. Create token-to-embedding pairs using zip operation\n",
    "        5. Convert pairs to DataFrame with descriptive column names\n",
    "\n",
    "    Outputs:\n",
    "        pd.DataFrame: Two-column DataFrame with 'Tokens' (text) and 'Embeddings' (token IDs)\n",
    "                     Shows direct mapping for debugging and educational visualization\n",
    "\n",
    "    Example:\n",
    "        >>> df_viz = visualized_sentence_embedding(df, tokenized)\n",
    "        >>> print(df_viz.head(3))\n",
    "           Tokens  Embeddings\n",
    "        0     CLS         101\n",
    "        1       a        1037\n",
    "        2  stirring     18385\n",
    "    \"\"\"\n",
    "    tokens = df.iloc[0,0].split(\" \")\n",
    "    tokens.insert(0, \"CLS\")\n",
    "    tokens.append(\"SEP\")\n",
    "    assert len(tokens) == len(tokenized[0])\n",
    "\n",
    "    token_embeddings = list(zip(tokens, tokenized[0]))\n",
    "    df_token_embeddings = pd.DataFrame(token_embeddings, columns=[\"Tokens\", \"Embeddings\"])\n",
    "    \n",
    "    return df_token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CLS</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>1037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stirring</td>\n",
       "      <td>18385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,</td>\n",
       "      <td>1010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>funny</td>\n",
       "      <td>6057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>finally</td>\n",
       "      <td>2633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>transporting</td>\n",
       "      <td>18276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>re</td>\n",
       "      <td>2128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>imagining</td>\n",
       "      <td>16603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tokens  Embeddings\n",
       "0           CLS         101\n",
       "1             a        1037\n",
       "2      stirring       18385\n",
       "3             ,        1010\n",
       "4         funny        6057\n",
       "5           and        1998\n",
       "6       finally        2633\n",
       "7  transporting       18276\n",
       "8            re        2128\n",
       "9     imagining       16603"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_token_embeddings = visualized_sentence_embedding(df, tokenized)\n",
    "df_token_embeddings.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "##### Padding\n",
    "\n",
    "Once the reviews in a DataFrame are tokenized, they are stored as a list of sentences (`tokenized`; data type =`pd.Series`), where each sentence is represented as a list of tokens. In order to process these examples in one batch using BERT, it is necessary to pad all of the lists to the same length. This allows the input to be represented as a single 2-dimensional array, rather than a list of variable-length lists. By doing this, the processing time can be greatly reduced.\n",
    "\n",
    "The code below performs the following steps:\n",
    "\n",
    "1. Initializes `max_len` to zero.\n",
    "2. Computes the maximum length of the tokenized reviews using a list comprehension that iterates over the tokenized reviews, returns their lengths. The resulting maximum length is assigned to the `max_len` variable.\n",
    "3. Pads the tokenized reviews with zeros to make them all the same length as the maximum length `max_len`. This is done using a list comprehension that iterates over the tokenized reviews, appends 0 to the end of each review until it has the same length as `max_len`, and converts the resulting list of padded reviews to a NumPy array. The resulting padded token embeddings are assigned to the `padded_token_embeddings` variable.\n",
    "\n",
    "4. Overall, this code computes the maximum length of the tokenized reviews and pads them with zeros to make them all the same length, which is necessary for feeding them into a deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 65)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Purpose:\n",
    "    Implements sequence padding to standardize tokenized text lengths for batch processing\n",
    "    in neural networks. Ensures all tokenized sequences have uniform dimensions by padding\n",
    "    shorter sequences with zeros to match the maximum sequence length.\n",
    "\n",
    "Parameters:\n",
    "    tokenized.values: Collection of variable-length tokenized sequences (token ID lists)\n",
    "\n",
    "Process Flow:\n",
    "    1. Initialize max_len counter to zero\n",
    "    2. Compute maximum sequence length across all tokenized reviews using list comprehension\n",
    "    3. Pad each sequence with zeros to reach max_len using right-padding strategy\n",
    "    4. Convert padded sequences to NumPy array for efficient tensor operations\n",
    "    5. Display final array shape for verification (samples, max_sequence_length)\n",
    "\n",
    "Outputs:\n",
    "    padded_token_embeddings (np.ndarray): 2D array of shape (n_samples, max_len)\n",
    "                                         Zero-padded sequences ready for model input\n",
    "    Printed shape: Tuple showing (number_of_samples, padded_sequence_length)\n",
    "\n",
    "Example:\n",
    "    >>> # Input: [[101, 1037], [101, 1037, 2633, 102]]  # Variable lengths\n",
    "    >>> # Output: [[101, 1037, 0, 0], [101, 1037, 2633, 102]]  # Uniform length\n",
    "    >>> print(padded_token_embeddings.shape)\n",
    "    (2500, 65)\n",
    "\"\"\"\n",
    "\n",
    "max_len = 0\n",
    "max_len = max([len(i) for i in tokenized.values if len(i) > max_len])\n",
    "padded_token_embeddings = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "\n",
    "print(padded_token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Masking\n",
    "\n",
    "In order to avoid confusing BERT with the padding added to the tokenized reviews, we need to create a separate variable called attention_mask. This variable indicates which tokens should be attended to by the model and which tokens should be ignored (masked) during processing. By setting the attention mask to 1 for the real tokens and 0 for the padding tokens, we can tell BERT to ignore the padding when processing the input. This helps to improve the accuracy of the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 65)\n"
     ]
    }
   ],
   "source": [
    "attention_mask = np.where(padded_token_embeddings != 0, 1, 0)\n",
    "assert attention_mask.shape == padded_token_embeddings.shape\n",
    "\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "##### Model inputs\n",
    "\n",
    "We're now ready to train a deep learning model using PyTorch. We will be using the pre-trained **DistilBERT** model that we previously loaded. First, we need to prepare our inputs for the model. We take our tokenized and padded sentences and convert them into PyTorch tensors using the `torch.tensor()` function.\n",
    "\n",
    "we can pass the `input_ids` (torch tensor) and `attention_mask` tensors to the DistilBERT model using the `model()` function. The output of the function, `last_hidden_states`, will contain the contextualized embeddings for each token in our input sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/21/25m9bdqj7pv3mhbdjmv2z1w40000gn/T/ipykernel_35882/1759468296.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.tensor(attention_mask)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Purpose:\n",
    "    Performs forward pass through DistilBERT model to extract contextualized embeddings\n",
    "    from padded token sequences. Uses inference mode to generate feature representations\n",
    "    without gradient computation for efficient feature extraction pipeline.\n",
    "\n",
    "Parameters:\n",
    "    padded_token_embeddings: NumPy array of zero-padded token ID sequences\n",
    "    attention_mask: Binary mask indicating real tokens (1) vs padding tokens (0)\n",
    "\n",
    "Process Flow:\n",
    "    1. Convert padded token embeddings to PyTorch tensor format (input_ids)\n",
    "    2. Convert attention mask to PyTorch tensor for proper masking\n",
    "    3. Disable gradient computation using torch.no_grad() context for inference\n",
    "    4. Pass input_ids and attention_mask through DistilBERT model\n",
    "    5. Extract last_hidden_states containing contextualized embeddings for all tokens\n",
    "\n",
    "Outputs:\n",
    "    last_hidden_states: Tuple containing tensor of shape (batch_size, seq_len, hidden_size)\n",
    "                       Contains 768-dimensional embeddings for each token position\n",
    "                       Ready for [CLS] token extraction and downstream classification\n",
    "\n",
    "Example:\n",
    "    >>> input_ids.shape  # (2500, 65)\n",
    "    >>> attention_mask.shape  # (2500, 65)\n",
    "    >>> last_hidden_states[0].shape  # (2500, 65, 768)\n",
    "    >>> cls_embeddings = last_hidden_states[0][:,0,:]  # Extract [CLS] tokens\n",
    "\"\"\"\n",
    "\n",
    "input_ids = torch.tensor(padded_token_embeddings)\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Explanation for feature extraction from `last_hidden_states`:**\n",
    "\n",
    "Suppose we have a batch of 2500 input sentences, where each sentence is tokenized and padded to a length of 65. So, the shape of our padded array would be (2500, 65).\n",
    "\n",
    "Now, we pass this padded array to BERT using the `model()` function, and it returns a tensor `last_hidden_states` of shape (2500, 65, 768). Here, 2500 is the batch size, 65 is the length of the padded sentence, and 768 is the size of the BERT embedding for each token.\n",
    "\n",
    "To get a fixed-length representation of each sentence, we take the first token of each sentence, which is the `[CLS]` token. So, we extract the embeddings corresponding to the `[CLS]` token, which is located at index 0 in the second dimension of last_hidden_states.\n",
    "\n",
    "To get these embeddings for each sentence in the batch, we use the slicing operation `[:,0,:]`. This selects all elements along the first dimension (which corresponds to the batch size), the first element along the second dimension (which corresponds to the `[CLS]` token), and all elements along the third dimension (which corresponds to the embedding size). This returns a tensor of shape (2500, 768), where each row corresponds to the embedding of a single sentence.\n",
    "\n",
    "Finally, we convert this tensor to a numpy array using `.numpy()`, which gives us a 2D numpy array features of shape (2500, 768), where each row represents the fixed-length representation of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose:\n",
    "    Extracts fixed-length sentence representations from DistilBERT's contextualized embeddings\n",
    "    by selecting the [CLS] token embeddings. Converts PyTorch tensors to NumPy arrays for\n",
    "    compatibility with scikit-learn classifiers in the hybrid ML pipeline.\n",
    "\n",
    "Parameters:\n",
    "    last_hidden_states[0]: PyTorch tensor of shape (batch_size, seq_len, hidden_size)\n",
    "                          Contains contextualized embeddings for all token positions\n",
    "\n",
    "Process Flow:\n",
    "    1. Access first element of last_hidden_states tuple (main embedding tensor)\n",
    "    2. Select [CLS] token embeddings using [:,0,:] slicing operation\n",
    "    3. Extract all samples (:), first token position (0), all embedding dimensions (:)\n",
    "    4. Convert PyTorch tensor to NumPy array using .numpy() method\n",
    "    5. Result: Fixed-length feature vectors ready for classical ML algorithms\n",
    "\n",
    "Outputs:\n",
    "    features (np.ndarray): 2D array of shape (batch_size, 768) containing sentence embeddings\n",
    "                          Each row represents one sentence's [CLS] token embedding\n",
    "                          Ready for logistic regression or other scikit-learn classifiers\n",
    "\n",
    "\n",
    "    Additional Technical Detail - Tensor Slicing Breakdown\n",
    "    ––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
    "\n",
    "    The [:,0,:] operation performs 3-dimensional array slicing on last_hidden_states[0]:\n",
    "\n",
    "    Tensor Shape Analysis:\n",
    "        last_hidden_states[0] = (2500, 65, 768)\n",
    "        - Axis 0: 2500 sentences in batch\n",
    "        - Axis 1: 65 token positions (padded sequence length)  \n",
    "        - Axis 2: 768 embedding dimensions per token\n",
    "\n",
    "    Slicing Operation Breakdown:\n",
    "        [:, 0, :] means:\n",
    "        - First ':' → Select ALL rows (all 2500 sentences)\n",
    "        - '0' → Select ONLY column 0 (first token position = [CLS] token)\n",
    "        - Last ':' → Select ALL depths (all 768 embedding dimensions)\n",
    "\n",
    "    Mathematical Transformation:\n",
    "        Input:  (2500, 65, 768) → 3D tensor with all token embeddings\n",
    "        Slice:  [:,0,:]         → Extract only [CLS] token from each sentence\n",
    "        Output: (2500, 768)     → 2D matrix with sentence representations\n",
    "\n",
    "    Why [CLS] Token (Position 0)?\n",
    "        - BERT adds [CLS] at beginning of every sequence during tokenization\n",
    "        - [CLS] is trained to aggregate entire sentence meaning\n",
    "        - Perfect for sentence-level classification tasks\n",
    "        - Contains contextualized information from all other tokens via self-attention\n",
    "\n",
    "    Result: Each of 2500 sentences becomes a single 768-dimensional vector\n",
    "\n",
    "Example:\n",
    "    >>> last_hidden_states[0].shape  # (2500, 65, 768)\n",
    "    >>> features.shape  # (2500, 768)\n",
    "    >>> # Each row is a 768-dim sentence representation from [CLS] token\n",
    "\"\"\"\n",
    "\n",
    "# extracting features and labels\n",
    "features = last_hidden_states[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose:\n",
    "    Extracts the ground truth sentiment labels from the DataFrame for training and evaluating\n",
    "    the classification model. These labels represent the target variable (positive or negative\n",
    "    sentiment) that the model aims to predict based on DistilBERT features.\n",
    "\n",
    "Parameters:\n",
    "    df: pandas DataFrame containing the SST2 dataset\n",
    "        - Column 0: Text reviews (sentences)\n",
    "        - Column 1: Binary sentiment labels (target variable)\n",
    "\n",
    "Process Flow:\n",
    "    1. Access column 1 of the DataFrame (df[1]) which contains the sentiment labels\n",
    "    2. Store these values in the 'labels' variable as a pandas Series\n",
    "    3. Labels are binary: 1 for positive sentiment, 0 for negative sentiment\n",
    "    4. Used as the target variable for logistic regression training and evaluation\n",
    "\n",
    "Outputs:\n",
    "    labels (pd.Series): Series of binary integers (0 or 1) representing sentiment\n",
    "                       - 0: Negative sentiment review\n",
    "                       - 1: Positive sentiment review\n",
    "                       Matches length of features array for supervised learning\n",
    "\n",
    "Example:\n",
    "    >>> labels.head()\n",
    "    0    1    # Positive sentiment for first review\n",
    "    1    0    # Negative sentiment for second review\n",
    "    2    0\n",
    "    3    1\n",
    "    4    1\n",
    "    >>> len(labels)  # 2500 (matches number of samples)\n",
    "\"\"\"\n",
    "labels = df[1]\n",
    "\n",
    "assert len(features) == len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-5 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-5 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-5 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-5 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-5 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-5 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-5 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-5 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-5 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=5, max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=5, max_iter=1000)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=5, max_iter=1000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Purpose:\n",
    "    Initializes and trains a Logistic Regression classifier using scikit-learn on top of\n",
    "    DistilBERT-extracted features. Serves as the final classification layer in the hybrid\n",
    "    deep learning + classical ML pipeline for binary sentiment prediction.\n",
    "\n",
    "Parameters:\n",
    "    C (float): Inverse of regularization strength, set to 5 for moderate regularization\n",
    "              Smaller values increase regularization to prevent overfitting\n",
    "    max_iter (int): Maximum number of iterations for solver convergence, set to 1000\n",
    "                   Ensures model training completes even with complex data\n",
    "    train_features (np.ndarray): Training data of shape (n_samples, 768)\n",
    "                                DistilBERT [CLS] token embeddings as features\n",
    "    train_labels (pd.Series): Training target values, binary labels (0=negative, 1=positive)\n",
    "\n",
    "Process Flow:\n",
    "    1. Create LogisticRegression instance with specified hyperparameters\n",
    "    2. Fit the model to training data using .fit() method\n",
    "    3. Optimize model weights to minimize binary cross-entropy loss\n",
    "    4. Use L2 regularization (default) with strength controlled by C parameter\n",
    "    5. Iterate up to max_iter times or until convergence criteria met\n",
    "\n",
    "Outputs:\n",
    "    lr_clf: Trained LogisticRegression model instance\n",
    "           Ready for prediction on test set and performance evaluation\n",
    "           Capable of classifying new DistilBERT features into binary sentiment\n",
    "\n",
    "Example:\n",
    "    >>> lr_clf = LogisticRegression(C=5, max_iter=1000)\n",
    "    >>> lr_clf.fit(train_features, train_labels)\n",
    "    >>> test_accuracy = lr_clf.score(test_features, test_labels)\n",
    "    >>> print(f\"Test accuracy: {test_accuracy:.3f}\")  # e.g., Test accuracy: 0.821\n",
    "\"\"\"\n",
    "\n",
    "lr_clf = LogisticRegression(C=5, max_iter=1000)\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8208"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see how our trained LR model performs on the test set\n",
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Further improvements\n",
    "- Fine tune DistilBERT\n",
    "- Use GridSearchCV for getting best hyperparameters for the LogisticRegression model.\n",
    "- Try other classifiers, build a NN for classification, or used another pretrained neural network for classification.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
