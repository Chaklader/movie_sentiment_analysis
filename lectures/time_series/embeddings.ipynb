{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T23:56:29.718237Z",
     "start_time": "2024-10-26T23:56:28.167475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/ml/bin/python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/chaklader/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/chaklader/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chaklader/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/chaklader/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/chaklader/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)  # Should show path to rnn environment\n",
    "\n",
    "import torch\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T23:51:52.090521Z",
     "start_time": "2024-10-26T23:51:52.088193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = sample_text.lower().split()\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "##### Word Embeddings Explained\n",
    "\n",
    "Word embeddings like GloVe are dense vector representations of words where:\n",
    "\n",
    "- Each word is mapped to a fixed-length vector of real numbers\n",
    "- The vectors capture semantic relationships between words\n",
    "- Words with similar meanings have vectors that are close in the vector space\n",
    "- The vector dimensions implicitly represent different semantic aspects of words\n",
    "\n",
    "GloVe (Global Vectors for Word Representation) specifically is trained to capture global word-word co-occurrence statistics from a corpus. The resulting embeddings have interesting properties:\n",
    "\n",
    "- Words that appear in similar contexts have similar embeddings\n",
    "- Vector arithmetic works meaningfully: e.g., vector(\"king\") - vector(\"man\") + vector(\"woman\") ≈ vector(\"queen\")\n",
    "- The distance between word vectors correlates with semantic similarity\n",
    "\n",
    "The file naming convention `glove.6B.50d.txt` indicates:\n",
    "- `6B`: Trained on 6 billion tokens\n",
    "- `50d`: Each word is represented by a 50-dimensional vector\n",
    "\n",
    "These pre-trained embeddings allow you to convert text data into numerical representations that machine learning models can process while preserving semantic relationships between words.\n",
    "\n",
    "##### How These Components Work Together\n",
    "\n",
    "In a typical NLP pipeline:\n",
    "\n",
    "1. The `preprocess_text` function would clean and tokenize raw text\n",
    "2. The tokens would be converted to embeddings using the loaded embedding dictionary\n",
    "3. These embeddings would then be fed into a machine learning model\n",
    "\n",
    "For example, after preprocessing a sentence, you might average the embeddings of all its words to get a sentence representation, or you might create sequences of embeddings to feed into an LSTM or other neural network.\n",
    "\n",
    "This approach is fundamental to many NLP tasks like sentiment analysis, text classification, and question answering.\n",
    "\n",
    "----\n",
    "\n",
    "# Understanding Word Embeddings: From Text to Mathematical Representations\n",
    "\n",
    "## The Fundamental Concept\n",
    "\n",
    "Word embeddings transform the discrete, symbolic nature of human language into continuous mathematical vectors that computers can process effectively. Rather than treating words as arbitrary symbols, embeddings capture semantic relationships through dense numerical representations where words with similar meanings occupy nearby positions in high-dimensional space.\n",
    "\n",
    "Consider how humans understand word relationships: we intuitively know that \"dog\" and \"puppy\" are more related than \"dog\" and \"computer.\" Word embeddings encode these relationships mathematically, enabling machines to perform similar semantic reasoning.\n",
    "\n",
    "## Dense vs. Sparse Representations\n",
    "\n",
    "Traditional text processing often uses sparse representations like one-hot encoding, where each word is represented by a vector with exactly one element set to 1 and all others set to 0. For a vocabulary of 10,000 words, each word becomes a 10,000-dimensional vector with 9,999 zeros and one 1.\n",
    "\n",
    "Word embeddings replace this inefficient representation with dense vectors containing all non-zero values. A 50-dimensional embedding captures far more semantic information than a 10,000-dimensional one-hot vector while using significantly less memory and computational resources.\n",
    "\n",
    "**Comparison Example**:\n",
    "- One-hot representation of \"king\": [0, 0, 0, 1, 0, 0, 0, ...] (9,999 zeros, one 1)\n",
    "- Dense embedding of \"king\": [0.2, -0.1, 0.8, 0.3, -0.4, 0.7, 0.1, -0.5, ...]\n",
    "\n",
    "## The GloVe Algorithm and Training Process\n",
    "\n",
    "GloVe (Global Vectors for Word Representation) learns embeddings by analyzing global word co-occurrence statistics across large text corpora. The algorithm constructs a word-word co-occurrence matrix where each entry represents how frequently two words appear together within a specified context window.\n",
    "\n",
    "The training objective minimizes the difference between the dot product of word vectors and the logarithm of their co-occurrence probability. This mathematical formulation ensures that frequently co-occurring words develop similar vector representations.\n",
    "\n",
    "**Co-occurrence Example**:\n",
    "In the sentence \"The quick brown fox jumps over the lazy dog,\" with a context window of 2, the word \"fox\" co-occurs with \"brown,\" \"jumps,\" \"quick,\" and \"over.\" Words that consistently appear in similar contexts across millions of sentences develop similar embeddings.\n",
    "\n",
    "## Numerical Properties and Semantic Relationships\n",
    "\n",
    "The most remarkable property of word embeddings is their ability to encode semantic relationships through vector arithmetic. Mathematical operations on embeddings often yield semantically meaningful results.\n",
    "\n",
    "**Classic Analogy Example**:\n",
    "- vector(\"king\") - vector(\"man\") + vector(\"woman\") ≈ vector(\"queen\")\n",
    "\n",
    "This works because the vector difference between \"king\" and \"man\" captures the concept of royalty, which when added to \"woman\" points toward the feminine royal equivalent.\n",
    "\n",
    "**Dimensional Analysis**:\n",
    "Using 50-dimensional GloVe embeddings, suppose we have:\n",
    "- king: [0.2, -0.1, 0.8, 0.3, ..., 0.1]\n",
    "- man: [0.1, 0.2, 0.4, 0.1, ..., 0.2]  \n",
    "- woman: [0.1, 0.2, 0.4, 0.7, ..., 0.3]\n",
    "\n",
    "The arithmetic operation produces a vector that should be closest to:\n",
    "- queen: [0.2, -0.1, 0.8, 0.9, ..., 0.2]\n",
    "\n",
    "## Semantic Clustering and Distance Metrics\n",
    "\n",
    "Words with similar meanings cluster together in the embedding space. The cosine similarity between vectors provides a measure of semantic relatedness, with values ranging from -1 (completely opposite) to 1 (identical meaning).\n",
    "\n",
    "**Similarity Examples**:\n",
    "- cosine_similarity(\"dog\", \"puppy\") ≈ 0.8 (highly similar)\n",
    "- cosine_similarity(\"dog\", \"computer\") ≈ 0.1 (weakly related)\n",
    "- cosine_similarity(\"hot\", \"cold\") ≈ -0.3 (opposites)\n",
    "\n",
    "These relationships emerge naturally from training data without explicit programming of semantic rules.\n",
    "\n",
    "## Dimensionality and Information Encoding\n",
    "\n",
    "The choice of embedding dimensions represents a trade-off between expressiveness and computational efficiency. Common dimensions include 50, 100, 200, and 300, with each serving different purposes:\n",
    "\n",
    "**50-dimensional embeddings**: Capture basic semantic relationships efficiently, suitable for smaller vocabularies and computational constraints.\n",
    "\n",
    "**300-dimensional embeddings**: Provide richer representations capable of encoding subtle semantic distinctions and complex relationships.\n",
    "\n",
    "Each dimension implicitly represents different semantic aspects, though these aspects aren't directly interpretable. Some dimensions might capture concepts like animacy, size, emotional valence, or grammatical properties, but these associations emerge implicitly during training.\n",
    "\n",
    "## Training Corpus and Statistical Foundation\n",
    "\n",
    "The \"6B\" in \"glove.6B.50d.txt\" indicates training on 6 billion tokens, representing an enormous collection of text from sources like Wikipedia and news articles. This massive scale ensures that statistical patterns reflect genuine language usage rather than idiosyncratic patterns from smaller datasets.\n",
    "\n",
    "The quality of embeddings depends heavily on corpus diversity and size. Larger, more diverse corpora produce embeddings that generalize better across different domains and capture more subtle semantic relationships.\n",
    "\n",
    "## Practical Applications in NLP Pipelines\n",
    "\n",
    "Word embeddings serve as the foundation for numerous NLP applications. In sentiment analysis, embeddings allow models to recognize that \"excellent\" and \"outstanding\" convey similar positive sentiment even if they weren't seen together during training.\n",
    "\n",
    "**Sentence Representation Example**:\n",
    "For the sentence \"The movie was excellent,\" individual word embeddings might be:\n",
    "- \"the\": [0.1, 0.2, -0.1, ...]\n",
    "- \"movie\": [0.3, -0.2, 0.5, ...]  \n",
    "- \"was\": [0.0, 0.1, 0.2, ...]\n",
    "- \"excellent\": [0.8, 0.6, 0.3, ...]\n",
    "\n",
    "A simple sentence representation averages these vectors, though more sophisticated methods use weighted averages or sequential processing through neural networks.\n",
    "\n",
    "## Limitations and Considerations\n",
    "\n",
    "Word embeddings have several important limitations. They struggle with polysemy (words with multiple meanings), as \"bank\" (financial institution) and \"bank\" (river edge) receive the same embedding despite different contexts. They also reflect biases present in training data, potentially perpetuating stereotypes or unfair associations.\n",
    "\n",
    "Additionally, embeddings are static representations that don't adapt to context within specific sentences. Recent advances like BERT and GPT address some limitations through contextualized embeddings, but classical embeddings like GloVe remain valuable for many applications due to their simplicity and computational efficiency.\n",
    "\n",
    "## Integration with Machine Learning Models\n",
    "\n",
    "Word embeddings bridge the gap between human language and machine learning algorithms. They provide a numerical foundation that enables traditional machine learning methods to process text data while preserving semantic relationships that rule-based approaches often miss.\n",
    "\n",
    "The mathematical properties of embeddings - their ability to encode relationships through vector arithmetic, cluster semantically similar concepts, and provide dense representations - make them indispensable tools in modern natural language processing systems.\n",
    "   \n",
    "----\n",
    "\n",
    "#### Sample GloVe embeddings:\n",
    "\n",
    "| Word | Dimension 1 | Dimension 2 | Dimension 3 | Dimension 4 | Dimension 5 | ... | Dimension 48 | Dimension 49 | Dimension 50 |\n",
    "|------|-------------|-------------|-------------|-------------|-------------|-----|--------------|--------------|--------------|\n",
    "| on | 0.30045 | 0.25006 | -0.16692 | 0.1923 | 0.026921 | ... | -0.07131 | 0.23052 | -0.51939 |\n",
    "| is | 0.6185 | 0.64254 | -0.46552 | 0.3757 | 0.74838 | ... | -0.27557 | 0.30899 | 0.48497 |\n",
    "| was | 0.086888 | -0.19416 | -0.24267 | -0.33391 | 0.56731 | ... | -0.77 | 0.3945 | -0.16937 |\n",
    "| said | 0.38973 | -0.2121 | 0.51837 | 0.80136 | 1.0336 | ... | 0.86119 | 0.1415 | 1.2018 |\n",
    "| with | 0.25616 | 0.43694 | -0.11889 | 0.20345 | 0.41959 | ... | -0.07573 | -0.25868 | -0.39339 |\n",
    "| he | -0.20092 | -0.060271 | -0.61766 | -0.8444 | 0.5781 | ... | -0.33317 | -0.041659 | -0.013171 |\n",
    "\n",
    "##### Key Observations from the Sample\n",
    "\n",
    "**Vector Diversity**: Each word has a unique 50-dimensional vector with values ranging approximately from -3 to +4, showing the continuous nature of the embedding space.\n",
    "\n",
    "**Semantic Relationships**: Notice how \"is\" and \"was\" (both forms of \"be\") have some similar patterns, while \"he\" has quite different values, reflecting their different semantic roles.\n",
    "\n",
    "**Dimensionality**: All vectors have exactly 50 dimensions as specified by the \"50d\" in the filename, with each dimension capturing different aspects of word meaning and context.\n",
    "\n",
    "**Value Distribution**: The embeddings contain both positive and negative values, allowing for rich representation of semantic relationships through vector arithmetic operations.\n",
    "\n",
    "This table format clearly shows how each word maps to its corresponding dense vector representation that can be used in machine learning models.\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T23:52:43.597215Z",
     "start_time": "2024-10-26T23:52:41.678124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'hello':\n",
      "[-0.38497   0.80092   0.064106 -0.28355  -0.026759 -0.34532  -0.64253\n",
      " -0.11729  -0.33257   0.55243  -0.087813  0.9035    0.47102   0.56657\n",
      "  0.6985   -0.35229  -0.86542   0.90573   0.03576  -0.071705 -0.12327\n",
      "  0.54923   0.47005   0.35572   1.2611   -0.67581  -0.94983   0.68666\n",
      "  0.3871   -1.3492    0.63512   0.46416  -0.48814   0.83827  -0.9246\n",
      " -0.33722   0.53741  -1.0616   -0.081403 -0.67111   0.30923  -0.3923\n",
      " -0.55002  -0.68827   0.58049  -0.11626   0.013139 -0.57654   0.048833\n",
      "  0.67204 ]\n",
      "Embedding dimension: 50\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess_text(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Preprocess text for NLP tasks through normalization, tokenization, and stopword removal.\n",
    "    \n",
    "    This function implements a standard text preprocessing pipeline commonly used for\n",
    "    machine learning and natural language processing tasks. The preprocessing steps\n",
    "    help normalize text variations and reduce noise by removing common but less\n",
    "    informative words.\n",
    "    \n",
    "    Processing steps:\n",
    "    1. Case normalization: Converts all text to lowercase for consistency\n",
    "    2. Punctuation removal: Eliminates common punctuation marks that may not contribute to meaning\n",
    "    3. Tokenization: Splits text into individual word tokens using NLTK's word_tokenize\n",
    "    4. Stopword filtering: Removes common English words (the, is, at, etc.) that typically\n",
    "       carry little semantic information for classification tasks\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw input text string to be preprocessed.\n",
    "            Can contain mixed case, punctuation, and common stopwords.\n",
    "            \n",
    "    Returns:\n",
    "        list: List of cleaned and filtered tokens ready for further NLP processing.\n",
    "            Tokens are lowercase strings with punctuation and stopwords removed.\n",
    "    \n",
    "    Example:\n",
    "        >>> preprocess_text(\"Hello! This is a great movie.\")\n",
    "        ['hello', 'great', 'movie']\n",
    "        \n",
    "    Note:\n",
    "        Requires NLTK data downloads: punkt tokenizer and stopwords corpus.\n",
    "        Run: nltk.download('punkt') and nltk.download('stopwords') if not available.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Normalize text to lowercase for consistent token matching.\n",
    "    Eliminates case variations that would create duplicate vocabulary entries\n",
    "    (e.g., \"Hello\" and \"hello\" become the same token).\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    \n",
    "    \"\"\"\n",
    "    Remove punctuation marks that typically don't contribute semantic meaning.\n",
    "    Uses character filtering to eliminate common punctuation while preserving\n",
    "    alphanumeric characters and spaces for proper tokenization.\n",
    "    \n",
    "    Removed characters: .,;:!?-\"'()[]{}\n",
    "    This covers most common punctuation but may need expansion for specific domains.\n",
    "    \"\"\"\n",
    "    text = ''.join(c for c in text if c not in '.,;:!?-\"\\'()[]{}')\n",
    "    \n",
    "    \"\"\"\n",
    "    Tokenize preprocessed text into individual word tokens.\n",
    "    NLTK's word_tokenize provides robust tokenization handling edge cases\n",
    "    like contractions, abbreviations, and various text formats better than\n",
    "    simple string splitting.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    \"\"\"\n",
    "    Filter out English stopwords to reduce noise and focus on content words.\n",
    "    Stopwords are frequently occurring words (the, is, at, which, on, etc.)\n",
    "    that typically carry little discriminative information for classification tasks.\n",
    "    \n",
    "    This step significantly reduces vocabulary size and can improve model\n",
    "    performance by emphasizing semantically meaningful words.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "def load_glove_model(file: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load pre-trained GloVe word embeddings from a text file into memory.\n",
    "    \n",
    "    GloVe (Global Vectors for Word Representation) embeddings provide dense vector\n",
    "    representations of words trained on large corpora. These embeddings capture\n",
    "    semantic relationships between words based on their co-occurrence patterns\n",
    "    in natural language text.\n",
    "    \n",
    "    This function parses the standard GloVe file format where each line contains:\n",
    "    word embedding_dim1 embedding_dim2 ... embedding_dimN\n",
    "    \n",
    "    The resulting embeddings can be used to initialize embedding layers in neural\n",
    "    networks or for similarity computations between words.\n",
    "    \n",
    "    Args:\n",
    "        file (str): Path to the GloVe embeddings file.\n",
    "            Common files include glove.6B.50d.txt, glove.6B.100d.txt, etc.\n",
    "            The file naming convention `glove.6B.50d.txt` indicates:\n",
    "            - `6B`: Trained on 6 billion tokens\n",
    "            - `50d`: Each word is represented by a 50-dimensional vector\n",
    "            Format: each line contains word followed by space-separated float values.\n",
    "            \n",
    "    Returns:\n",
    "        dict: Dictionary mapping words (str) to their embedding vectors (numpy.ndarray).\n",
    "            Keys are vocabulary words, values are dense vector representations.\n",
    "            Vector dimensions match the GloVe file specification (e.g., 50, 100, 200, 300).\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified GloVe file path doesn't exist.\n",
    "        ValueError: If file format is incorrect or contains invalid numeric values.\n",
    "        \n",
    "    Example:\n",
    "        >>> embeddings = load_glove_model(\"glove.6B.50d.txt\")\n",
    "        >>> print(embeddings['king'].shape)  # (50,)\n",
    "        >>> similarity = np.dot(embeddings['king'], embeddings['queen'])\n",
    "        \n",
    "    Note:\n",
    "        Large GloVe files (several GB) may take significant time to load and\n",
    "        consume substantial memory. Consider loading only required words for\n",
    "        memory-constrained applications.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Initialize empty dictionary to store word-to-embedding mappings.\n",
    "    Dictionary provides O(1) lookup time for word embeddings during model training.\n",
    "    \"\"\"\n",
    "    glove_model = {}\n",
    "\n",
    "    \"\"\"\n",
    "    Process GloVe file line by line to avoid loading entire file into memory.\n",
    "    Each line represents one word and its corresponding embedding vector.\n",
    "    \n",
    "    File format: \"word dim1 dim2 dim3 ... dimN\"\n",
    "    Example: \"hello 0.12 -0.45 0.78 ... 0.23\"\n",
    "    \"\"\"\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            \"\"\"\n",
    "            Parse each line to extract word and embedding components.\n",
    "            split() separates word (first element) from embedding values (remaining elements).\n",
    "            \"\"\"\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            \n",
    "            \"\"\"\n",
    "            Convert embedding string values to numpy array of float64.\n",
    "            Using float64 provides high precision for embedding values, though\n",
    "            float32 might be sufficient for most applications with memory benefits.\n",
    "            \"\"\"\n",
    "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "            \n",
    "            \"\"\"\n",
    "            Store word-embedding pair in dictionary for fast lookup.\n",
    "            Overwrites any duplicate words (shouldn't occur in standard GloVe files).\n",
    "            \"\"\"\n",
    "            glove_model[word] = embedding\n",
    "\n",
    "    return glove_model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load pre-trained GloVe embeddings for use in NLP models.\n",
    "\n",
    "This loads the 50-dimensional GloVe embeddings trained on 6 billion tokens\n",
    "from Wikipedia and Gigaword corpora. The embeddings provide semantic word\n",
    "representations that can be used for:\n",
    "- Initializing embedding layers in neural networks\n",
    "- Computing word similarities and analogies\n",
    "- Feature engineering for traditional ML models\n",
    "\"\"\"\n",
    "embedding_dict = load_glove_model(\"data/glove.6B.50d.txt\")\n",
    "\n",
    "\"\"\"\n",
    "Demonstrate embedding lookup and inspect embedding properties.\n",
    "\n",
    "The 'hello' embedding shows how common words are represented as dense\n",
    "50-dimensional vectors capturing semantic relationships learned from\n",
    "large text corpora.\n",
    "\"\"\"\n",
    "hello_embedding = embedding_dict['hello']\n",
    "print(\"Embedding for 'hello':\")\n",
    "print(hello_embedding)\n",
    "\n",
    "\"\"\"\n",
    "Verify embedding dimensionality matches expected GloVe specification.\n",
    "\n",
    "For glove.6B.50d.txt, each word should have exactly 50 dimensions.\n",
    "This verification ensures the file was loaded correctly and embeddings\n",
    "have the expected shape for downstream usage.\n",
    "\"\"\"\n",
    "print(f\"Embedding dimension: {hello_embedding.shape[0]}\")  # This should be 50 for this specific file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sentence Embedding Example ---\n",
      "Original sentence: 'The quick brown fox jumps over the lazy dog'\n",
      "Preprocessed tokens: ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n",
      "Sentence embedding shape: (50,)\n",
      "First 5 values of sentence embedding: [-0.15505333 -0.18144967 -0.12989    -0.17379167  0.29983667]\n",
      "\n",
      "--- Finding words similar to 'king' ---\n",
      "Most similar words (with similarity scores):\n",
      "prince: 0.8236\n",
      "queen: 0.7839\n",
      "ii: 0.7746\n",
      "emperor: 0.7736\n",
      "son: 0.7667\n",
      "\n",
      "--- Word Vector Arithmetic Example ---\n",
      "king - man + woman ≈ queen (similarity: 0.8610)\n"
     ]
    }
   ],
   "source": [
    "# DEMONSTRATION: Working with word embeddings\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def get_sentence_embedding(text: str, embedding_dict: dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a sentence to its embedding representation by averaging constituent word vectors.\n",
    "    \n",
    "    This function implements a simple but effective approach to sentence-level embeddings\n",
    "    by taking the element-wise average of all valid word embeddings in the sentence.\n",
    "    While more sophisticated methods exist (weighted averages, neural encoders), mean\n",
    "    pooling often provides surprisingly good results for many NLP tasks.\n",
    "    \n",
    "    The approach handles out-of-vocabulary words gracefully by filtering them out,\n",
    "    and returns a zero vector for sentences with no valid embeddings to prevent\n",
    "    errors in downstream processing.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input sentence or text to convert to embedding representation.\n",
    "            Can contain punctuation, mixed case, and stopwords which will be\n",
    "            preprocessed according to the preprocess_text function.\n",
    "        embedding_dict (dict): Dictionary mapping words to their embedding vectors.\n",
    "            Typically loaded from pre-trained embeddings like GloVe or Word2Vec.\n",
    "            Keys are lowercase strings, values are numpy arrays of consistent dimensionality.\n",
    "            \n",
    "    Returns:\n",
    "        np.ndarray: Dense vector representation of the sentence with same dimensionality\n",
    "            as individual word embeddings. For empty sentences or sentences with no\n",
    "            valid words, returns zero vector of appropriate dimension.\n",
    "    \n",
    "    Example:\n",
    "        >>> sentence = \"The cat sat on the mat\"\n",
    "        >>> embedding = get_sentence_embedding(sentence, glove_embeddings)\n",
    "        >>> print(embedding.shape)  # (50,) for 50-dimensional GloVe\n",
    "        \n",
    "    Note:\n",
    "        This averaging approach loses word order information and may not capture\n",
    "        complex compositional semantics. Consider using sequential models (LSTM, \n",
    "        Transformer) for tasks requiring word order understanding.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Apply text preprocessing to extract clean, normalized tokens.\n",
    "    Removes punctuation, converts to lowercase, tokenizes, and filters stopwords\n",
    "    to focus on content-bearing words for embedding lookup.\n",
    "    \"\"\"\n",
    "    tokens = preprocess_text(text)\n",
    "    \n",
    "    \"\"\"\n",
    "    Filter tokens to include only words present in the embedding vocabulary.\n",
    "    This step handles out-of-vocabulary (OOV) words by excluding them from\n",
    "    the sentence representation, preventing KeyError exceptions and ensuring\n",
    "    robust processing of diverse text inputs.\n",
    "    \"\"\"\n",
    "    valid_tokens = [token for token in tokens if token in embedding_dict]\n",
    "    \n",
    "    \"\"\"\n",
    "    Handle edge case of sentences with no valid embeddings.\n",
    "    Returns zero vector with same dimensionality as word embeddings to maintain\n",
    "    consistent output shape and prevent downstream processing errors.\n",
    "    \"\"\"\n",
    "    if not valid_tokens:\n",
    "        embedding_dim = next(iter(embedding_dict.values())).shape[0]\n",
    "        return np.zeros(embedding_dim)\n",
    "    \n",
    "    \"\"\"\n",
    "    Retrieve embedding vectors for all valid tokens in the sentence.\n",
    "    Creates list of numpy arrays, each representing one word's dense vector\n",
    "    representation learned from the pre-trained embedding model.\n",
    "    \"\"\"\n",
    "    token_embeddings = [embedding_dict[token] for token in valid_tokens]\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute element-wise average across all word embeddings to create sentence representation.\n",
    "    numpy.mean with axis=0 averages across the first dimension (words) while preserving\n",
    "    the embedding dimension, resulting in a single vector representing the entire sentence.\n",
    "    \n",
    "    Mathematical formulation: sentence_vector = (1/n) * Σ(word_vector_i) for i=1 to n\n",
    "    \"\"\"\n",
    "    sentence_embedding = np.mean(token_embeddings, axis=0)\n",
    "    \n",
    "    return sentence_embedding\n",
    "\n",
    "\n",
    "def find_similar_words(word: str, embedding_dict: dict, n: int = 5) -> list:\n",
    "    \"\"\"\n",
    "    Identify the n most semantically similar words to a target word using cosine similarity.\n",
    "    \n",
    "    This function leverages the geometric properties of word embeddings where semantically\n",
    "    related words cluster together in the vector space. Cosine similarity measures the\n",
    "    angle between vectors, providing a scale-invariant measure of semantic relatedness\n",
    "    that ranges from -1 (opposite) to 1 (identical direction).\n",
    "    \n",
    "    The cosine similarity metric is preferred over Euclidean distance for word embeddings\n",
    "    because it focuses on the direction rather than magnitude of vectors, making it\n",
    "    more robust to variations in vector norms while preserving semantic relationships.\n",
    "    \n",
    "    Args:\n",
    "        word (str): Target word to find semantic neighbors for. Must exist in the\n",
    "            embedding dictionary vocabulary. Case-sensitive based on embedding keys.\n",
    "        embedding_dict (dict): Pre-trained word embeddings mapping words to vectors.\n",
    "            All vectors should have consistent dimensionality for valid comparisons.\n",
    "        n (int, optional): Number of most similar words to return. Defaults to 5.\n",
    "            Larger values provide broader semantic neighborhoods but increase computation.\n",
    "            \n",
    "    Returns:\n",
    "        list: List of tuples (word, similarity_score) ordered by decreasing similarity.\n",
    "            Similarity scores are float values between -1 and 1, with higher values\n",
    "            indicating greater semantic similarity. Returns error message if word\n",
    "            not found in vocabulary.\n",
    "    \n",
    "    Example:\n",
    "        >>> similar = find_similar_words(\"king\", embeddings, n=3)\n",
    "        >>> print(similar)\n",
    "        [('queen', 0.8547), ('prince', 0.7834), ('monarch', 0.7456)]\n",
    "        \n",
    "    Note:\n",
    "        Computation scales O(V) with vocabulary size V. For large vocabularies,\n",
    "        consider using approximate nearest neighbor algorithms for efficiency.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Validate that the target word exists in the embedding vocabulary.\n",
    "    Prevents KeyError exceptions and provides informative feedback for\n",
    "    out-of-vocabulary queries.\n",
    "    \"\"\"\n",
    "    if word not in embedding_dict:\n",
    "        return [(\"Word not found in vocabulary\", 0)]\n",
    "    \n",
    "    \"\"\"\n",
    "    Retrieve the embedding vector for the target word.\n",
    "    This serves as the reference point for computing similarities with all\n",
    "    other words in the vocabulary.\n",
    "    \"\"\"\n",
    "    word_embedding = embedding_dict[word]\n",
    "    \n",
    "    \"\"\"\n",
    "    Define cosine similarity computation for vector comparison.\n",
    "    \n",
    "    Cosine similarity formula: cos(θ) = (A·B) / (||A|| * ||B||)\n",
    "    where A·B is dot product and ||A|| is vector magnitude (L2 norm).\n",
    "    \n",
    "    This measures the cosine of the angle between vectors, providing a\n",
    "    normalized similarity score independent of vector magnitudes.\n",
    "    \"\"\"\n",
    "    def cosine_similarity(vec1, vec2):\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm_vec1 = np.linalg.norm(vec1)\n",
    "        norm_vec2 = np.linalg.norm(vec2)\n",
    "        return dot_product / (norm_vec1 * norm_vec2)\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute similarity scores between target word and all vocabulary words.\n",
    "    Iterates through entire embedding dictionary to build comprehensive\n",
    "    similarity rankings for semantic neighbor identification.\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "    for other_word, other_embedding in embedding_dict.items():\n",
    "        \"\"\"\n",
    "        Skip self-comparison to avoid trivial perfect similarity.\n",
    "        Target word would always rank first with similarity 1.0.\n",
    "        \"\"\"\n",
    "        if other_word == word:\n",
    "            continue\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculate cosine similarity between target and candidate word vectors.\n",
    "        Higher scores indicate greater semantic relatedness based on\n",
    "        distributional similarity in the training corpus.\n",
    "        \"\"\"\n",
    "        similarity = cosine_similarity(word_embedding, other_embedding)\n",
    "        similarities.append((other_word, similarity))\n",
    "    \n",
    "    \"\"\"\n",
    "    Sort candidate words by similarity score in descending order.\n",
    "    Returns top n most similar words as ranked list of (word, score) tuples.\n",
    "    \"\"\"\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:n]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Demonstrate sentence-level embedding computation on sample text.\n",
    "Shows how individual word embeddings combine to create document representations\n",
    "suitable for tasks like semantic similarity, classification, or clustering.\n",
    "\"\"\"\n",
    "sample_sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "print(\"\\n--- Sentence Embedding Example ---\")\n",
    "\n",
    "sentence_embedding = get_sentence_embedding(sample_sentence, embedding_dict)\n",
    "print(f\"Original sentence: '{sample_sentence}'\")\n",
    "print(f\"Preprocessed tokens: {preprocess_text(sample_sentence)}\")\n",
    "print(f\"Sentence embedding shape: {sentence_embedding.shape}\")\n",
    "print(f\"First 5 values of sentence embedding: {sentence_embedding[:5]}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Demonstrate semantic similarity search using pre-trained embeddings.\n",
    "Illustrates how word embeddings capture semantic relationships through\n",
    "vector space proximity, enabling automated discovery of related concepts.\n",
    "\"\"\"\n",
    "target_word = \"king\"\n",
    "print(f\"\\n--- Finding words similar to '{target_word}' ---\")\n",
    "\n",
    "similar_words = find_similar_words(target_word, embedding_dict)\n",
    "print(\"Most similar words (with similarity scores):\")\n",
    "\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score:.4f}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Demonstrate famous word analogy solving through vector arithmetic.\n",
    "The equation \"king - man + woman ≈ queen\" shows how embeddings encode\n",
    "relational knowledge that can be manipulated through linear algebra operations.\n",
    "\n",
    "This works because:\n",
    "1. (king - man) captures the concept \"royalty\"\n",
    "2. Adding \"woman\" applies this concept to the feminine domain\n",
    "3. The result vector points toward \"queen\" in the embedding space\n",
    "\"\"\"\n",
    "if all(word in embedding_dict for word in [\"king\", \"man\", \"woman\"]):\n",
    "    print(\"\\n--- Word Vector Arithmetic Example ---\")\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute analogy vector through arithmetic operations on word embeddings.\n",
    "    This algebraic manipulation encodes the semantic relationship:\n",
    "    \"king is to man as queen is to woman\"\n",
    "    \"\"\"\n",
    "    result_vector = embedding_dict[\"king\"] - embedding_dict[\"man\"] + embedding_dict[\"woman\"]\n",
    "    \n",
    "    \"\"\"\n",
    "    Find the vocabulary word whose embedding is closest to the computed result vector.\n",
    "    Uses cosine similarity to identify the word that best completes the analogy\n",
    "    based on geometric proximity in the embedding space.\n",
    "    \"\"\"\n",
    "    closest_word = None\n",
    "    highest_similarity = -1\n",
    "    \n",
    "    for word, embedding in embedding_dict.items():\n",
    "        \"\"\"\n",
    "        Exclude words used in the analogy equation to find novel completions.\n",
    "        Prevents trivial solutions and focuses on discovering the intended\n",
    "        analogical relationship (queen).\n",
    "        \"\"\"\n",
    "        if word in [\"king\", \"man\", \"woman\"]:\n",
    "            continue\n",
    "            \n",
    "        \"\"\"\n",
    "        Calculate cosine similarity between result vector and candidate word embedding.\n",
    "        Tracks the word with highest similarity as the best analogy completion.\n",
    "        \"\"\"\n",
    "        similarity = np.dot(result_vector, embedding) / (np.linalg.norm(result_vector) * np.linalg.norm(embedding))\n",
    "        \n",
    "        if similarity > highest_similarity:\n",
    "            highest_similarity = similarity\n",
    "            closest_word = word\n",
    "    \n",
    "    print(f\"king - man + woman ≈ {closest_word} (similarity: {highest_similarity:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T23:56:35.078776Z",
     "start_time": "2024-10-26T23:56:35.014913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fox\n",
      "[ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
      " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
      "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
      "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
      "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
      " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
      " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
      "  1.5064  ]\n"
     ]
    }
   ],
   "source": [
    "# Now let's create the embedding matrix for sample_text\n",
    "sample_tokens = preprocess_text(sample_text)\n",
    "sample_embedding_matrix = []\n",
    "\n",
    "for sample_token in sample_tokens:\n",
    "    sample_embedding_matrix.append(embedding_dict[sample_token])\n",
    "\n",
    "# we should have as many embedding vectors (rows of embedding matrix) as there are sample tokens\n",
    "assert len(sample_embedding_matrix) == len(sample_tokens)\n",
    "\n",
    "# lets print a token and its embedding\n",
    "print(sample_tokens[2])\n",
    "print(sample_embedding_matrix[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
