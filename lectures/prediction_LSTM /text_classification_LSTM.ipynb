{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification using LSTM\n",
    "\n",
    "In this coding exercise, you will create a simple LSTM model using PyTorch to perform text classification on a dataset of short phrases. We will perform the following steps:\n",
    "\n",
    "- Create a vocabulary to represent words as indices.\n",
    "- Tokenize, encode, and pad the phrases.\n",
    "- Convert the phrases and categories to PyTorch tensors.\n",
    "- Instantiate the LSTM model with the vocabulary size, embedding dimensions, hidden dimensions, and output dimensions.\n",
    "- Define the loss function and optimizer.\n",
    "- Train the model for a number of epochs.\n",
    "- Test the model on new phrases and print the category predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[4, 5, 0]\n",
      "[6, 7, 8]\n",
      "[9, 10, 11]\n",
      "[12, 13, 0]\n",
      "[14, 15, 0]\n",
      "\n",
      "\n",
      "GREAT GOAL SCORED\n",
      "AMAZING TOUCHDOWN\n",
      "NEW PHONE RELEASE\n",
      "LATEST LAPTOP MODEL\n",
      "TASTY PIZZA\n",
      "DELICIOUS BURGER\n",
      "\n",
      "\n",
      "GREAT GOAL SCORED\n",
      "AMAZING TOUCHDOWN\n",
      "NEW PHONE RELEASE\n",
      "LATEST LAPTOP MODEL\n",
      "TASTY PIZZA\n",
      "DELICIOUS BURGER\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Phrases (textual data) and their category labels (0 for sports, 1 for technology, 2 for food)\n",
    "This data is extremely less for realistically training an LSTM model.  This model might overfit \n",
    "as the data is less. Feel free to use any other data source for training or create your own dummy data\n",
    "\"\"\"\n",
    "\n",
    "phrases = [\"great goal scored\", \"amazing touchdown\", \"new phone release\", \"latest laptop model\", \"tasty pizza\", \"delicious burger\"]\n",
    "categories = [0, 0, 1, 1, 2, 2]\n",
    "\n",
    "\"\"\"\n",
    "Create a vocabulary to represent words as indices\n",
    "\"\"\"\n",
    "vocab = {\"<PAD>\": 0, \"great\": 1, \"goal\": 2, \"scored\": 3, \"amazing\": 4, \"touchdown\": 5, \"new\": 6, \"phone\": 7, \"release\": 8, \"latest\": 9, \"laptop\": 10, \"model\": 11, \"tasty\": 12, \"pizza\": 13, \"delicious\": 14, \"burger\": 15}\n",
    "\n",
    "\"\"\"\n",
    "Tokenize, encode, and pad phrases\n",
    "\"\"\"\n",
    "encoded_phrases = [[vocab[word] for word in phrase.split()] for phrase in phrases]\n",
    "max_length = max([len(phrase) for phrase in encoded_phrases])\n",
    "padded_phrases = [phrase + [vocab[\"<PAD>\"]] * (max_length - len(phrase)) for phrase in encoded_phrases]\n",
    "\n",
    "\"\"\"\n",
    "Concise unpacking approach for printing all elements.\n",
    "Uses print()'s ability to handle multiple arguments with sep parameter.\n",
    "\"\"\"\n",
    "print(*padded_phrases, sep='\\n')\n",
    "print(\"\\n\")\n",
    "\n",
    "\"\"\"\n",
    "Proper use of map() for data transformation.\n",
    "Converts each phrase to uppercase - this creates new data.\n",
    "\"\"\"\n",
    "uppercase_phrases = list(map(str.upper, phrases))\n",
    "print(*uppercase_phrases, sep='\\n')\n",
    "print(\"\\n\")\n",
    "\n",
    "\"\"\"\n",
    "Even better: use list comprehension for transformations.\n",
    "More readable and Pythonic than map() in most cases.\n",
    "\"\"\"\n",
    "uppercase_phrases = [phrase.upper() for phrase in phrases]\n",
    "print(*uppercase_phrases, sep='\\n')\n",
    "print(\"\\n\")\n",
    "\n",
    "\"\"\"\n",
    "Convert phrases and categories to PyTorch tensors\n",
    "\"\"\"\n",
    "inputs = torch.LongTensor(padded_phrases)\n",
    "labels = torch.LongTensor(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "##### Detailed Dimension Analysis\n",
    "\n",
    "###### Step-by-Step Dimension Tracking\n",
    "\n",
    "**Starting dimensions**: Assume `seq_len=10`, `batch_size=32`, `embedding_dim=100`, `hidden_dim=128`, `output_dim=5`\n",
    "\n",
    "1. **Input**: `x.shape = (10, 32)` - 10 tokens per sequence, 32 sequences in batch\n",
    "\n",
    "2. **After Embedding**: `embedded.shape = (10, 32, 100)`\n",
    "   - Each of 10×32 token indices becomes 100-dimensional vector\n",
    "   - Total: 320 embedding vectors, each 100-dimensional\n",
    "\n",
    "3. **After LSTM**: \n",
    "   - `output.shape = (10, 32, 128)` - Hidden states for all 10 timesteps\n",
    "   - `hidden.shape = (1, 32, 128)` - Only final hidden state\n",
    "   - The \"1\" comes from `num_layers=1` (default for single-layer LSTM)\n",
    "\n",
    "4. **After squeeze(0)**: `hidden.squeeze(0).shape = (32, 128)`\n",
    "   - Removes the layer dimension, keeping batch and hidden dimensions\n",
    "\n",
    "5. **After Linear**: `logits.shape = (32, 5)`\n",
    "   - 32 sequences → 32 predictions, each with 5 class scores\n",
    "\n",
    "###### Why These Specific Dimensions?\n",
    "\n",
    "**LSTM Output Format**: PyTorch LSTM returns `(num_layers, batch_size, hidden_dim)` for hidden state, hence the need for `squeeze(0)` to remove the single layer dimension.\n",
    "\n",
    "**Sequence-to-One**: We discard all intermediate LSTM outputs and use only the final hidden state, assuming it encodes sufficient information about the entire sequence for classification.\n",
    "\n",
    "**Batch Processing**: All operations preserve the batch dimension, allowing efficient parallel processing of multiple sequences simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "#### Numerical Example of Embedding Layer\n",
    "\n",
    "Let me demonstrate with a concrete example using small dimensions for clarity.\n",
    "\n",
    "##### Setup Parameters\n",
    "```python\n",
    "vocab_size = 6      # Small vocabulary: [\"hello\", \"world\", \"good\", \"bad\", \"movie\", \"<PAD>\"]\n",
    "embedding_dim = 4   # 4-dimensional embeddings\n",
    "```\n",
    "\n",
    "##### The Embedding Matrix\n",
    "\n",
    "When you create `nn.Embedding(6, 4)`, PyTorch initializes a learnable matrix:\n",
    "\n",
    "```python\n",
    "# Initial embedding matrix (randomly initialized)\n",
    "embedding_matrix = torch.tensor([\n",
    "    [0.2, -0.1,  0.8,  0.3],  # Index 0: \"hello\"\n",
    "    [0.5,  0.9, -0.4,  0.1],  # Index 1: \"world\" \n",
    "    [-0.2, 0.7,  0.6, -0.8],  # Index 2: \"good\"\n",
    "    [0.9, -0.5,  0.2,  0.4],  # Index 3: \"bad\"\n",
    "    [0.1,  0.3, -0.9,  0.7],  # Index 4: \"movie\"\n",
    "    [0.0,  0.0,  0.0,  0.0]   # Index 5: \"<PAD>\" (often zero)\n",
    "])\n",
    "# Shape: (6, 4) = (vocab_size, embedding_dim)\n",
    "```\n",
    "\n",
    "##### Understanding Embedding Dimensions vs. Features\n",
    "\n",
    "The `embedding_dim = 4` is **not** the features we want to extract for classification. It's an **intermediate representation** that helps the model learn those features.\n",
    "\n",
    "##### The Distinction\n",
    "\n",
    "**Embedding dimensions**: Dense vector representation of individual tokens\n",
    "**Features for classification**: Higher-level patterns learned by the LSTM\n",
    "\n",
    "##### What Each Layer Extracts\n",
    "\n",
    "```python\n",
    "# Input: Token indices [2, 4] representing \"good movie\"\n",
    "\n",
    "# Embedding layer output (embedding_dim = 4):\n",
    "embedded = [\n",
    "    [-0.2,  0.7,  0.6, -0.8],  # \"good\" token representation\n",
    "    [ 0.1,  0.3, -0.9,  0.7]   # \"movie\" token representation  \n",
    "]\n",
    "# These are NOT the final features - just token-level representations\n",
    "\n",
    "# LSTM layer processes these embeddings and extracts:\n",
    "final_hidden_state = [0.3, -0.1, 0.8, 0.2, -0.5, 0.9, ...]  # hidden_dim=128\n",
    "# THIS is the feature vector we want - sentence-level representation\n",
    "```\n",
    "\n",
    "##### The Learning Hierarchy\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[\"Token Level<br>Individual word meanings<br>embedding_dim=4\"] --> B[\"Sequence Level<br>Sentence patterns & context<br>hidden_dim=128\"]\n",
    "    B --> C[\"Classification Features<br>Sentiment, topic, etc.<br>output_dim=num_classes\"]\n",
    "    \n",
    "    A1[\"'good' → [-0.2, 0.7, 0.6, -0.8]<br>'movie' → [0.1, 0.3, -0.9, 0.7]\"] --> A\n",
    "    B1[\"'good movie' → [0.3, -0.1, 0.8, ...]<br>Captures: positive sentiment<br>+ movie context\"] --> B\n",
    "    C1[\"Final prediction:<br>[0.1, 0.9] = 90% positive\"] --> C\n",
    "    \n",
    "    style A fill:#E8F4FD,color:#000\n",
    "    style B fill:#A3CCF6,color:#000\n",
    "    style C fill:#E0F0E0,color:#000\n",
    "    style A1 fill:#FFE0E0,color:#000\n",
    "    style B1 fill:#FFF0E0,color:#000\n",
    "    style C1 fill:#FFFFCC,color:#000\n",
    "```\n",
    "\n",
    "##### What We Actually Want\n",
    "\n",
    "The **classification features** we want are in the LSTM's final hidden state:\n",
    "\n",
    "```python\n",
    "# This 128-dimensional vector contains the learned features:\n",
    "final_features = [0.3, -0.1, 0.8, 0.2, -0.5, 0.9, ...]  # 128 values\n",
    "\n",
    "# These might represent concepts like:\n",
    "# feature[0] = 0.3   → \"positive sentiment strength\"\n",
    "# feature[1] = -0.1  → \"movie genre indicator\" \n",
    "# feature[2] = 0.8   → \"emotional intensity\"\n",
    "# feature[67] = 0.2  → \"grammatical complexity\"\n",
    "# etc.\n",
    "```\n",
    "\n",
    "##### Dimensionality Purpose\n",
    "\n",
    "**embedding_dim (4)**: Just enough dimensions to distinguish between vocabulary words and capture basic semantic relationships.\n",
    "\n",
    "**hidden_dim (128)**: Much larger dimension to capture complex sequential patterns, context dependencies, and classification-relevant features.\n",
    "\n",
    "##### Analogy\n",
    "\n",
    "Think of embeddings like individual LEGO pieces (simple building blocks), while the LSTM hidden state is like the complex structure built from those pieces. The embedding dimensions give you the basic components, but the LSTM features give you the architectural patterns needed for classification.\n",
    "\n",
    "The 4-dimensional embeddings are tools for learning, not the final features we want for classification.\n",
    "\n",
    "\n",
    "##### Token-to-Index Mapping\n",
    "```python\n",
    "word_to_idx = {\n",
    "    \"hello\": 0, \"world\": 1, \"good\": 2, \n",
    "    \"bad\": 3, \"movie\": 4, \"<PAD>\": 5\n",
    "}\n",
    "```\n",
    "\n",
    "##### Input Processing Example\n",
    "\n",
    "**Input sentence**: \"good movie\"\n",
    "**Tokenized indices**: `[2, 4]`\n",
    "\n",
    "```python\n",
    "# Input tensor of token indices\n",
    "x = torch.tensor([2, 4])  # Shape: (2,) representing \"good movie\"\n",
    "\n",
    "# Embedding lookup\n",
    "embedded = embedding_layer(x)\n",
    "```\n",
    "\n",
    "##### Step-by-Step Lookup Process\n",
    "\n",
    "```python\n",
    "# Manual lookup to show what happens:\n",
    "token_2_embedding = embedding_matrix[2, :]  # \"good\" → [-0.2, 0.7, 0.6, -0.8]\n",
    "token_4_embedding = embedding_matrix[4, :]  # \"movie\" → [0.1, 0.3, -0.9, 0.7]\n",
    "\n",
    "# Result\n",
    "embedded = torch.tensor([\n",
    "    [-0.2,  0.7,  0.6, -0.8],  # \"good\"\n",
    "    [ 0.1,  0.3, -0.9,  0.7]   # \"movie\"\n",
    "])\n",
    "# Shape: (2, 4) = (sequence_length, embedding_dim)\n",
    "```\n",
    "\n",
    "##### Batch Processing Example\n",
    "\n",
    "**Multiple sentences**:\n",
    "- Sentence 1: \"good movie\" → `[2, 4]`\n",
    "- Sentence 2: \"bad movie\" → `[3, 4]`\n",
    "\n",
    "```python\n",
    "# Batched input (assuming sequences padded to same length)\n",
    "x_batch = torch.tensor([\n",
    "    [2, 4],  # \"good movie\"\n",
    "    [3, 4]   # \"bad movie\"  \n",
    "])\n",
    "# Shape: (2, 2) = (batch_size, seq_len)\n",
    "\n",
    "# After embedding\n",
    "embedded_batch = embedding_layer(x_batch)\n",
    "# Shape: (2, 2, 4) = (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "embedded_batch = torch.tensor([\n",
    "    [[-0.2,  0.7,  0.6, -0.8],   # \"good\"\n",
    "     [ 0.1,  0.3, -0.9,  0.7]],  # \"movie\"\n",
    "    [[ 0.9, -0.5,  0.2,  0.4],   # \"bad\"\n",
    "     [ 0.1,  0.3, -0.9,  0.7]]   # \"movie\"\n",
    "])\n",
    "```\n",
    "\n",
    "##### Learning Process\n",
    "\n",
    "During training, gradients update the embedding matrix:\n",
    "\n",
    "```python\n",
    "# Initial: \"good\" embedding\n",
    "embedding_matrix[2] = [-0.2, 0.7, 0.6, -0.8]\n",
    "\n",
    "# After some training (example update)\n",
    "embedding_matrix[2] = [-0.1, 0.8, 0.5, -0.7]  # Learned better representation\n",
    "\n",
    "# The model learns that \"good\" should have embeddings that help\n",
    "# distinguish positive sentiment in classification tasks\n",
    "```\n",
    "\n",
    "##### Key Properties\n",
    "\n",
    "**Discrete → Continuous**: Token index `2` becomes dense vector `[-0.2, 0.7, 0.6, -0.8]`\n",
    "\n",
    "**Learnable**: These numbers change during training to better represent semantic relationships\n",
    "\n",
    "**Shared Representations**: Token `4` (\"movie\") gets the same embedding `[0.1, 0.3, -0.9, 0.7]` wherever it appears\n",
    "\n",
    "**Efficient Lookup**: No matrix multiplication - just indexing into the embedding table\n",
    "\n",
    "This embedding layer effectively converts sparse, discrete token indices into dense, continuous representations that neural networks can process effectively.\n",
    "\n",
    "---\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[\"Input Tokens<br>(seq_len, batch_size)<br>Token indices\"] --> B[\"Embedding Layer<br>nn.Embedding<br>vocab_size → embedding_dim\"]\n",
    "    B --> C[\"Dense Vectors<br>(seq_len, batch_size, embedding_dim)<br>Continuous representations\"]\n",
    "    C --> D[\"LSTM Layer<br>nn.LSTM<br>Sequential processing\"]\n",
    "    D --> E[\"All Hidden States<br>(seq_len, batch_size, hidden_dim)<br>Temporal features\"]\n",
    "    D --> F[\"Final Hidden State<br>(1, batch_size, hidden_dim)<br>Sequence summary\"]\n",
    "    F --> G[\"Squeeze Operation<br>Remove layer dimension<br>(batch_size, hidden_dim)\"]\n",
    "    G --> H[\"Linear Layer<br>nn.Linear<br>hidden_dim → output_dim\"]\n",
    "    H --> I[\"Class Logits<br>(batch_size, output_dim)<br>Raw classification scores\"]\n",
    "    \n",
    "    E -.->|\"Discarded\"| J[\"Not used for<br>classification\"]\n",
    "    \n",
    "    style A fill:#E8F4FD,color:#000\n",
    "    style B fill:#D1E7FB,color:#000\n",
    "    style C fill:#A3CCF6,color:#000\n",
    "    style D fill:#75B1F1,color:#000\n",
    "    style E fill:#FFE0E0,color:#000\n",
    "    style F fill:#4796EC,color:#000\n",
    "    style G fill:#1E88E5,color:#000\n",
    "    style H fill:#1976D2,color:#000\n",
    "    style I fill:#E0F0E0,color:#000\n",
    "    style J fill:#FFCCCC,color:#000\n",
    "```\n",
    "\n",
    "The diagram illustrates the key architectural decisions:\n",
    "\n",
    "**Information Flow**: Input tokens undergo progressive transformation from discrete indices to dense embeddings to sequential features to final classification scores.\n",
    "\n",
    "**Dimensionality Changes**: Each step shows how tensor shapes evolve, with the critical dimension changes highlighted at each transformation.\n",
    "\n",
    "**Selection Strategy**: The dotted line shows that intermediate LSTM outputs are discarded, with only the final hidden state used for classification - a sequence-to-one prediction approach.\n",
    "\n",
    "**Bottleneck Design**: The final hidden state serves as a compressed representation of the entire input sequence, requiring the LSTM to encode all relevant information for classification in this single vector.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhraseClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based phrase classifier for text classification tasks using sequence-to-one prediction.\n",
    "    \n",
    "    Architecture Flow:\n",
    "    Input tokens → Embedding → LSTM → Final hidden state → Linear → Class logits\n",
    "    \n",
    "    The model processes variable-length text sequences by:\n",
    "    1. Converting token indices to dense vector representations via embedding\n",
    "    2. Processing embedded sequences through LSTM to capture sequential patterns\n",
    "    3. Using the final LSTM hidden state as a sentence-level representation\n",
    "    4. Mapping this representation to class probabilities via linear transformation\n",
    "    \n",
    "    This architecture is effective for document-level classification where the entire\n",
    "    sequence context determines the output label (sentiment, topic, etc.).\n",
    "\n",
    "    Attributes:\n",
    "        embedding (nn.Embedding): Token index to dense vector conversion\n",
    "        lstm (nn.LSTM): Sequential pattern learning with memory\n",
    "        fc (nn.Linear): Classification head for final prediction\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Total number of unique tokens in vocabulary\n",
    "        embedding_dim (int): Dimensionality of dense word vectors (typically 50-300)\n",
    "        hidden_dim (int): Size of LSTM hidden state (controls model capacity)\n",
    "        output_dim (int): Number of target classes for classification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Initialize phrase classifier with specified architecture dimensions.\n",
    "        \n",
    "        Layer initialization creates:\n",
    "        - Embedding matrix: (vocab_size × embedding_dim) learnable lookup table\n",
    "        - LSTM cell: Processes sequences with hidden_dim internal memory\n",
    "        - Linear classifier: Maps hidden_dim features to output_dim classes\n",
    "        \"\"\"\n",
    "        super(PhraseClassifier, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Embedding layer converts discrete token indices to continuous vectors.\n",
    "        \n",
    "        Creates learnable lookup table of shape (vocab_size, embedding_dim) where:\n",
    "        - Each row represents one vocabulary token's dense embedding\n",
    "        - Token index i maps to embedding_matrix[i, :] vector\n",
    "        - Gradients update embeddings during training for task-specific representations\n",
    "        \"\"\"\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        \"\"\"\n",
    "        LSTM layer processes embedded sequences to learn temporal dependencies.\n",
    "        \n",
    "        Default configuration (no batch_first=True) expects input shape:\n",
    "        (sequence_length, batch_size, embedding_dim)\n",
    "        \n",
    "        LSTM maintains internal cell state and hidden state to capture:\n",
    "        - Long-range dependencies across sequence positions\n",
    "        - Sequential patterns relevant for classification\n",
    "        - Contextual information from bidirectional processing\n",
    "        \"\"\"\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        \"\"\"\n",
    "        Final classification layer maps LSTM output to class logits.\n",
    "        \n",
    "        Linear transformation from hidden_dim → output_dim:\n",
    "        - Takes sentence-level representation (final LSTM hidden state)\n",
    "        - Produces raw scores (logits) for each possible class\n",
    "        - Applied softmax during inference converts to class probabilities\n",
    "        \"\"\"\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Process input token sequence through embedding, LSTM, and classification layers.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input token indices of shape (seq_len, batch_size)\n",
    "                Each element x[i,j] is vocabulary index for token i in sequence j\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Classification logits of shape (batch_size, output_dim)\n",
    "                Raw scores for each class, typically passed through softmax for probabilities\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Convert token indices to dense embedding vectors.\n",
    "        \n",
    "        Dimension transformation: (seq_len, batch_size) → (seq_len, batch_size, embedding_dim)\n",
    "        \n",
    "        Process:\n",
    "        - Each token index x[i,j] gets replaced by embedding_matrix[x[i,j], :]\n",
    "        - Result: embedded[i,j,:] contains embedding_dim-dimensional vector for token i in sequence j\n",
    "        - Embeddings are learned parameters that capture semantic token relationships\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)  # (seq_len, batch_size, embedding_dim)\n",
    "        \n",
    "        \"\"\"\n",
    "        Process embedded sequence through LSTM to extract sequential patterns.\n",
    "        \n",
    "        LSTM returns tuple: (output_states, (final_hidden_state, final_cell_state))\n",
    "        \n",
    "        Dimension transformations:\n",
    "        - Input: (seq_len, batch_size, embedding_dim)\n",
    "        - output: (seq_len, batch_size, hidden_dim) - hidden states for ALL timesteps\n",
    "        - hidden: (1, batch_size, hidden_dim) - FINAL hidden state only\n",
    "        - cell: (1, batch_size, hidden_dim) - final cell state (discarded with _)\n",
    "        \n",
    "        Key insight: We use only final hidden state for classification, assuming it\n",
    "        contains sufficient information about the entire sequence for classification.\n",
    "        \"\"\"\n",
    "        output, (hidden, _) = self.lstm(embedded)  # output: (seq_len, batch_size, hidden_dim)\n",
    "                                                   # hidden: (1, batch_size, hidden_dim)\n",
    "        \n",
    "        \"\"\"\n",
    "        Generate classification logits from final LSTM hidden state.\n",
    "        \n",
    "        Dimension transformation: (1, batch_size, hidden_dim) → (batch_size, output_dim)\n",
    "        \n",
    "        Process:\n",
    "        1. hidden.squeeze(0): Remove first dimension (1) → (batch_size, hidden_dim)\n",
    "        2. self.fc(): Linear transformation → (batch_size, output_dim)\n",
    "        \n",
    "        The squeeze(0) removes the sequence length dimension from hidden state since\n",
    "        LSTM returns shape (num_layers, batch_size, hidden_dim) and we have 1 layer.\n",
    "        \"\"\"\n",
    "        logits = self.fc(hidden.squeeze(0))  # (batch_size, output_dim)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss: 0.4263986051082611\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Initialize and train PhraseClassifier model using CrossEntropyLoss and Adam optimizer.\n",
    "\n",
    "Model configuration:\n",
    "- vocab_size: len(vocab) - total number of unique tokens in vocabulary\n",
    "- embedding_dim=10: Compact 10-dimensional word embeddings for token representation\n",
    "- hidden_dim=20: Small LSTM hidden state size (20 units) for sequence processing\n",
    "- output_dim=3: Three classification classes (likely sentiment: negative/neutral/positive)\n",
    "\n",
    "Training setup optimized for small-scale text classification with minimal overfitting risk\n",
    "due to compact architecture and moderate learning rate.\n",
    "\"\"\"\n",
    "model = PhraseClassifier(len(vocab), embedding_dim=10, hidden_dim=20, output_dim=3)\n",
    "\n",
    "\"\"\"\n",
    "Configure CrossEntropyLoss for multi-class classification training.\n",
    "\n",
    "CrossEntropyLoss combines LogSoftmax and NLLLoss, making it ideal for classification:\n",
    "- Automatically applies softmax to model outputs (raw logits)\n",
    "- Computes negative log-likelihood loss for true class labels\n",
    "- Provides stable gradients for backpropagation in classification tasks\n",
    "- Expects raw logits from model (not probabilities) and integer class labels\n",
    "\n",
    "Mathematical formulation: Loss = -log(softmax(logits)[true_class])\n",
    "\"\"\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\"\"\"\n",
    "Initialize Adam optimizer for adaptive learning rate optimization.\n",
    "\n",
    "Adam configuration:\n",
    "- lr=0.001: Conservative learning rate suitable for small text classification models\n",
    "- Default momentum parameters (β₁=0.9, β₂=0.999) provide stable convergence\n",
    "- Adaptive per-parameter learning rates help with embedding layer training\n",
    "- Efficient for sparse gradients common in NLP tasks with vocabulary lookups\n",
    "\"\"\"\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\"\"\"\n",
    "Train model for 100 epochs using full-batch gradient descent.\n",
    "\n",
    "Training characteristics:\n",
    "- epochs=100: Sufficient iterations for small model convergence on text data\n",
    "- Full-batch training: Uses entire dataset per iteration (suitable for small datasets)\n",
    "- No data shuffling: May be suboptimal for larger datasets but acceptable here\n",
    "- Loss reporting every 100 epochs: Monitors training progress and convergence\n",
    "\"\"\"\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \"\"\"\n",
    "    Clear accumulated gradients from previous iteration.\n",
    "    Essential step preventing gradient accumulation across batches.\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    \"\"\"\n",
    "    Forward pass: compute model predictions for input sequences.\n",
    "    \n",
    "    inputs.t() transposes input tensor from (batch_size, seq_len) to (seq_len, batch_size)\n",
    "    to match LSTM's expected input format (since batch_first=False by default).\n",
    "    \n",
    "    Returns: predictions tensor of shape (batch_size, output_dim=3) containing\n",
    "    raw logits for each of the 3 classification classes.\n",
    "    \"\"\"\n",
    "    predictions = model(inputs.t())\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute CrossEntropyLoss between predictions and true labels.\n",
    "    \n",
    "    Arguments:\n",
    "    - predictions: (batch_size, 3) raw logits from model\n",
    "    - labels: (batch_size,) true class indices [0, 1, or 2]\n",
    "    \n",
    "    CrossEntropyLoss automatically applies softmax to predictions and computes\n",
    "    negative log-likelihood for the correct class.\n",
    "    \"\"\"\n",
    "    loss = criterion(predictions, labels)\n",
    "    \n",
    "    \"\"\"\n",
    "    Backward pass: compute gradients for all model parameters.\n",
    "    Uses automatic differentiation to compute ∂loss/∂parameter for:\n",
    "    - Embedding matrix weights\n",
    "    - LSTM gate parameters (input, forget, cell, output gates)\n",
    "    - Final linear layer weights and biases\n",
    "    \"\"\"\n",
    "    loss.backward()\n",
    "    \n",
    "    \"\"\"\n",
    "    Update model parameters using computed gradients and Adam optimizer.\n",
    "    Adam applies adaptive learning rates and momentum to each parameter\n",
    "    for stable and efficient convergence.\n",
    "    \"\"\"\n",
    "    optimizer.step()\n",
    "\n",
    "    \"\"\"\n",
    "    Monitor training progress by printing loss every 100 epochs.\n",
    "    loss.item() extracts scalar value from tensor for logging.\n",
    "    Expected behavior: loss should decrease over epochs indicating learning.\n",
    "    \"\"\"\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch: {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions: tensor([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Inference pipeline for testing trained PhraseClassifier on new text samples.\n",
    "\n",
    "This block demonstrates proper inference practices:\n",
    "1. Disable gradient computation for memory efficiency and speed\n",
    "2. Set model to evaluation mode to handle dropout/batch normalization appropriately\n",
    "3. Preprocess new text using same tokenization and padding as training\n",
    "4. Generate predictions and convert logits to class predictions\n",
    "\n",
    "The pipeline handles unknown words gracefully by mapping them to <PAD> tokens,\n",
    "ensuring robust inference on out-of-vocabulary terms.\n",
    "\"\"\"\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    \"\"\"\n",
    "    Set model to evaluation mode for proper inference behavior.\n",
    "    \n",
    "    model.eval() affects certain layers:\n",
    "    - Dropout layers: Disabled (no random neuron dropping)\n",
    "    - BatchNorm layers: Use running statistics instead of batch statistics\n",
    "    - Other layers: May have different behavior between training/eval modes\n",
    "    \n",
    "    Essential for consistent, deterministic predictions during inference.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    \"\"\"\n",
    "    Define test phrases for model evaluation.\n",
    "    These represent typical input text that the model should classify.\n",
    "    The model will predict sentiment/category for each phrase based on training.\n",
    "    \"\"\"\n",
    "    test_phrases = [\"incredible match\", \"newest gadget\", \"yummy cake\"]\n",
    "    \n",
    "    \"\"\"\n",
    "    Tokenize test phrases using training vocabulary with unknown word handling.\n",
    "    \n",
    "    Process for each phrase:\n",
    "    1. Split phrase into individual tokens (words)\n",
    "    2. Map each token to vocabulary index using vocab.get()\n",
    "    3. Unknown tokens default to <PAD> index for graceful degradation\n",
    "    4. Result: List of token indices representing each phrase\n",
    "    \n",
    "    Example: \"incredible match\" → [vocab[\"incredible\"], vocab[\"match\"]]\n",
    "    If \"incredible\" not in vocab → [vocab[\"<PAD>\"], vocab[\"match\"]]\n",
    "    \"\"\"\n",
    "    encoded_test_phrases = [\n",
    "        [vocab.get(word, vocab[\"<PAD>\"]) for word in phrase.split()] \n",
    "        for phrase in test_phrases\n",
    "    ]\n",
    "    \n",
    "    \"\"\"\n",
    "    Pad sequences to uniform length matching training data requirements.\n",
    "    \n",
    "    Padding process:\n",
    "    1. Calculate padding needed: max_length - current_phrase_length\n",
    "    2. Append <PAD> token indices to reach max_length\n",
    "    3. Ensures all sequences have identical length for batch processing\n",
    "    \n",
    "    Example: If max_length=5 and phrase has 2 tokens:\n",
    "    [token1, token2] → [token1, token2, <PAD>, <PAD>, <PAD>]\n",
    "    \"\"\"\n",
    "    padded_test_phrases = [\n",
    "        phrase + [vocab[\"<PAD>\"]] * (max_length - len(phrase)) \n",
    "        for phrase in encoded_test_phrases\n",
    "    ]\n",
    "    \n",
    "    \"\"\"\n",
    "    Convert preprocessed sequences to PyTorch tensor for model input.\n",
    "    \n",
    "    torch.LongTensor creates integer tensor suitable for embedding layer lookup.\n",
    "    Shape: (batch_size=3, seq_len=max_length) where each element is vocab index.\n",
    "    LongTensor is required for nn.Embedding which expects integer indices.\n",
    "    \"\"\"\n",
    "    test_inputs = torch.LongTensor(padded_test_phrases)\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate model predictions and extract most probable class for each input.\n",
    "    \n",
    "    Processing steps:\n",
    "    1. test_inputs.t(): Transpose (3, seq_len) → (seq_len, 3) for LSTM input format\n",
    "    2. model(...): Forward pass returns (3, output_dim=3) logits tensor\n",
    "    3. torch.argmax(..., dim=1): Find class index with highest logit per sample\n",
    "    4. Result: (3,) tensor with predicted class indices [0, 1, or 2]\n",
    "    \n",
    "    argmax converts raw logits to discrete class predictions:\n",
    "    [logit_class0, logit_class1, logit_class2] → class_with_highest_logit\n",
    "    \"\"\"\n",
    "    test_predictions = torch.argmax(model(test_inputs.t()), dim=1)\n",
    "    \n",
    "    \"\"\"\n",
    "    Display predicted class indices for interpretation.\n",
    "    \n",
    "    Output interpretation depends on training label mapping:\n",
    "    - 0: Negative sentiment\n",
    "    - 1: Neutral sentiment  \n",
    "    - 2: Positive sentiment\n",
    "    (or whatever class encoding was used during training)\n",
    "    \"\"\"\n",
    "    print(\"Test predictions:\", test_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
