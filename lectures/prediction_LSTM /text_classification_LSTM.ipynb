{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification using LSTM\n",
    "\n",
    "In this coding exercise, you will create a simple LSTM model using PyTorch to perform text classification on a dataset of short phrases. We will perform the following steps:\n",
    "\n",
    "- Create a vocabulary to represent words as indices.\n",
    "- Tokenize, encode, and pad the phrases.\n",
    "- Convert the phrases and categories to PyTorch tensors.\n",
    "- Instantiate the LSTM model with the vocabulary size, embedding dimensions, hidden dimensions, and output dimensions.\n",
    "- Define the loss function and optimizer.\n",
    "- Train the model for a number of epochs.\n",
    "- Test the model on new phrases and print the category predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Phrases (textual data) and their category labels (0 for sports, 1 for technology, 2 for food)\n",
    "This data is extremely less for realistically training an LSTM model. Feel free to use\n",
    "a relevant data source or create your own dummy data for this exercise.\n",
    "\n",
    "Phrases (textual data) and their category labels (0 for sports, 1 for technology, 2 for food)\n",
    "This model might overfit as the data is less. Feel free to use any other data source for training\n",
    "or create your own dummy data\n",
    "\"\"\"\n",
    "\n",
    "phrases = [\"great goal scored\", \"amazing touchdown\", \"new phone release\", \"latest laptop model\", \"tasty pizza\", \"delicious burger\"]\n",
    "categories = [0, 0, 1, 1, 2, 2]\n",
    "\n",
    "\"\"\"\n",
    "Create a vocabulary to represent words as indices\n",
    "\"\"\"\n",
    "vocab = {\"<PAD>\": 0, \"great\": 1, \"goal\": 2, \"scored\": 3, \"amazing\": 4, \"touchdown\": 5, \"new\": 6, \"phone\": 7, \"release\": 8, \"latest\": 9, \"laptop\": 10, \"model\": 11, \"tasty\": 12, \"pizza\": 13, \"delicious\": 14, \"burger\": 15}\n",
    "\n",
    "\"\"\"\n",
    "Tokenize, encode, and pad phrases\n",
    "\"\"\"\n",
    "encoded_phrases = [[vocab[word] for word in phrase.split()] for phrase in phrases]\n",
    "max_length = max([len(phrase) for phrase in encoded_phrases])\n",
    "padded_phrases = [phrase + [vocab[\"<PAD>\"]] * (max_length - len(phrase)) for phrase in encoded_phrases]\n",
    "\n",
    "\"\"\"\n",
    "Convert phrases and categories to PyTorch tensors\n",
    "\"\"\"\n",
    "inputs = torch.LongTensor(padded_phrases)\n",
    "labels = torch.LongTensor(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhraseClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple LSTM-based phrase classifier for text classification tasks.\n",
    "    \n",
    "    This model processes input sequences through an embedding layer, followed by an LSTM layer,\n",
    "    and finally a fully connected layer to produce classification logits. It's suitable for\n",
    "    tasks like sentiment analysis, topic classification, or any text classification problem.\n",
    "\n",
    "    Attributes:\n",
    "        embedding (nn.Embedding): Converts token indices to dense embeddings\n",
    "        lstm (nn.LSTM): Processes the embedded sequences using LSTM\n",
    "        fc (nn.Linear): Final fully connected layer for classification\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the vocabulary\n",
    "        embedding_dim (int): Dimension of the word embeddings\n",
    "        hidden_dim (int): Number of features in the LSTM hidden state\n",
    "        output_dim (int): Number of output classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Initializes the PhraseClassifier with the specified dimensions.\n",
    "        \"\"\"\n",
    "        super(PhraseClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (seq_len, batch_size) containing token indices\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)  # (seq_len, batch_size, embedding_dim)\n",
    "        output, (hidden, _) = self.lstm(embedded)  # output: (seq_len, batch_size, hidden_dim)\n",
    "                                                   # hidden: (1, batch_size, hidden_dim)\n",
    "        logits = self.fc(hidden.squeeze(0))  # (batch_size, output_dim)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss: 0.34661993384361267\n"
     ]
    }
   ],
   "source": [
    "model = PhraseClassifier(len(vocab), embedding_dim=10, hidden_dim=20, output_dim=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(inputs.t())\n",
    "    loss = criterion(predictions, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch: {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions: tensor([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Inference block for testing trained LSTM model on new phrases\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    model.eval()  # Set model to evaluation mode (affects dropout, batch norm etc.)\n",
    "    \n",
    "    # Sample phrases for testing model predictions\n",
    "    test_phrases = [\"incredible match\", \"newest gadget\", \"yummy cake\"]\n",
    "    \n",
    "    # Tokenize and encode phrases using vocabulary\n",
    "    # Uses vocab.get() with fallback to <PAD> for unknown words\n",
    "    encoded_test_phrases = [\n",
    "        [vocab.get(word, vocab[\"<PAD>\"]) for word in phrase.split()] \n",
    "        for phrase in test_phrases\n",
    "    ]\n",
    "    \n",
    "    # Pad sequences to match training data length\n",
    "    padded_test_phrases = [\n",
    "        phrase + [vocab[\"<PAD>\"]] * (max_length - len(phrase)) \n",
    "        for phrase in encoded_test_phrases\n",
    "    ]\n",
    "    \n",
    "    # Convert to PyTorch tensor (shape: [batch_size, seq_len])\n",
    "    test_inputs = torch.LongTensor(padded_test_phrases)\n",
    "    \n",
    "    # Get model predictions (transpose inputs to [seq_len, batch_size])\n",
    "    # torch.argmax gets the class with highest probability\n",
    "    test_predictions = torch.argmax(model(test_inputs.t()), dim=1)\n",
    "    \n",
    "    # Print predicted class indices\n",
    "    print(\"Test predictions:\", test_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aipnd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
