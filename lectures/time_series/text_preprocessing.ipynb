{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/chaklader/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chaklader/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/chaklader/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/chaklader/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK data (run once)\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Next, we need to load the data that we want to preprocess. In this example, we will use the following sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The quick brown fox jumped over the lazy dog.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization\n",
    "\n",
    "Text normalization is the process of converting text into a standard format. This involves converting all characters to lowercase and removing any punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase\n",
    "text = text.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "text = ''.join(c for c in text if c not in '.,;:-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization is the process of splitting a sentence into individual words or tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal\n",
    "\n",
    "Stopwords are common words that do not carry much meaning and can be removed from the text. We will use NLTK's list of stopwords and remove them from the tokenized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming is the process of reducing a word to its base or root form. We will use Porter stemmer from NLTK for stemming.\n",
    "\n",
    "### Lemmatization\n",
    "\n",
    "Lemmatization is the process of converting a word to its base or dictionary form. We will use WordNet lemmatizer from NLTK for lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n",
      "['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog']\n",
      "============================================================\n",
      "STEMMING vs LEMMATIZATION COMPARISON\n",
      "============================================================\n",
      "Original Word   Stemmed         Lemmatized      Difference\n",
      "-----------------------------------------------------------------\n",
      "running         run             running         ✗ Different\n",
      "runs            run             run             ✓ Same\n",
      "ran             ran             ran             ✓ Same\n",
      "better          better          better          ✓ Same\n",
      "best            best            best            ✓ Same\n",
      "good            good            good            ✓ Same\n",
      "mice            mice            mouse           ✗ Different\n",
      "mouse           mous            mouse           ✗ Different\n",
      "feet            feet            foot            ✗ Different\n",
      "foot            foot            foot            ✓ Same\n",
      "children        children        child           ✗ Different\n",
      "child           child           child           ✓ Same\n",
      "beautiful       beauti          beautiful       ✗ Different\n",
      "beautifully     beauti          beautifully     ✗ Different\n",
      "connection      connect         connection      ✗ Different\n",
      "connected       connect         connected       ✗ Different\n",
      "connecting      connect         connecting      ✗ Different\n",
      "happiness       happi           happiness       ✗ Different\n",
      "happy           happi           happy           ✗ Different\n",
      "happier         happier         happier         ✓ Same\n",
      "studies         studi           study           ✗ Different\n",
      "studying        studi           studying        ✗ Different\n",
      "studied         studi           studied         ✗ Different\n",
      "\n",
      "============================================================\n",
      "ADVANCED LEMMATIZATION WITH POS TAGS\n",
      "============================================================\n",
      "Word         POS   Default Lemma   POS-aware Lemma    Difference\n",
      "----------------------------------------------------------------------\n",
      "better       a     better          good               ✗ Different\n",
      "better       r     better          well               ✗ Different\n",
      "running      v     running         run                ✗ Different\n",
      "running      n     running         running            ✓ Same\n",
      "leaves       n     leaf            leaf               ✓ Same\n",
      "leaves       v     leaf            leave              ✗ Different\n",
      "\n",
      "============================================================\n",
      "SENTENCE PROCESSING COMPARISON\n",
      "============================================================\n",
      "Original:    The children were running faster and played better games\n",
      "Tokens:      ['The', 'children', 'were', 'running', 'faster', 'and', 'played', 'better', 'games']\n",
      "Stemmed:     ['the', 'children', 'were', 'run', 'faster', 'and', 'play', 'better', 'game']\n",
      "Lemmatized:  ['The', 'child', 'were', 'running', 'faster', 'and', 'played', 'better', 'game']\n",
      "Stemmed text:     the children were run faster and play better game\n",
      "Lemmatized text:  The child were running faster and played better game\n",
      "\n",
      "============================================================\n",
      "KEY DIFFERENCES SUMMARY\n",
      "============================================================\n",
      "\n",
      "STEMMING (Porter Stemmer):\n",
      "✓ Fast and simple rule-based approach\n",
      "✓ Language-independent algorithm  \n",
      "✓ Good for information retrieval and search\n",
      "✗ May produce non-dictionary words (e.g., 'beauti' from 'beautiful')\n",
      "✗ Can over-stem or under-stem words\n",
      "✗ No understanding of word meaning or context\n",
      "\n",
      "LEMMATIZATION (WordNet):\n",
      "✓ Produces valid dictionary words (lemmas)\n",
      "✓ Uses linguistic knowledge and morphology\n",
      "✓ More accurate for semantic analysis\n",
      "✓ Handles irregular word forms correctly\n",
      "✗ Slower due to dictionary lookups\n",
      "✗ Requires language-specific resources\n",
      "✗ May need POS tags for optimal accuracy\n",
      "\n",
      "WHEN TO USE WHICH:\n",
      "- Stemming: Information retrieval, search engines, quick preprocessing\n",
      "- Lemmatization: Semantic analysis, sentiment analysis, text classification\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(stemmed_tokens)\n",
    "\n",
    "# Perform lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(lemmatized_tokens)\n",
    "\n",
    "##################################\n",
    "\n",
    "def demonstrate_stemming_vs_lemmatization(words_list: list) -> None:\n",
    "    \"\"\"\n",
    "    Compare stemming and lemmatization results side by side for educational purposes.\n",
    "    \n",
    "    This function demonstrates the key differences between stemming and lemmatization\n",
    "    by processing the same set of words through both techniques and displaying\n",
    "    results in a formatted table for easy comparison.\n",
    "    \n",
    "    Stemming uses rule-based suffix removal to reduce words to approximate roots,\n",
    "    while lemmatization uses dictionary lookups and morphological analysis to find\n",
    "    actual dictionary word forms (lemmas).\n",
    "    \n",
    "    Args:\n",
    "        words_list (list): List of words to process through both stemming and lemmatization.\n",
    "            Should include various word forms (plurals, verb forms, adjectives) to\n",
    "            showcase the differences between the two approaches.\n",
    "    \n",
    "    Returns:\n",
    "        None: Prints formatted comparison table to console.\n",
    "        \n",
    "    Example:\n",
    "        >>> test_words = [\"running\", \"better\", \"mice\", \"feet\"]\n",
    "        >>> demonstrate_stemming_vs_lemmatization(test_words)\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Initialize both stemming and lemmatization tools.\n",
    "    PorterStemmer: Rule-based algorithm that removes common suffixes\n",
    "    WordNetLemmatizer: Dictionary-based tool that finds proper word forms\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    print(f\"{'Original Word':<15} {'Stemmed':<15} {'Lemmatized':<15} {'Difference'}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for word in words_list:\n",
    "        \"\"\"\n",
    "        Apply both stemming and lemmatization to each word for comparison.\n",
    "        Stemming uses algorithmic rules, lemmatization uses linguistic knowledge.\n",
    "        \"\"\"\n",
    "        stemmed = stemmer.stem(word)\n",
    "        lemmatized = lemmatizer.lemmatize(word)\n",
    "        \n",
    "        # Highlight when results differ\n",
    "        difference = \"✓ Same\" if stemmed == lemmatized else \"✗ Different\"\n",
    "        \n",
    "        print(f\"{word:<15} {stemmed:<15} {lemmatized:<15} {difference}\")\n",
    "\n",
    "def advanced_lemmatization_demo(words_with_pos: list) -> None:\n",
    "    \"\"\"\n",
    "    Demonstrate advanced lemmatization with part-of-speech (POS) tagging for accuracy.\n",
    "    \n",
    "    Lemmatization accuracy improves significantly when provided with grammatical\n",
    "    context through POS tags. The same word can have different lemmas depending\n",
    "    on whether it's used as a noun, verb, adjective, etc.\n",
    "    \n",
    "    This function shows how specifying POS tags leads to more accurate lemmatization\n",
    "    results compared to using default POS assumptions.\n",
    "    \n",
    "    Args:\n",
    "        words_with_pos (list): List of tuples containing (word, pos_tag) pairs.\n",
    "            POS tags use WordNet format: 'n' (noun), 'v' (verb), 'a' (adjective), 'r' (adverb).\n",
    "            \n",
    "    Returns:\n",
    "        None: Prints comparison of lemmatization with and without POS context.\n",
    "        \n",
    "    Example:\n",
    "        >>> words = [(\"better\", \"a\"), (\"better\", \"r\"), (\"running\", \"v\"), (\"running\", \"n\")]\n",
    "        >>> advanced_lemmatization_demo(words)\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    print(f\"{'Word':<12} {'POS':<5} {'Default Lemma':<15} {'POS-aware Lemma':<18} {'Difference'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for word, pos in words_with_pos:\n",
    "        \"\"\"\n",
    "        Compare lemmatization results with and without POS information.\n",
    "        Default lemmatization assumes noun form, POS-aware uses grammatical context.\n",
    "        \"\"\"\n",
    "        default_lemma = lemmatizer.lemmatize(word)\n",
    "        pos_aware_lemma = lemmatizer.lemmatize(word, pos=pos)\n",
    "        \n",
    "        difference = \"✓ Same\" if default_lemma == pos_aware_lemma else \"✗ Different\"\n",
    "        \n",
    "        print(f\"{word:<12} {pos:<5} {default_lemma:<15} {pos_aware_lemma:<18} {difference}\")\n",
    "\n",
    "def process_sentence_comparison(sentence: str) -> None:\n",
    "    \"\"\"\n",
    "    Process an entire sentence through both stemming and lemmatization pipelines.\n",
    "    \n",
    "    This function demonstrates how stemming and lemmatization affect real text\n",
    "    processing by applying both techniques to a complete sentence. It shows how\n",
    "    the choice between methods impacts the final processed output that would be\n",
    "    used in downstream NLP tasks.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): Input sentence to process through both methods.\n",
    "            Should contain various word forms to illustrate differences.\n",
    "            \n",
    "    Returns:\n",
    "        None: Prints original sentence and processed versions for comparison.\n",
    "        \n",
    "    Example:\n",
    "        >>> process_sentence_comparison(\"The children were running quickly\")\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Initialize processing tools for both approaches.\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    \"\"\"\n",
    "    Tokenize sentence into individual words for processing.\n",
    "    Both stemming and lemmatization operate on individual tokens.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(sentence)\n",
    "    \n",
    "    \"\"\"\n",
    "    Apply stemming: rule-based suffix removal for approximate root forms.\n",
    "    Fast but may produce non-dictionary words (over-stemming or under-stemming).\n",
    "    \"\"\"\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    \"\"\"\n",
    "    Apply lemmatization: dictionary-based reduction to proper word forms.\n",
    "    Slower but produces valid dictionary words (lemmas).\n",
    "    \"\"\"\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    print(f\"Original:    {sentence}\")\n",
    "    print(f\"Tokens:      {tokens}\")\n",
    "    print(f\"Stemmed:     {stemmed_tokens}\")\n",
    "    print(f\"Lemmatized:  {lemmatized_tokens}\")\n",
    "    print(f\"Stemmed text:     {' '.join(stemmed_tokens)}\")\n",
    "    print(f\"Lemmatized text:  {' '.join(lemmatized_tokens)}\")\n",
    "\n",
    "\n",
    "# Demonstration Examples\n",
    "print(\"=\" * 60)\n",
    "print(\"STEMMING vs LEMMATIZATION COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\"\"\"\n",
    "Test words chosen to highlight key differences between stemming and lemmatization:\n",
    "- Irregular plurals (mice, feet, children)\n",
    "- Verb forms (running, better used as verb)\n",
    "- Comparative/superlative adjectives (better, worst)\n",
    "- Words where stemming may over-reduce (beautiful, connection)\n",
    "\"\"\"\n",
    "test_words = [\n",
    "    \"running\", \"runs\", \"ran\",\n",
    "    \"better\", \"best\", \"good\",\n",
    "    \"mice\", \"mouse\", \"feet\", \"foot\",\n",
    "    \"children\", \"child\",\n",
    "    \"beautiful\", \"beautifully\",\n",
    "    \"connection\", \"connected\", \"connecting\",\n",
    "    \"happiness\", \"happy\", \"happier\",\n",
    "    \"studies\", \"studying\", \"studied\"\n",
    "]\n",
    "\n",
    "demonstrate_stemming_vs_lemmatization(test_words)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ADVANCED LEMMATIZATION WITH POS TAGS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\"\"\"\n",
    "Demonstrate how POS context improves lemmatization accuracy.\n",
    "Same words can have different lemmas based on grammatical role.\n",
    "\"\"\"\n",
    "words_with_pos = [\n",
    "    (\"better\", \"a\"),    # adjective: better -> good\n",
    "    (\"better\", \"r\"),    # adverb: better -> well  \n",
    "    (\"running\", \"v\"),   # verb: running -> run\n",
    "    (\"running\", \"n\"),   # noun: running -> running\n",
    "    (\"leaves\", \"n\"),    # noun: leaves -> leaf\n",
    "    (\"leaves\", \"v\"),    # verb: leaves -> leave\n",
    "]\n",
    "\n",
    "advanced_lemmatization_demo(words_with_pos)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SENTENCE PROCESSING COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\"\"\"\n",
    "Real-world example showing how both methods affect sentence processing.\n",
    "\"\"\"\n",
    "sample_sentence = \"The children were running faster and played better games\"\n",
    "process_sentence_comparison(sample_sentence)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY DIFFERENCES SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "STEMMING (Porter Stemmer):\n",
    "✓ Fast and simple rule-based approach\n",
    "✓ Language-independent algorithm  \n",
    "✓ Good for information retrieval and search\n",
    "✗ May produce non-dictionary words (e.g., 'beauti' from 'beautiful')\n",
    "✗ Can over-stem or under-stem words\n",
    "✗ No understanding of word meaning or context\n",
    "\n",
    "LEMMATIZATION (WordNet):\n",
    "✓ Produces valid dictionary words (lemmas)\n",
    "✓ Uses linguistic knowledge and morphology\n",
    "✓ More accurate for semantic analysis\n",
    "✓ Handles irregular word forms correctly\n",
    "✗ Slower due to dictionary lookups\n",
    "✗ Requires language-specific resources\n",
    "✗ May need POS tags for optimal accuracy\n",
    "\n",
    "WHEN TO USE WHICH:\n",
    "- Stemming: Information retrieval, search engines, quick preprocessing\n",
    "- Lemmatization: Semantic analysis, sentiment analysis, text classification\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output the results\n",
    "\n",
    "Finally, we will output the results of each step of the text preprocessing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:  the quick brown fox jumped over the lazy dog\n",
      "Tokenized text:  ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
      "Filtered tokens:  ['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog']\n",
      "Stemmed tokens:  ['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n",
      "Lemmatized tokens:  ['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original text: \", text)\n",
    "print(\"Tokenized text: \", tokens)\n",
    "print(\"Filtered tokens: \", filtered_tokens)\n",
    "print(\"Stemmed tokens: \", stemmed_tokens)\n",
    "print(\"Lemmatized tokens: \", lemmatized_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
