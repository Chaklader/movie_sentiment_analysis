{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T16:22:34.240913Z",
     "start_time": "2023-04-12T16:22:34.095776Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T16:22:34.293026Z",
     "start_time": "2023-04-12T16:22:34.108507Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sentences (textual data) and their sentiment labels (1 for positive, 0 for negative)\n",
    "sentences = [\"i love this movie\", \"this film is amazing\", \"i didn't like it\", \"it was terrible\"]\n",
    "sentiment = [1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T16:22:34.293614Z",
     "start_time": "2023-04-12T16:22:34.119452Z"
    }
   },
   "outputs": [],
   "source": [
    "# Simple vocabulary to represent words as indices\n",
    "vocab = {\"<PAD>\": 0, \"i\": 1, \"love\": 2, \"this\": 3, \"movie\": 4, \"film\": 5, \"is\": 6, \"amazing\": 7, \"didn't\": 8, \"like\": 9, \"it\": 10, \"was\": 11, \"terrible\": 12}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a simple vocabulary to represent words as indices. This allows us to convert words in our sentences to numbers, which can be fed as input to our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize, encode and pad sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T16:22:34.295464Z",
     "start_time": "2023-04-12T16:22:34.136556Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_sentences = [[vocab[word] for word in sentence.split()] for sentence in sentences]\n",
    "max_length = max([len(sentence) for sentence in encoded_sentences])\n",
    "padded_sentences = [sentence + [vocab[\"<PAD>\"]] * (max_length - len(sentence)) for sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenize and encode the sentences using the vocabulary created earlier. We also pad the sentences with the `<PAD>` token to make them all the same length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert data to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T16:22:34.297533Z",
     "start_time": "2023-04-12T16:22:34.154982Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = torch.LongTensor(padded_sentences)\n",
    "labels = torch.FloatTensor(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the input data and labels to PyTorch tensors. Inputs are converted to LongTensors, while labels are converted to FloatTensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T16:22:34.297819Z",
     "start_time": "2023-04-12T16:22:34.168771Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple LSTM model for sentiment analysis.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, _) = self.lstm(embedded)\n",
    "        logits = self.fc(hidden.squeeze(0))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a simple LSTM model class that inherits from `nn.Module`. The model consists of an embedding layer, an LSTM layer, and a fully connected (linear) layer. The forward method takes an input tensor `x`, passes it through the embedding layer, the LSTM layer, and finally the fully connected layer to produce the output logits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate model and define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T16:22:34.322481Z",
     "start_time": "2023-04-12T16:22:34.174251Z"
    }
   },
   "outputs": [],
   "source": [
    "model = SimpleLSTM(len(vocab), embedding_dim=10, hidden_dim=20, output_dim=1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the LSTM model with the vocabulary size, embedding dimensions, hidden dimensions, and output dimensions. We also define the binary cross-entropy with logits loss (`BCEWithLogitsLoss`) and the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentiment_data(sentences, sentiment, vocab):\n",
    "    \"\"\"\n",
    "    Prepare sentiment analysis data for LSTM model training.\n",
    "\n",
    "    This function takes raw sentences and their corresponding sentiment labels,\n",
    "    converts them to numerical format, pads sequences to equal length, and\n",
    "    prepares them in the format expected by PyTorch's LSTM.\n",
    "\n",
    "    Args:\n",
    "        sentences (list of str): List of input sentences to be processed\n",
    "        sentiment (list of int): List of sentiment labels (0 for negative, 1 for positive)\n",
    "        vocab (dict): Vocabulary mapping words to their corresponding indices\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - inputs (torch.LongTensor): Padded and encoded input sequences \n",
    "              with shape (seq_len, batch_size)\n",
    "            - labels (torch.FloatTensor): Tensor of sentiment labels with shape (batch_size,)\n",
    "    \"\"\"\n",
    "    # Tokenize and encode the sentences\n",
    "    encoded_sentences = [[vocab[word] for word in sentence.split()] for sentence in sentences]\n",
    "\n",
    "    # Find the maximum sequence length\n",
    "    max_length = max(len(sentence) for sentence in encoded_sentences)\n",
    "\n",
    "    # Pad sequences to the same length\n",
    "    padded_sentences = [sentence + [vocab[\"<PAD>\"]] * (max_length - len(sentence)) \n",
    "                       for sentence in encoded_sentences]\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    inputs = torch.LongTensor(padded_sentences)\n",
    "    labels = torch.FloatTensor(sentiment)\n",
    "\n",
    "    # Transpose inputs to match LSTM's expected input shape (seq_len, batch_size, input_size)\n",
    "    inputs = inputs.t()\n",
    "    \n",
    "    return inputs, labels\n",
    "\n",
    "# Example usage:\n",
    "inputs, labels = prepare_sentiment_data(sentences, sentiment, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T16:22:35.864714Z",
     "start_time": "2023-04-12T16:22:34.207066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss: 0.0011476636864244938\n",
      "Epoch: 200, Loss: 0.0009985864162445068\n",
      "Epoch: 300, Loss: 0.0008769434643909335\n",
      "Epoch: 400, Loss: 0.0007760371081531048\n",
      "Epoch: 500, Loss: 0.0006912948447279632\n",
      "Epoch: 600, Loss: 0.0006192974396981299\n",
      "Epoch: 700, Loss: 0.0005576762487180531\n",
      "Epoch: 800, Loss: 0.0005042588454671204\n",
      "Epoch: 900, Loss: 0.0004578128573484719\n",
      "Epoch: 1000, Loss: 0.0004170317552052438\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(inputs.t()).squeeze(1)\n",
    "    loss = criterion(predictions, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch: {epoch + 1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model for 1000 epochs. In each epoch, we:\n",
    "\n",
    "- Reset the gradients by calling optimizer.zero_grad()\n",
    "- Get the model's predictions for the input sentences by calling model(inputs.t()).squeeze(1)\n",
    "- Calculate the loss between the predictions and the true labels using the criterion defined earlier\n",
    "- Perform backpropagation by calling loss.backward()\n",
    "- Update the model's parameters by calling optimizer.step()\n",
    "- We also print the loss every 100 epochs for monitoring the training progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T16:22:35.900667Z",
     "start_time": "2023-04-12T16:22:35.865321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions: tensor([0.9949, 0.0283])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_sentences = [\"i love this film\", \"it was terrible\"]\n",
    "    encoded_test_sentences = [[vocab[word] for word in sentence.split()] for sentence in test_sentences]\n",
    "    padded_test_sentences = [sentence + [vocab[\"<PAD>\"]] * (max_length - len(sentence)) for sentence in encoded_test_sentences]\n",
    "    test_inputs = torch.LongTensor(padded_test_sentences)\n",
    "    test_predictions = torch.sigmoid(model(test_inputs.t()).squeeze(1))\n",
    "    print(\"Test predictions:\", test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the model on two new sentences. First, we tokenize, encode, and pad the test sentences in the same way as we did for the training sentences. We then convert the test sentences to PyTorch tensors and pass them through the model. We apply the sigmoid function to the output logits to obtain the final predictions, which represent the probability of each sentence being positive.\n",
    "\n",
    "The resulting `test_predictions` tensor contains the model's sentiment predictions for the given test sentences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aipnd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
