# RNNs and Transformers

# C-1: Intro to RNN

Recurrent Neural Networks (RNNs) are a class of artificial neural networks designed specifically for processing
sequential data. Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of
inputs. RNNs are ideal for Sequential Data Processing such as :

- Natural Language Processing
- Time Series Analysis
- Speech Recognition
- DNA Sequence Analysis
- Video Processing

#### Basic RNN Architecture

The fundamental RNN equation:

$$h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$$ $$y_t = W_{hy}h_t + b_y$$

Where:

- $h_t$: Hidden state at time t
- $x_t$: Input at time t
- $y_t$: Output at time t
- $W$: Weight matrices
- $b$: Bias terms
- $\tanh$: Hyperbolic tangent activation function

## 3. Types of RNNs

### 3.1 One-to-One

- Standard neural network
- Single input → Single output

### 3.2 One-to-Many

- Image captioning
- Single input → Sequence output

### 3.3 Many-to-One

- Sentiment analysis
- Sequence input → Single output

### 3.4 Many-to-Many

- Machine translation
- Sequence input → Sequence output

## 4. Training RNNs

### 4.1 Backpropagation Through Time (BPTT)

The loss function is:

$$L = \sum L_t(y_t, y_{t_{target}})$$

Where:

- $L_t$: Loss at time step t
- $y_t$: Predicted output
- $y_{t_{target}}$: Target output

### 4.2 Gradient Problems

#### Vanishing Gradients

- Problem: Gradients become tiny as they're propagated back through time
- Caused by repeated multiplication of small numbers
- Makes learning long-term dependencies difficult

#### Exploding Gradients

- Problem: Gradients become extremely large
- Solution: Gradient clipping

$$\text{if } \|\nabla\| > \text{threshold}: \nabla = \text{threshold} \cdot \frac{\nabla}{\|\nabla\|}$$

$
if ||gradient|| > threshold:
    gradient = (threshold * gradient) / ||gradient||
$

## 5. Advanced RNN Architectures

### 5.1 LSTM (Long Short-Term Memory)

Key equations:

$$f_t = \sigma(W_f[h_{t-1}, x_t] + b_f)$$ $$i_t = \sigma(W_i[h_{t-1}, x_t] + b_i)$$
$$\tilde{c}_t = \tanh(W_c[h_{t-1}, x_t] + b_c)$$ $$c_t = f_t \cdot c_{t-1} + i_t \cdot \tilde{c}_t$$
$$o_t = \sigma(W_o[h_{t-1}, x_t] + b_o)$$ $$h_t = o_t \cdot \tanh(c_t)$$

Where:

- $f_t$: Forget gate
- $i_t$: Input gate
- $c_t$: Cell state
- $o_t$: Output gate
- $\sigma$: Sigmoid function

### 5.2 GRU (Gated Recurrent Unit)

Simplified version of LSTM with fewer parameters:

$$z_t = \sigma(W_z[h_{t-1}, x_t] + b_z)$$ $$r_t = \sigma(W_r[h_{t-1}, x_t] + b_r)$$
$$\tilde{h}_t = \tanh(W_h[r_t \cdot h_{t-1}, x_t] + b_h)$$ $$h_t = (1 - z_t) \cdot h_{t-1} + z_t \cdot \tilde{h}_t$$

## 6. Practical Applications

### 6.1 Language Modeling

- Predicting next word in sequence
- Character-level text generation

### 6.2 Machine Translation

- Encoder-decoder architecture
- Attention mechanisms

### 6.3 Speech Recognition

- Converting audio signals to text
- Real-time processing

## 7. Best Practices

### 7.1 Architecture Selection

- Use LSTM/GRU for long sequences
- Bidirectional RNNs for context in both directions
- Stack multiple layers for complex tasks

### 7.2 Optimization

- Proper learning rate selection
- Gradient clipping
- Batch normalization
- Dropout for regularization

## 8. Implementation Tips

```textmate
# Basic PyTorch RNN implementation
class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleRNN, self).__init__()
        self.hidden_size = hidden_size
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        rnn_out, hidden = self.rnn(x)
        output = self.fc(rnn_out[:, -1, :])
        return output
```

## 9. Common Challenges and Solutions

### 9.1 Memory Requirements

- Use truncated BPTT
- Implement batch processing
- Apply sequence bucketing

### 9.2 Performance Optimization

- Use GPU acceleration
- Implement attention mechanisms
- Apply layer normalization

## 10. Recent Developments

### 10.1 Attention Mechanisms

$$\text{Attention} = \text{softmax}(QK^T/\sqrt{d})V$$

Where:

- $Q$: Query matrix
- $K$: Key matrix
- $V$: Value matrix
- $d$: Dimension of keys

### 10.2 Transformers

- Self-attention based architecture
- Parallel processing capability
- State-of-the-art performance

## Summary

RNNs are powerful architectures for sequential data processing, with various specialized versions (LSTM, GRU) addressing
specific challenges. Understanding their mathematics, architecture, and practical implementation is crucial for
successful applications in real-world problems. As mentioned in this video, RNNs have a key flaw, as capturing
relationships that span more than 8 or 10 steps back is practically impossible. This flaw stems from the "vanishing
gradient" problem in which the contribution of information decays geometrically over time.

As you may recall, while training our network, we use backpropagation. In the backpropagation process, we adjust our
weight matrices using a gradient. In the process, gradients are calculated by continuous multiplications of derivatives.
The value of these derivatives may be so small that these continuous multiplications may cause the gradient to
practically "vanish." LSTM is one option to overcome the Vanishing Gradient problem in RNNs. As mentioned in the video,
Long Short-Term Memory Cells (LSTMs) and Gated Recurrent Units (GRUs) give a solution to the vanishing gradient problem
by helping us apply networks that have temporal dependencies.

# The Vanishing Gradient Problem in Neural Networks

## Introduction

The vanishing gradient problem is a fundamental challenge in training deep neural networks using gradient-based learning
methods and backpropagation. During training, each neural network weight is updated proportionally to the partial
derivative of the loss function with respect to the current weight. As networks become deeper or sequence lengths
increase, gradient magnitudes tend to decrease exponentially, severely slowing or halting learning.

## Prototypical Models

### 1. Recurrent Network Model

Basic evolution equation: $$(h_t, x_t) = F(h_{t-1}, u_t, \theta)$$

Where:

- $h_t$: Hidden states
- $x_t$: Outputs
- $u_t$: Inputs
- $\theta$: Parameters

#### Example: RNN with Sigmoid Activation

Network definition: $$x_t = F(x_{t-1}, u_t, \theta) = W_{rec}\sigma(x_{t-1}) + W_{in}u_t + b$$

Gradient analysis: $$\nabla_x F(x_{t-1}, u_t, \theta) = W_{rec}diag(\sigma'(x_{t-1}))$$

### 2. Dynamical Systems Model

One-neuron recurrent network with sigmoid activation: $$\frac{dx}{dt} = -x(t) + \sigma(wx(t) + b) + w'u(t)$$

For autonomous case ($u = 0$), stable points are: $$\left(x, \ln\left(\frac{x}{1-x}\right)-5x\right)$$

### 3. Geometric Model

Loss function example: $$L(x(T)) = (0.855 - x(T))^2$$

## Solutions

### 1. LSTM Architecture

Key equations:

$$f_t = \sigma(W_f[h_{t-1}, x_t] + b_f)$$ $$i_t = \sigma(W_i[h_{t-1}, x_t] + b_i)$$
$$\tilde{c}_t = \tanh(W_c[h_{t-1}, x_t] + b_c)$$ $$c_t = f_t \cdot c_{t-1} + i_t \cdot \tilde{c}_t$$
$$o_t = \sigma(W_o[h_{t-1}, x_t] + b_o)$$ $$h_t = o_t \cdot \tanh(c_t)$$

Where:

- $f_t$: Forget gate
- $i_t$: Input gate
- $c_t$: Cell state
- $o_t$: Output gate
- $\sigma$: Sigmoid function

### 2. Batch Normalization

For mini-batch: $$\hat{x} = \frac{x - E[x]}{\sqrt{Var[x] + \epsilon}}$$ $$y = \gamma\hat{x} + \beta$$

### 3. Residual Connections

Function transformation: $$x \mapsto f(x) + x$$

Gradient becomes: $$\nabla f + I$$

### 4. Weight Initialization

For logistic activation function:

- Gaussian distribution with mean = 0
- Standard deviation = $\frac{3.6}{\sqrt{N}}$ where N is neurons per layer

### 5. Gradient Clipping

For exploding gradients: $$g_{clipped} = g_{max}\frac{g}{\|g\|} \text{ if } \|g\| > g_{max}$$

## Advanced Considerations

1. **Deep Belief Networks**: Pre-training through unsupervised learning followed by supervised fine-tuning

2. **Alternative Activation Functions**: ReLU and variants that reduce gradient vanishing

3. **Multi-level Hierarchy**: Layer-wise pre-training through unsupervised learning

Would you like me to expand on any of these sections or provide more detailed mathematical derivations?

# Understanding LSTM and GRU

## What's the Problem They Solve?

Imagine you're reading a book. By the time you reach page 100, you might forget some details from page 1. Regular RNNs
have this same problem - they struggle to remember information from far back. LSTM and GRU are like giving our neural
network a notepad to write down important things to remember.

## Long Short-Term Memory (LSTM)

Think of LSTM as a smart safe box with three keys:

1. **Forget Gate** (First Key)

    - Decides what to throw away
    - Like cleaning your room: "Do I still need this?"

2. **Input Gate** (Second Key)

    - Decides what new information to save
    - Like taking notes: "This is important, let me write it down"

3. **Output Gate** (Third Key)
    - Decides what to share/use
    - Like deciding what to tell someone: "From everything I know, here's what's relevant"

### Real-world Analogy

Imagine you're watching a TV series:

- Forget Gate: Forgetting irrelevant subplots
- Input Gate: Remembering major plot points
- Output Gate: Deciding what to tell your friend about the show

## Gated Recurrent Unit (GRU)

GRU is like LSTM's younger sibling - simpler but still effective. Instead of three keys, it has two:

1. **Update Gate**

    - Decides how much of the old information to keep
    - Like updating your to-do list: combining old and new tasks

2. **Reset Gate**
    - Decides how much past information to forget
    - Like starting a new page in your notebook

### Real-world Analogy

Think of it as taking notes in class:

- Update Gate: Deciding whether to add to your existing notes or start fresh
- Reset Gate: Choosing when to start a new topic

## When to Use Which?

### Use LSTM When:

- You need to remember things for a very long time
- You have lots of computing power
- Your task is complex (like language translation)

### Use GRU When:

- You need something simpler and faster
- You have limited computing resources
- Your task is relatively straightforward

## Simple Example in Code

```textmate
# Simple LSTM-like pseudocode
def LSTM(input):
    forget = decide_what_to_forget(input)
    store = decide_what_to_store(input)
    output = decide_what_to_output(input)

    memory = update_memory(forget, store)
    return output_filtered_memory(memory, output)

# Simple GRU-like pseudocode
def GRU(input):
    update = decide_update_level(input)
    reset = decide_reset_level(input)

    new_memory = combine_memory(update, reset, input)
    return new_memory
```

## Practical Applications

1. **Text Generation**

    - Writing assistant
    - Auto-completion

2. **Translation**

    - Google Translate
    - Real-time translation apps

3. **Speech Recognition**

    - Siri
    - Alexa

4. **Music Generation**
    - Creating melodies
    - Continuing musical patterns

# Technical Deep-Dive into LSTM and GRU

## Long Short-Term Memory (LSTM)

LSTMs revolutionized sequence modeling by introducing a sophisticated gating mechanism that controls information flow
through the network. The architecture maintains two states: a cell state (Ct) that acts as a memory conveyor belt, and a
hidden state (ht) that functions as the working memory. The cell state is regulated by three gates - forget gate (ft),
input gate (it), and output gate (ot), each implemented as a sigmoid neural network layer. These gates produce values
between 0 and 1, determining how much information should flow through. The forget gate examines the previous hidden
state (ht-1) and current input (xt) to decide what information to discard from the cell state, while the input gate
determines what new information should be stored.

The LSTM's power lies in its ability to maintain constant error flow through its constant error carousel (CEC),
effectively addressing the vanishing gradient problem. When the forget gate is open (close to 1) and the input gate is
closed (close to 0), the cell state maintains its values without degradation, allowing the network to preserve
information over hundreds or even thousands of time steps. The key equation governing the cell state update is Ct = ft _
Ct-1 + it _ C̃t, where C̃t is the candidate cell state computed using a tanh layer. This multiplicative gating mechanism
allows for precise control over the memory content, making LSTMs particularly effective for tasks requiring long-term
dependencies like machine translation or complex sequence generation.

## Gated Recurrent Unit (GRU)

GRUs represent a streamlined alternative to LSTMs, combining the forget and input gates into a single "update gate" and
merging the cell state and hidden state. The update gate (zt) determines how much of the previous hidden state should be
retained versus how much of the new candidate state should be used. This is complemented by a reset gate (rt) that
controls access to the previous hidden state when computing the new candidate state. The mathematical elegance of GRUs
lies in their ability to achieve similar performance to LSTMs with fewer parameters and simpler computation.

The core innovation of GRUs is their interpolation mechanism for updating the hidden state: ht = (1 - zt) _ ht-1 + zt _
h̃t, where h̃t is the candidate activation. This formulation ensures that the network can effectively learn to keep
existing information or replace it with new content. The reset gate operates earlier in the pipeline, controlling how
much of the previous state contributes to the candidate activation, allowing the network to effectively "forget" when
needed. This architecture makes GRUs particularly efficient at capturing medium-range dependencies while being
computationally less intensive than LSTMs, making them an attractive choice for many practical applications where
computational resources are constrained.

Both architectures represent significant advancements in sequence modeling, with the choice between them often depending
on specific task requirements and computational constraints. I can provide more detailed mathematical formulations or
specific implementation examples if you're interested.

# Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU)

## 1. Introduction to Gated Units

### 1.1 Motivation

- Traditional RNNs suffer from vanishing/exploding gradients
- Need for long-term dependency learning
- Requirement for controlled information flow

## 2. Long Short-Term Memory (LSTM)

### 2.1 Core Concepts

- Introduced by Hochreiter & Schmidhuber (1997)
- Maintains separate cell state and hidden state
- Uses gates to control information flow

### 2.2 LSTM Architecture Components

#### A. Gates

1. Forget Gate (ft):

$$ft = \sigma(W_f[h_{t-1}, x_t] + b_f)$$

- Controls what information to discard from cell state
- Outputs values between 0 (forget) and 1 (keep)

2. Input Gate (it):

$$it = \sigma(W_i[h_{t-1}, x_t] + b_i)$$

- Controls what new information to store in cell state

3. Output Gate (ot):

$$ot = \sigma(W_o[h_{t-1}, x_t] + b_o)$$

- Controls what parts of cell state to output

#### B. Cell State Update

$$c̃_t = \tanh(W_c[h_{t-1}, x_t] + b_c)$$ $$c_t = ft \cdot c_{t-1} + it \cdot c̃_t$$

#### C. Hidden State Update

$$h_t = ot \cdot \tanh(c_t)$$

### 2.3 LSTM Information Flow

1. Forget: Decide what to forget from cell state
2. Store: Decide what new information to store
3. Update: Update cell state
4. Output: Create filtered output

## 3. Gated Recurrent Units (GRU)

### 3.1 Overview

- Introduced by Cho et al. (2014)
- Simplified version of LSTM
- Combines cell state and hidden state
- Uses two gates instead of three

### 3.2 GRU Architecture Components

#### A. Gates

1. Reset Gate (rt):

$$rt = \sigma(W_r[h_{t-1}, x_t] + b_r)$$

- Controls how much past information to forget

2. Update Gate (zt):

$$zt = \sigma(W_z[h_{t-1}, x_t] + b_z)$$

- Controls how much new information to add

#### B. Hidden State Update

$$h̃_t = \tanh(W[h_{t-1}, x_t] + b)$$ $$h_t = (1 - zt) \cdot h_{t-1} + zt \cdot h̃_t$$

## 4. Comparison: LSTM vs GRU

### 4.1 Structural Differences

- LSTM: 3 gates (forget, input, output)
- GRU: 2 gates (reset, update)
- LSTM: Separate cell state and hidden state
- GRU: Combined state

### 4.2 Advantages and Disadvantages

#### LSTM

Pros:

- More expressive
- Better for longer sequences
- More control over memory

Cons:

- More parameters to train
- More computational complexity
- Higher memory requirements

#### GRU

Pros:

- Simpler architecture
- Faster training
- Fewer parameters
- Good for smaller datasets

Cons:

- Might not capture long-term dependencies as well as LSTM
- Less control over memory flow

## 5. Implementation Examples

### 5.1 PyTorch LSTM Implementation

```textmate
class CustomLSTM(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(CustomLSTM, self).__init__()
        self.hidden_size = hidden_size

        # Gates
        self.forget_gate = nn.Linear(input_size + hidden_size, hidden_size)
        self.input_gate = nn.Linear(input_size + hidden_size, hidden_size)
        self.output_gate = nn.Linear(input_size + hidden_size, hidden_size)
        self.cell_gate = nn.Linear(input_size + hidden_size, hidden_size)

    def forward(self, x, hidden, cell):
        combined = torch.cat((x, hidden), dim=1)

        forget = torch.sigmoid(self.forget_gate(combined))
        input = torch.sigmoid(self.input_gate(combined))
        output = torch.sigmoid(self.output_gate(combined))
        cell_candidate = torch.tanh(self.cell_gate(combined))

        cell = forget * cell + input * cell_candidate
        hidden = output * torch.tanh(cell)

        return hidden, cell
```

### 5.2 PyTorch GRU Implementation

```textmate
class CustomGRU(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(CustomGRU, self).__init__()
        self.hidden_size = hidden_size

        # Gates
        self.reset_gate = nn.Linear(input_size + hidden_size, hidden_size)
        self.update_gate = nn.Linear(input_size + hidden_size, hidden_size)
        self.cell_gate = nn.Linear(input_size + hidden_size, hidden_size)

    def forward(self, x, hidden):
        combined = torch.cat((x, hidden), dim=1)

        reset = torch.sigmoid(self.reset_gate(combined))
        update = torch.sigmoid(self.update_gate(combined))

        combined_reset = torch.cat((x, reset * hidden), dim=1)
        cell_candidate = torch.tanh(self.cell_gate(combined_reset))

        hidden = (1 - update) * hidden + update * cell_candidate

        return hidden
```

## 6. Best Practices

### 6.1 When to Use Which

- LSTM: Complex sequences, long-term dependencies
- GRU: Smaller datasets, simpler sequences, limited computational resources

### 6.2 Optimization Tips

- Use gradient clipping
- Initialize forget gate biases to 1.0 (LSTM)
- Apply layer normalization
- Use dropout between layers
- Stack multiple layers for complex tasks

## 7. Advanced Topics

### 7.1 Variants

- Bidirectional LSTM/GRU
- Attention-augmented LSTM/GRU
- Convolutional LSTM
- Peephole connections

### 7.2 Modern Applications

- Machine translation
- Speech recognition
- Time series prediction
- Music generation
- Video analysis

Both LSTM and GRU are powerful solutions to the vanishing gradient problem in RNNs. While LSTM offers more control
through its three-gate architecture, GRU provides a simpler, often equally effective alternative. Choice between them
depends on specific use case requirements.

# RNN vs Feed-Forward Neural Networks: Key Differences

## 1. Architecture and Memory

- **Feed-Forward Networks**

    - Unidirectional flow: Information flows only forward
    - No memory of previous inputs
    - Fixed input size
    - Each input is processed independently

    $$\text{Output} = f(W \cdot \text{Input} + b)$$

- **Recurrent Neural Networks**

    - Cyclic connections: Information can flow in cycles
    - Has memory through hidden states
    - Variable input size
    - Sequential processing with memory:

    $$h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$$ $$y_t = W_{hy}h_t + b_y$$

    Where:

    - $h_t$: Current hidden state
    - $h_{t-1}$: Previous hidden state
    - $x_t$: Current input
    - $W$: Weight matrices
    - $b$: Bias terms

## 2. Application Domains

- **Feed-Forward Networks**

    - Image classification
    - Single-frame prediction
    - Pattern recognition
    - Fixed-size input problems

- **Recurrent Networks**
    - Time series analysis
    - Natural language processing
    - Speech recognition
    - Sequential data prediction
    - Variable-length sequences

## 3. Training Process

- **Feed-Forward Networks**

    - Standard backpropagation
    - Each sample trained independently
    - Simpler gradient computation
    - Faster training generally

- **Recurrent Networks**

    - Backpropagation through time (BPTT)
    - Sequential dependency in training
    - More complex gradient computation:

    $$\frac{\partial L}{\partial W} = \sum_{t=1}^{T} \frac{\partial L}{\partial y_t} \frac{\partial y_t}{\partial h_t} \frac{\partial h_t}{\partial W}$$

    - More prone to vanishing/exploding gradients
    - Generally slower training

## 4. Memory and Context

- **Feed-Forward Networks**

    - No internal memory
    - Each input-output pair is independent
    - Context must be explicitly provided
    - Fixed context window

- **Recurrent Networks**

    - Internal state maintains memory
    - Hidden state equation:

    $$h(t) = f(h(t-1), x(t))$$

    - Can learn long-term dependencies
    - Dynamic context window
    - Variants like LSTM handle long-term dependencies:

    $$f_t = \sigma(W_f[h_{t-1}, x_t] + b_f)$$ $$i_t = \sigma(W_i[h_{t-1}, x_t] + b_i)$$
    $$c̃_t = \tanh(W_c[h_{t-1}, x_t] + b_c)$$ $$c_t = f_t \cdot c_{t-1} + i_t \cdot c̃_t$$
    $$o_t = \sigma(W_o[h_{t-1}, x_t] + b_o)$$ $$h_t = o_t \cdot \tanh(c_t)$$

This fundamental architectural difference makes RNNs more suitable for sequential data processing while feed-forward
networks excel at fixed-input pattern recognition tasks.

### Recurrent Neural Networks

RNNs are based on the same principles as those behind FFNNs, which is why we spent so much time reminding ourselves of
the feedforward and backpropagation steps used in the training phase. There are two main differences between FFNNs and
RNNs. The Recurrent Neural Network uses:

sequences as inputs in the training phase, and memory elements Memory is defined as the output of hidden layer neurons,
which will serve as additional input to the network during the next training step. The basic three layer neural network
with feedback that serve as memory inputs is called the Elman Network. The text describes the fundamental difference
between Feed-Forward Neural Networks (FFNN) and Recurrent Neural Networks (RNN):

1. For FFNN: The output at any time t is a function of the current input and weights: $$\bar{y_t} = F(\bar{x_t}, W)$$
   [Equation 1]

2. For RNN: The output at time t depends not only on current input and weight, but also on previous inputs:
   $$\bar{y_t} = F(\bar{x_t}, \bar{x_{t-1}}, \bar{x_{t-2}}, \cdots, \bar{x_{t-t_0}}, W)$$

The text explains the notation:

- $\bar{x}$ represents the input vector
- $\bar{y}$ represents the output vector
- $\bar{s}$ denotes the state vector

Weight matrices:

- $W_x$ connects inputs to state layer
- $W_y$ connects state layer to output layer
- $W_s$ connects state from previous timestep to current timestep

The text mentions that the model can be "unfolded in time" and this unfolded model is typically used when working with
RNNs. The text explains the Unfolded Model and provides equations:

State calculation: $$\bar{s_t} = \Phi(\bar{x_t}W_x + \bar{s_{t-1}}W_s)$$ [Equation 3]

Output calculation can be either: $$\bar{y_t} = \bar{s_t}W_y$$ or $$\bar{y_t} = \sigma(\bar{s_t}W_y)$$

The text emphasizes that this unfolded model:

- Separates State Vector from Input Vector
- Shows interactions between State Vector and Input Vector
- Makes it easier to understand how State at T-1 and Input Vector at T produce Output at T

The output vector calculation remains similar to FFNNs, using either a linear combination or softmax function of the
inputs with corresponding weight matrix $W_y$.

### RNN Example

In this example, we will illustrate how RNNs can help detect sequences. When detecting a sequence, the system must
remember the previous inputs, so it makes sense to use a recurrent network. If you are unfamiliar with sequence
detection, the idea is to see if a specific pattern of inputs has entered the system. In our example, the pattern will
be the word U,D,A,C,I,T,Y.

# Backpropagation Through Time (BPTT) Explanation

## Core Concept

BPTT is an extension of standard backpropagation designed specifically for RNNs. The key difference is that in RNNs, we
need to account for the temporal dependencies in the network.

## Mathematical Formulation

### 1. Forward Pass

$$\text{For each time step t:}$$ $$h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$$ $$y_t = W_{hy}h_t + b_y$$
$$E_t = (d_t - y_t)^2$$

Where:

- $x_t$ is input at time t
- $h_t$ is hidden state at time t
- $W_{hh}$, $W_{xh}$, $W_{hy}$ are weight matrices
- $b_h$, $b_y$ are bias terms

### 2. Backward Pass

The total loss is sum of losses over all time steps: $$E_{total} = \sum_{t=1}^{T} E_t$$

Gradient calculations:

1. Output layer:

    $$\frac{\partial E}{\partial W_{hy}} = \sum_{t=1}^{T} \frac{\partial E_t}{\partial y_t} \frac{\partial y_t}{\partial W_{hy}}$$

2. Hidden layer:
   $$\frac{\partial E}{\partial W_{hh}} = \sum_{t=1}^{T} \sum_{k=1}^{t} \frac{\partial E_t}{\partial h_t} \frac{\partial h_t}{\partial h_k} \frac{\partial h_k}{\partial W_{hh}}$$

## Key Steps in BPTT

1. **Forward Pass**

    - Run the RNN forward for all time steps
    - Store all hidden states and outputs
    - Calculate error at each time step

2. **Backward Pass**

    - Start from the last time step
    - Compute gradients at each time step
    - Accumulate gradients through time

3. **Weight Updates** $$W_{hh}^{new} = W_{hh} - \text{learning rate} \cdot \frac{\partial E}{\partial W_{hh}}$$
   $$W_{xh}^{new} = W_{xh} - \text{learning rate} \cdot \frac{\partial E}{\partial W_{xh}}$$
   $$W_{hy}^{new} = W_{hy} - \text{learning rate} \cdot \frac{\partial E}{\partial W_{hy}}$$

## Challenges

1. **Vanishing Gradients**

    - When backpropagating through many time steps
    - Gradients become very small
    - Solution: LSTM/GRU cells

2. **Exploding Gradients**

    - Gradients become very large
    - Solution: Gradient clipping
      $$\text{if } \|\nabla\| > \text{threshold}: \nabla = \text{threshold} \cdot \frac{\nabla}{\|\nabla\|}$$

    $
   if gradient > threshold:
      gradient = gradient * (threshold/gradient_magnitude)
   $

## Practical Implementation

```textmate
def bptt(inputs, targets, hidden_state):
    # Forward pass
    states = []
    outputs = []
    for t in range(len(inputs)):
        hidden_state = tanh(dot(W_{hh}, hidden_state) + dot(W_{xh}, inputs[t]) + b_h)
        output = dot(W_{hy}, hidden_state) + b_y
        states.append(hidden_state)
        outputs.append(output)

    # Backward pass
    total_loss = sum((target - output) ** 2 for target, output in zip(targets, outputs))
    gradients = initialize_gradients()

    for t in reversed(range(len(inputs))):
        # Calculate gradients at each timestep
        dWhy += outer(error[t], states[t])
        dWhh += calculate_hidden_gradients(t, states)
        dWxh += calculate_input_gradients(t, inputs)

    return gradients, total_loss
```

## Best Practices

1. Use truncated BPTT for long sequences
2. Implement gradient clipping
3. Use adaptive learning rates
4. Consider bidirectional RNNs for better context
5. Monitor gradient magnitudes during training

## Variants

1. **Truncated BPTT**

    - Limit the number of timesteps to backpropagate through
    - More computationally efficient
    - May miss long-term dependencies

2. **Full BPTT**
    - Backpropagate through entire sequence
    - More accurate but computationally expensive
    - Better for capturing long-term dependencies

# Training RNNs: Backpropagation Through Time (BPTT)

When training RNNs, we use backpropagation with a conceptual change. While the process is similar to FFNNs, we must
consider previous time steps due to the system's memory. This process is called Backpropagation Through Time (BPTT).

## MSE Loss Function

The Mean Squared Error (MSE) loss function is used to explain BPTT:

$$E_t = (d_t - \bar{y_t})^2$$

Where:

- $E_t$ represents the output error at time t
- $d_t$ represents the desired output at time t
- $y_t$ represents the calculated output at time t

## BPTT Process

In BPTT, we train the network at timestep t while considering all previous timesteps. For example, at timestep t=3:

- Loss function: $E_3 = (\bar{d_3} - \bar{y_3})^2$
- Need to adjust three weight matrices: $W_x$, $W_s$, and $W_y$
- Must consider timesteps 3, 2, and 1

To update each weight matrix, we calculate partial derivatives of the Loss Function at time 3 for all weight matrices,
using gradient descent while considering previous timesteps. The unfolded model helps visualize the number of steps (
multiplications) needed in the BPTT process. These multiplications come from the chain rule and are more easily
understood using this model.

The Folded Model at Timestep 3 shows:

- Input vector ($\bar{x_3}$)
- State vector ($\bar{s_3}$)
- Weight matrices ($W_x$, $W_s$, $W_y$)
- Error ($E_3$)

# Unfolding the Model in Time: BPTT Weight Matrix Adjustments

## Weight Matrices to Adjust

- $W_y$ - the weight matrix connecting the state of the output
- $W_s$ - the weight matrix connecting one state to the next state

## Gradient Calculations for $W_y$

The partial derivative of the Loss Function concerning $W_y$ is found by a simple one-step chain rule:

For timestep 3:
$$\frac{\partial E_3}{\partial W_y} = \frac{\partial E_3}{\partial \bar{y_3}} \frac{\partial \bar{y_3}}{\partial W_y}$$

For N steps:
$$\frac{\partial E_N}{\partial W_y} = \frac{\partial E_N}{\partial y_N} \frac{\partial \bar{y_N}}{\partial W_y}$$

## Gradient Calculations for $W_s$

When calculating the partial derivative of the Loss Function for $W_s$, we need to consider all of the states
contributing to the output. In the case of this example, it will be states $\bar{s_3}$ which depends on its predecessor
$\bar{s_2}$ which depends on its predecessor $\bar{s_1}$, the first state.

In BPTT, we will consider every gradient stemming from each state, accumulating all of these contributions.

At timestep t=3, the contribution to the gradient stemming from $\bar{s_3}$, $\bar{s_2}$, and $\bar{s_1}$ is:

$$
\frac{\partial E_3}{\partial W_s} = \frac{\partial E_3}{\partial \bar{y_3}} \frac{\partial \bar{s_3}}{\partial W_s} +
\frac{\partial E_3}{\partial \bar{y_3}} \frac{\partial \bar{s_3}}{\partial \bar{s_2}} \frac{\partial \bar{s_2}}{\partial
W_s} + \frac{\partial E_3}{\partial \bar{y_3}} \frac{\partial \bar{s_3}}{\partial \bar{s_2}} \frac{\partial
\bar{s_2}}{\partial \bar{s_1}} \frac{\partial \bar{s_1}}{\partial W_s}
$$

For N timesteps:

$$
\frac{\partial E_N}{\partial W_s} = \sum_{i=1}^{N} \frac{\partial E_N}{\partial \bar{y_N}} \frac{\partial
\bar{s_i}}{\partial W_s}
$$

## Adjusting/Updating $W_x$

When calculating the partial derivative of the Loss Function concerning $W_x$, we need to consider, again, all the
states contributing to the output. As we saw before, in the case of this example, it will be states $\bar{s_3}$, which
depends on its predecessor $\bar{s_2}$, which depends on its predecessor $\bar{s_1}$, the first state. After considering
the contributions from all three states: $\bar{s_3}$, $\bar{s_2}$ and $\bar{s_1}$, we will accumulate them to find the
final gradient calculation.

# RNN Equations and Weight Matrices

As you have seen, in RNNs the current state depends on the input and the previous states, with an activation function.
Equation showing the current state as a function of input and the previous state:

$$\bar{s_t} = \Phi(\bar{x_t}W_x + \bar{s_{t-1}}W_s)$$

The current output is a simple linear combination of the current state elements with the corresponding weight matrix.
Equation showing the current output:

Without the use of an activation function:

$$\bar{y_t} = \bar{s_t}W_y \text{ (without the use of an activation function)}$$

With the use of an activation function:

$$\bar{y_t} = \sigma(\bar{s_t}W_y) \text{ (with the use of an activation function)}$$

We can represent the recurrent network with the use of a folded model or an unfolded model - We will have three weight
matrices to consider in the case of a single hidden (state) layer. Here we use the following notations:

$W_x$ - represents the weight matrix connecting the inputs to the state layer.

$W_y$ - represents the weight matrix connecting the state to the output.

$W_s$ - represents the weight matrix connecting the state from the previous timestep to the state in the following
timestep.

The gradient calculations for the purpose of adjusting the weight matrices are the following:

Equation 1:
$$\frac{\partial E_N}{\partial W_y} = \frac{\partial E_N}{\partial \bar{y_N}} \frac{\partial \bar{y_N}}{\partial W_y}$$

Equation 2:

$$
\frac{\partial E_N}{\partial W_s} = \sum_{i=1}^N \frac{\partial E_N}{\partial \bar{y_N}} \frac{\partial
\bar{s_i}}{\partial W_s}
$$

Equation 3:

$$
\frac{\partial E_N}{\partial W_x} = \sum_{i=1}^N \frac{\partial E_N}{\partial \bar{y_N}} \frac{\partial
\bar{s_i}}{\partial W_x}
$$

When training RNNs using BPTT, we can choose to use mini-batches, where we update the weights in batches periodically (
as opposed to once every inputs sample). We calculate the gradient for each step but do not update the weights
immediately. Instead, we update the weights once every fixed number of steps. This helps reduce the complexity of the
training process and helps remove noise from the weight updates.

The following is the equation used for Mini-Batch Training Using Gradient Descent:(where $\delta_{ij}$ represents the
gradient calculated once every inputs sample, and M represents the number of gradients we accumulate in the process).

Equation 4: $$\delta_{ij} = \frac{1}{M} \sum_{k=1}^M \delta_{ijk}$$

If we backpropagate more than ~10 timestamps, the gradient will become too small. This phenomenon is known as the
vanishing gradient problem, where the contribution of information decays geometrically over time. Therefore, the network
will effectively discard temporal dependencies that span many time steps. Long Short-Term Memory (LSTM) cells were
designed to solve this problem specifically.

In RNNs, we can also have the opposite problem, called the exploding gradient problem, in which the value of the
gradient grows uncontrollably. A simple solution for the exploding gradient problem is Gradient Clipping. More
information about Gradient Clipping can be found here. You can concentrate on Algorithm 1 which describes the gradient
clipping idea in simplicity.

## Natural Language Processing (NLP)

We have use different techniques for texual data processing such as

1. Normalization
2. Tokenization
3. Stop Word Removal
4. Stemming and Lemmatization

# Natural Language Processing (NLP) Core Concepts

## 1. Normalization

Normalization is the process of transforming text into a single canonical form to ensure consistency. It's typically the
first step in any NLP pipeline.

Key aspects:

- Converting text to lowercase/uppercase
- Removing special characters, punctuation
- Handling contractions (e.g., "don't" → "do not")
- Managing numbers and dates
- Removing extra whitespace

Importance:

- Reduces text redundancy
- Improves consistency
- Makes subsequent processing more effective

## 2. Tokenization

The process of breaking down text into smaller units called tokens. These tokens can be words, characters, or subwords.

Types:

1. **Word Tokenization**

    - Splits text into words
    - Handles punctuation and special cases
    - Considers language-specific rules

2. **Sentence Tokenization**

    - Splits text into sentences
    - Handles abbreviations
    - Manages multiple punctuation marks

3. **Subword Tokenization**
    - Creates tokens smaller than words
    - Useful for handling unknown words
    - Common in modern NLP systems

## 3. Stop Word Removal

Stop words are common words that typically don't carry significant meaning in text analysis.

Characteristics:

- High frequency words (a, an, the, in, at)
- Context-dependent importance
- Language-specific
- May need customization based on use case

Benefits:

- Reduces noise in text analysis
- Decreases processing time
- Focuses on meaningful content
- Saves storage space

## 4. Stemming

A rule-based process of reducing words to their root or base form by removing affixes.

Key Points:

- Fast but aggressive approach
- May produce non-dictionary words
- Language-dependent rules
- Multiple algorithms available (Porter, Lancaster)

Limitations:

- Can produce incorrect stems
- Loses word context
- May reduce accuracy in some applications

## 5. Lemmatization

A more sophisticated approach that converts words to their dictionary base form (lemma) while considering context.

Features:

- Uses morphological analysis
- Produces valid dictionary words
- Considers word context and part of speech
- More accurate than stemming

Considerations:

- Computationally more intensive
- Requires part-of-speech information
- More accurate for meaning preservation

## Practical Considerations

### When to Use Which Technique:

1. **Normalization**

    - Always use as first step
    - Crucial for consistency
    - Adapt to specific needs

2. **Tokenization**

    - Essential for most NLP tasks
    - Choose level based on application
    - Consider language specifics

3. **Stop Word Removal**

    - Use for document classification
    - Skip for sentiment analysis
    - Customize list per application

4. **Stemming**

    - Use when speed is priority
    - Good for search applications
    - Accept some inaccuracy

5. **Lemmatization**
    - Use when accuracy is crucial
    - Better for meaning preservation
    - Accept slower processing

### Common Challenges:

1. **Language Dependency**

    - Different rules for different languages
    - Varying effectiveness of techniques
    - Need for language-specific tools

2. **Context Sensitivity**

    - Word meaning changes with context
    - Impact on accuracy
    - Trade-offs between speed and precision

3. **Processing Speed**
    - Balancing accuracy vs performance
    - Resource constraints
    - Scalability considerations

### Word Embeddings

Word embeddings are a type of distributed representation used in natural language processing (NLP) that allow words to
be represented as dense vectors of real numbers. Each word is mapped to a unique vector, and the vector space is
designed such that words that are semantically similar are located close to each other in the vector space. Word
embeddings are typically learned through unsupervised learning techniques, such as neural network models like Word2Vec(
opens in a new tab) and GloVe, which are trained on large corpora of text. During training, the model learns to predict
the context in which a word appears, such as the surrounding words in a sentence, and uses this information to assign a
vector representation to each word.

Word embeddings have revolutionized the field of natural language processing by providing a way to represent words as
dense vectors that capture semantic and syntactic relationships between words. These representations are particularly
useful for downstream NLP tasks, such as text classification, sentiment analysis, and machine translation, where
traditional techniques may struggle to capture the underlying structure of the text.

For example, in a sentiment analysis task, word embeddings can be used to capture the sentiment of a sentence by summing
the vector representations of the words in the sentence and passing the result through a neural network. In a machine
translation task, word embeddings can be used to map words from one language to another by finding the closest vector
representation in the target language.

#### Models for word embedding in Pytorch

GloVe (Global Vectors): It is a method for generating word embeddings, which are dense vector representations of words
that capture their semantic meaning. The main idea behind GloVe is to use co-occurrence statistics to generate
embeddings that reflect the words' semantic relationships. GloVe embeddings are generated by factorizing a co-occurrence
matrix. The co-occurrence matrix is a square matrix where each row and column represents a word in the vocabulary, and
the cell at position (i, j) represents the number of times word i and word j appear together in a context window. The
context window is a fixed-size window of words surrounding the target word. The factorization of the co-occurrence
matrix results in two smaller matrices: one representing the words, and the other representing the contexts. Each row of
the word matrix represents a word in the vocabulary, and the entries in that row are the weights assigned to each
dimension of the embedding. Similarly, each row of the context matrix represents a context word, and the entries in that
row are the weights assigned to each dimension of the context embedding. The GloVe embeddings are computed by
multiplying the word and context embeddings together and summing them up. This produces a single scalar value that
represents the strength of the relationship between the two words. The resulting scalar is used as the value of the (i,
j) entry in the word-context co-occurrence matrix. In PyTorch, you can use the torchtext package to load pre-trained
GloVe embeddings. The torchtext.vocab.GloVe class allows you to specify the dimensionality of the embeddings (e.g. 50,
100, 200, or 300), and the pre-trained embeddings are downloaded automatically.

FastText: FastText is a popular method for generating word embeddings that extends the concept of word embeddings to
subword units, rather than just whole words. The main idea behind FastText is to represent each word as a bag of
character n-grams, which are contiguous sequences of n characters. FastText embeddings are generated by training a
shallow neural network on the subword units of the corpus. The input to the network is a bag of character n-grams for
each word in the vocabulary, and the output is a dense vector representation of the word. During training, the network
uses a negative sampling objective to learn the embeddings. The objective is to predict whether or not a given word is
in the context of a target word. The model learns to predict the context of a word by computing the dot product between
the target word's embedding and the embedding of each subword unit in the context. FastText embeddings have several
advantages over traditional word embeddings. For example, they can handle out-of-vocabulary words, as long as their
character n-grams are present in the training corpus. They can also capture morphological information and handle
misspellings, since they are based on subword units. In PyTorch, you can use the torchtext package to load pre-trained
FastText embeddings. The torchtext.vocab.FastText class allows you to specify the language and the dimensionality of the
embeddings (e.g. 300).

CharNgram: It refers to a method of generating character-level embeddings for words. The idea behind charNgram is to
represent each word as a sequence of character n-grams (substrings of length n), and then use these n-grams to generate
a fixed-length embedding for the word. For example, if we use CharNGram with n=3, the word "hello" would be represented
as a sequence of 3-character n-grams: "hel", "ell", "llo". We would then use these n-grams to generate a fixed-length
embedding for the word "hello". This embedding would be a concatenation of the embeddings of each n-gram. The benefit of
using charNgram embeddings is that they can capture information about the morphology of words (i.e. how the word is
formed from its constituent parts), which can be useful for certain NLP tasks. However, charNgram embeddings may not
work as well for tasks that rely heavily on semantic meaning, since they do not capture the full meaning of a word. In
PyTorch, you can generate charNgram embeddings using the torchtext package. The torchtext.vocab.CharNGram class allows
you to generate character n-grams for a given text corpus, and the resulting n-grams can be used to generate charNgram
embeddings for individual words.
