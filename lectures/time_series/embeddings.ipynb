{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T23:49:44.110416Z",
     "start_time": "2024-10-26T23:49:44.107793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/aipnd/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)  # Should show path to rnn environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T23:56:29.718237Z",
     "start_time": "2024-10-26T23:56:28.167475Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/chaklader/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/chaklader/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chaklader/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/chaklader/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/chaklader/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T23:51:52.090521Z",
     "start_time": "2024-10-26T23:51:52.088193Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = sample_text.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Embeddings Explained\n",
    "\n",
    "Word embeddings like GloVe are dense vector representations of words where:\n",
    "\n",
    "- Each word is mapped to a fixed-length vector of real numbers\n",
    "- The vectors capture semantic relationships between words\n",
    "- Words with similar meanings have vectors that are close in the vector space\n",
    "- The vector dimensions implicitly represent different semantic aspects of words\n",
    "\n",
    "GloVe (Global Vectors for Word Representation) specifically is trained to capture global word-word co-occurrence statistics from a corpus. The resulting embeddings have interesting properties:\n",
    "\n",
    "- Words that appear in similar contexts have similar embeddings\n",
    "- Vector arithmetic works meaningfully: e.g., vector(\"king\") - vector(\"man\") + vector(\"woman\") ≈ vector(\"queen\")\n",
    "- The distance between word vectors correlates with semantic similarity\n",
    "\n",
    "The file naming convention `glove.6B.50d.txt` indicates:\n",
    "- `6B`: Trained on 6 billion tokens\n",
    "- `50d`: Each word is represented by a 50-dimensional vector\n",
    "\n",
    "These pre-trained embeddings allow you to convert text data into numerical representations that machine learning models can process while preserving semantic relationships between words.\n",
    "\n",
    "##### How These Components Work Together\n",
    "\n",
    "In a typical NLP pipeline:\n",
    "\n",
    "1. The `preprocess_text` function would clean and tokenize raw text\n",
    "2. The tokens would be converted to embeddings using the loaded embedding dictionary\n",
    "3. These embeddings would then be fed into a machine learning model\n",
    "\n",
    "For example, after preprocessing a sentence, you might average the embeddings of all its words to get a sentence representation, or you might create sequences of embeddings to feed into an LSTM or other neural network.\n",
    "\n",
    "This approach is fundamental to many NLP tasks like sentiment analysis, text classification, and question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T23:52:43.597215Z",
     "start_time": "2024-10-26T23:52:41.678124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'hello':\n",
      "[-0.38497   0.80092   0.064106 -0.28355  -0.026759 -0.34532  -0.64253\n",
      " -0.11729  -0.33257   0.55243  -0.087813  0.9035    0.47102   0.56657\n",
      "  0.6985   -0.35229  -0.86542   0.90573   0.03576  -0.071705 -0.12327\n",
      "  0.54923   0.47005   0.35572   1.2611   -0.67581  -0.94983   0.68666\n",
      "  0.3871   -1.3492    0.63512   0.46416  -0.48814   0.83827  -0.9246\n",
      " -0.33722   0.53741  -1.0616   -0.081403 -0.67111   0.30923  -0.3923\n",
      " -0.55002  -0.68827   0.58049  -0.11626   0.013139 -0.57654   0.048833\n",
      "  0.67204 ]\n",
      "Embedding dimension: 50\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess_text(text: str) -> list:\n",
    "    \"\"\"Preprocess text by normalizing, tokenizing, and removing stopwords.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to preprocess\n",
    "        \n",
    "    Returns:\n",
    "        list: List of processed tokens\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation (expanded punctuation set)\n",
    "    text = ''.join(c for c in text if c not in '.,;:!?-\"\\'()[]{}')\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return filtered_tokens  # Changed from 'tokens' to 'filtered_tokens'\n",
    "\n",
    "def load_glove_model(file) -> dict:\n",
    "    \"\"\"Load pre-trained GloVe word embeddings from file.\n",
    "    \n",
    "    Args:\n",
    "        file (str): Path to the GloVe embeddings file\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping words to their embedding vectors\n",
    "    \"\"\"\n",
    "    # Init an empty dict to store \"word\" as key and its \"embedding\" as value\n",
    "    glove_model = {}\n",
    "\n",
    "    with open(file,'r') as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "            glove_model[word] = embedding\n",
    "\n",
    "    return glove_model\n",
    "\n",
    "# Load the pre-trained GloVe embeddings (50-dimensional)\n",
    "embedding_dict = load_glove_model(\"data/glove.6B.50d.txt\")\n",
    "\n",
    "# Let's check embeddings of a word\n",
    "hello_embedding = embedding_dict['hello']\n",
    "print(\"Embedding for 'hello':\")\n",
    "print(hello_embedding)\n",
    "\n",
    "# Let's print the embedding vector dimension\n",
    "print(f\"Embedding dimension: {hello_embedding.shape[0]}\")  # This should be 50 for this specific file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sentence Embedding Example ---\n",
      "Original sentence: 'The quick brown fox jumps over the lazy dog'\n",
      "Preprocessed tokens: ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n",
      "Sentence embedding shape: (50,)\n",
      "First 5 values of sentence embedding: [-0.15505333 -0.18144967 -0.12989    -0.17379167  0.29983667]\n",
      "\n",
      "--- Finding words similar to 'king' ---\n",
      "Most similar words (with similarity scores):\n",
      "prince: 0.8236\n",
      "queen: 0.7839\n",
      "ii: 0.7746\n",
      "emperor: 0.7736\n",
      "son: 0.7667\n",
      "\n",
      "--- Word Vector Arithmetic Example ---\n",
      "king - man + woman ≈ queen (similarity: 0.8610)\n"
     ]
    }
   ],
   "source": [
    "# DEMONSTRATION: Working with word embeddings\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def get_sentence_embedding(text: str, embedding_dict: dict) -> np.ndarray:\n",
    "    \"\"\"Convert a sentence to its embedding representation by averaging word vectors.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to convert to embedding\n",
    "        embedding_dict (dict): Dictionary of word embeddings\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Average embedding vector for the sentence\n",
    "    \"\"\"\n",
    "    # Preprocess the text to get clean tokens\n",
    "    tokens = preprocess_text(text)\n",
    "    \n",
    "    # Filter out words not in our embedding dictionary\n",
    "    valid_tokens = [token for token in tokens if token in embedding_dict]\n",
    "    \n",
    "    if not valid_tokens:\n",
    "        # If no valid tokens, return a zero vector with same dimension as embeddings\n",
    "        embedding_dim = next(iter(embedding_dict.values())).shape[0]\n",
    "        return np.zeros(embedding_dim)\n",
    "    \n",
    "    # Get embeddings for all valid tokens\n",
    "    token_embeddings = [embedding_dict[token] for token in valid_tokens]\n",
    "    \n",
    "    # Average the embeddings to get a sentence-level representation\n",
    "    sentence_embedding = np.mean(token_embeddings, axis=0)\n",
    "    \n",
    "    return sentence_embedding\n",
    "\n",
    "def find_similar_words(word: str, embedding_dict: dict, n: int = 5) -> list:\n",
    "    \"\"\"Find n most similar words to the given word based on cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        word (str): Target word to find similar words for\n",
    "        embedding_dict (dict): Dictionary of word embeddings\n",
    "        n (int): Number of similar words to return\n",
    "        \n",
    "    Returns:\n",
    "        list: List of tuples (word, similarity_score) of the most similar words\n",
    "    \"\"\"\n",
    "    # Check if word exists in the embedding dictionary\n",
    "    if word not in embedding_dict:\n",
    "        return [(\"Word not found in vocabulary\", 0)]\n",
    "    \n",
    "    # Get the embedding for the target word\n",
    "    word_embedding = embedding_dict[word]\n",
    "    \n",
    "    # Function to calculate cosine similarity between two vectors\n",
    "    def cosine_similarity(vec1, vec2):\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm_vec1 = np.linalg.norm(vec1)\n",
    "        norm_vec2 = np.linalg.norm(vec2)\n",
    "        return dot_product / (norm_vec1 * norm_vec2)\n",
    "    \n",
    "    # Calculate similarity with all words in the dictionary\n",
    "    similarities = []\n",
    "    for other_word, other_embedding in embedding_dict.items():\n",
    "        # Skip the same word\n",
    "        if other_word == word:\n",
    "            continue\n",
    "        \n",
    "        # Calculate similarity score\n",
    "        similarity = cosine_similarity(word_embedding, other_embedding)\n",
    "        similarities.append((other_word, similarity))\n",
    "    \n",
    "    # Sort by similarity (highest first) and take top n\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:n]\n",
    "\n",
    "# Demonstrate embedding an entire sentence\n",
    "sample_sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "print(\"\\n--- Sentence Embedding Example ---\")\n",
    "\n",
    "sentence_embedding = get_sentence_embedding(sample_sentence, embedding_dict)\n",
    "print(f\"Original sentence: '{sample_sentence}'\")\n",
    "print(f\"Preprocessed tokens: {preprocess_text(sample_sentence)}\")\n",
    "print(f\"Sentence embedding shape: {sentence_embedding.shape}\")\n",
    "print(f\"First 5 values of sentence embedding: {sentence_embedding[:5]}\")\n",
    "\n",
    "# Demonstrate finding similar words\n",
    "target_word = \"king\"\n",
    "print(f\"\\n--- Finding words similar to '{target_word}' ---\")\n",
    "\n",
    "similar_words = find_similar_words(target_word, embedding_dict)\n",
    "print(\"Most similar words (with similarity scores):\")\n",
    "\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score:.4f}\")\n",
    "\n",
    "# Demonstrate vector arithmetic (king - man + woman ≈ queen)\n",
    "if all(word in embedding_dict for word in [\"king\", \"man\", \"woman\"]):\n",
    "    print(\"\\n--- Word Vector Arithmetic Example ---\")\n",
    "    result_vector = embedding_dict[\"king\"] - embedding_dict[\"man\"] + embedding_dict[\"woman\"]\n",
    "    \n",
    "    # Find the closest word to this result vector\n",
    "    closest_word = None\n",
    "    highest_similarity = -1\n",
    "    \n",
    "    for word, embedding in embedding_dict.items():\n",
    "        # Skip the words used in the equation\n",
    "        if word in [\"king\", \"man\", \"woman\"]:\n",
    "            continue\n",
    "            \n",
    "        # Calculate cosine similarity\n",
    "        similarity = np.dot(result_vector, embedding) / (np.linalg.norm(result_vector) * np.linalg.norm(embedding))\n",
    "        \n",
    "        if similarity > highest_similarity:\n",
    "            highest_similarity = similarity\n",
    "            closest_word = word\n",
    "    \n",
    "    print(f\"king - man + woman ≈ {closest_word} (similarity: {highest_similarity:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T23:56:35.078776Z",
     "start_time": "2024-10-26T23:56:35.014913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fox\n",
      "[ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
      " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
      "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
      "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
      "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
      " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
      " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
      "  1.5064  ]\n"
     ]
    }
   ],
   "source": [
    "# Now let's create the embedding matrix for sample_text\n",
    "sample_tokens = preprocess_text(sample_text)\n",
    "sample_embedding_matrix = []\n",
    "\n",
    "for sample_token in sample_tokens:\n",
    "    sample_embedding_matrix.append(embedding_dict[sample_token])\n",
    "\n",
    "# we should have as many embedding vectors (rows of embedding matrix) as there are sample tokens\n",
    "assert len(sample_embedding_matrix) == len(sample_tokens)\n",
    "\n",
    "# lets print a token and its embedding\n",
    "print(sample_tokens[2])\n",
    "print(sample_embedding_matrix[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aipnd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
