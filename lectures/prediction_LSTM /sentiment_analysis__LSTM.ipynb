{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis using LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T16:22:34.293026Z",
     "start_time": "2023-04-12T16:22:34.108507Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Sentences (textual data) and their sentiment labels (1 for positive, 0 for negative)\n",
    "sentences = [\"i love this movie\", \"this film is amazing\", \"i didn't like it\", \"it was terrible\"]\n",
    "sentiment = [1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T16:22:34.293614Z",
     "start_time": "2023-04-12T16:22:34.119452Z"
    }
   },
   "outputs": [],
   "source": [
    "# Simple vocabulary to represent words as indices\n",
    "vocab = {\"<PAD>\": 0, \"i\": 1, \"love\": 2, \"this\": 3, \"movie\": 4, \"film\": 5, \"is\": 6, \"amazing\": 7, \"didn't\": 8, \"like\": 9, \"it\": 10, \"was\": 11, \"terrible\": 12}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a simple vocabulary to represent words as indices. This allows us to convert words in our sentences to numbers, which can be fed as input to our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize, encode and pad sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T16:22:34.295464Z",
     "start_time": "2023-04-12T16:22:34.136556Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_sentences = [[vocab[word] for word in sentence.split()] for sentence in sentences]\n",
    "max_length = max([len(sentence) for sentence in encoded_sentences])\n",
    "padded_sentences = [sentence + [vocab[\"<PAD>\"]] * (max_length - len(sentence)) for sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenize and encode the sentences using the vocabulary created earlier. We also pad the sentences with the `<PAD>` token to make them all the same length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert data to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T16:22:34.297533Z",
     "start_time": "2023-04-12T16:22:34.154982Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = torch.LongTensor(padded_sentences)\n",
    "labels = torch.FloatTensor(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the input data and labels to PyTorch tensors. Inputs are converted to LongTensors, while labels are converted to FloatTensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T16:22:34.297819Z",
     "start_time": "2023-04-12T16:22:34.168771Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based binary sentiment classifier for text sequences.\n",
    "    \n",
    "    This model implements a standard sequence-to-one LSTM architecture for sentiment\n",
    "    analysis tasks. It processes variable-length text sequences and outputs sentiment\n",
    "    predictions by using the final LSTM hidden state as a sentence-level representation.\n",
    "    \n",
    "    Architecture components:\n",
    "    1. Embedding layer: Converts discrete token indices to dense vector representations\n",
    "    2. LSTM layer: Processes sequences to capture temporal dependencies and context\n",
    "    3. Linear layer: Maps final hidden state to sentiment classification logits\n",
    "    \n",
    "    The model assumes binary sentiment classification (positive/negative) but can be\n",
    "    extended to multi-class sentiment analysis by adjusting output_dim.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of vocabulary (number of unique tokens)\n",
    "        embedding_dim (int): Dimensionality of word embeddings\n",
    "        hidden_dim (int): Number of hidden units in LSTM layer\n",
    "        output_dim (int): Number of output classes (typically 1 for binary, 2+ for multi-class)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize embedding layer for token-to-vector conversion.\n",
    "        Creates learnable lookup table mapping vocabulary indices to dense embeddings.\n",
    "        \"\"\"\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize LSTM layer for sequential processing.\n",
    "        Default configuration (batch_first=False) expects input shape (seq_len, batch_size, embedding_dim).\n",
    "        Processes embedded sequences to learn temporal patterns and context dependencies.\n",
    "        \"\"\"\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize linear classification layer.\n",
    "        Maps final LSTM hidden state to sentiment prediction logits.\n",
    "        For binary sentiment: output_dim=1, for multi-class: output_dim=num_classes.\n",
    "        \"\"\"\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for sentiment prediction.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input token indices of shape (seq_len, batch_size)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Sentiment logits of shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Convert token indices to dense embeddings.\n",
    "        Shape transformation: (seq_len, batch_size) → (seq_len, batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        \"\"\"\n",
    "        Process embedded sequence through LSTM to extract temporal features.\n",
    "        Returns all hidden states and final (hidden, cell) state tuple.\n",
    "        hidden shape: (1, batch_size, hidden_dim) for single-layer LSTM.\n",
    "        \"\"\"\n",
    "        output, (hidden, _) = self.lstm(embedded)\n",
    "        \n",
    "        \"\"\"\n",
    "        Generate sentiment predictions from final hidden state.\n",
    "        squeeze(0) removes layer dimension: (1, batch_size, hidden_dim) → (batch_size, hidden_dim)\n",
    "        Linear layer maps to sentiment logits: (batch_size, hidden_dim) → (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        logits = self.fc(hidden.squeeze(0))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a simple LSTM model class that inherits from `nn.Module`. The model consists of an embedding layer, an LSTM layer, and a fully connected (linear) layer. The forward method takes an input tensor `x`, passes it through the embedding layer, the LSTM layer, and finally the fully connected layer to produce the output logits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate model and define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T16:22:34.322481Z",
     "start_time": "2023-04-12T16:22:34.174251Z"
    }
   },
   "outputs": [],
   "source": [
    "model = SimpleLSTM(len(vocab), embedding_dim=10, hidden_dim=20, output_dim=1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the LSTM model with the vocabulary size, embedding dimensions, hidden dimensions, and output dimensions. We also define the binary cross-entropy with logits loss (`BCEWithLogitsLoss`) and the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentiment_data(sentences, sentiment, vocab):\n",
    "    \"\"\"\n",
    "    Transform raw text and sentiment labels into LSTM-compatible tensor format.\n",
    "\n",
    "    This preprocessing function handles the complete pipeline from raw text to model-ready\n",
    "    tensors, including tokenization, vocabulary encoding, sequence padding, and tensor\n",
    "    conversion. It ensures all sequences have uniform length and proper formatting for\n",
    "    batch processing in PyTorch LSTM models.\n",
    "\n",
    "    The function assumes text is already cleaned and tokenized (split by spaces), and\n",
    "    that all vocabulary words are present in the vocab dictionary. Unknown words will\n",
    "    cause KeyError exceptions.\n",
    "\n",
    "    Args:\n",
    "        sentences (list of str): Raw text sentences for sentiment analysis.\n",
    "            Example: [\"this movie is great\", \"terrible acting\"]\n",
    "        sentiment (list of int): Corresponding sentiment labels for each sentence.\n",
    "            Typically 0 for negative, 1 for positive sentiment.\n",
    "            Must have same length as sentences list.\n",
    "        vocab (dict): Word-to-index mapping for vocabulary.\n",
    "            Example: {\"this\": 0, \"movie\": 1, \"is\": 2, \"great\": 3, \"<PAD>\": 4}\n",
    "            Must include \"<PAD>\" token for sequence padding.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - inputs (torch.LongTensor): Encoded and padded sequences with shape \n",
    "              (max_seq_len, batch_size). Transposed for LSTM's expected input format.\n",
    "            - labels (torch.FloatTensor): Sentiment labels with shape (batch_size,).\n",
    "              Float tensor suitable for binary classification with BCELoss or similar.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If any word in sentences is not found in vocab dictionary.\n",
    "        ValueError: If sentences and sentiment lists have different lengths.\n",
    "\n",
    "    Example:\n",
    "        >>> sentences = [\"good movie\", \"bad film\"]\n",
    "        >>> sentiment = [1, 0]  \n",
    "        >>> vocab = {\"good\": 0, \"movie\": 1, \"bad\": 2, \"film\": 3, \"<PAD>\": 4}\n",
    "        >>> inputs, labels = prepare_sentiment_data(sentences, sentiment, vocab)\n",
    "        >>> print(inputs.shape)  # torch.Size([2, 2]) -> (max_seq_len, batch_size)\n",
    "        >>> print(labels.shape)  # torch.Size([2])\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Tokenize sentences and convert words to vocabulary indices.\n",
    "    \n",
    "    Process:\n",
    "    1. Split each sentence on whitespace to get individual tokens\n",
    "    2. Map each token to its vocabulary index using vocab dictionary\n",
    "    3. Results in list of lists where each inner list contains token indices\n",
    "    \n",
    "    Example: \"good movie\" with vocab {\"good\": 0, \"movie\": 1} becomes [0, 1]\n",
    "    \n",
    "    Note: This will raise KeyError if any word is not in vocabulary.\n",
    "    Consider using vocab.get(word, vocab[\"<UNK>\"]) for unknown word handling.\n",
    "    \"\"\"\n",
    "    encoded_sentences = [[vocab[word] for word in sentence.split()] for sentence in sentences]\n",
    "\n",
    "    \"\"\"\n",
    "    Determine maximum sequence length for padding standardization.\n",
    "    All sequences will be padded to this length to enable batch processing.\n",
    "    Longer sequences require more memory but preserve all textual information.\n",
    "    \"\"\"\n",
    "    max_length = max(len(sentence) for sentence in encoded_sentences)\n",
    "\n",
    "    \"\"\"\n",
    "    Pad all sequences to uniform length using <PAD> token indices.\n",
    "    \n",
    "    Padding process:\n",
    "    1. Calculate required padding: max_length - current_sequence_length\n",
    "    2. Append <PAD> token indices to reach max_length\n",
    "    3. Ensures all sequences have identical length for tensor conversion\n",
    "    \n",
    "    Example: If max_length=4 and sequence=[0, 1], result=[0, 1, 4, 4] (assuming <PAD>=4)\n",
    "    \"\"\"\n",
    "    padded_sentences = [sentence + [vocab[\"<PAD>\"]] * (max_length - len(sentence)) \n",
    "                       for sentence in encoded_sentences]\n",
    "\n",
    "    \"\"\"\n",
    "    Convert preprocessed data to PyTorch tensors with appropriate data types.\n",
    "    \n",
    "    LongTensor for inputs: Required for embedding layer which expects integer indices\n",
    "    FloatTensor for labels: Compatible with most loss functions (BCELoss, MSELoss, etc.)\n",
    "    \n",
    "    Initial tensor shapes:\n",
    "    - inputs: (batch_size, max_seq_len)\n",
    "    - labels: (batch_size,)\n",
    "    \"\"\"\n",
    "    inputs = torch.LongTensor(padded_sentences)\n",
    "    labels = torch.FloatTensor(sentiment)\n",
    "\n",
    "    \"\"\"\n",
    "    Transpose input tensor to match LSTM's expected input format.\n",
    "    \n",
    "    LSTM with batch_first=False expects: (seq_len, batch_size, input_size)\n",
    "    Transpose changes: (batch_size, seq_len) → (seq_len, batch_size)\n",
    "    \n",
    "    This ensures compatibility with the SimpleLSTM model's forward pass.\n",
    "    \"\"\"\n",
    "    inputs = inputs.t()\n",
    "    \n",
    "    return inputs, labels\n",
    "\n",
    "# Example usage:\n",
    "inputs, labels = prepare_sentiment_data(sentences, sentiment, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T16:22:35.864714Z",
     "start_time": "2023-04-12T16:22:34.207066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss: 0.0011476636864244938\n",
      "Epoch: 200, Loss: 0.0009985864162445068\n",
      "Epoch: 300, Loss: 0.0008769434643909335\n",
      "Epoch: 400, Loss: 0.0007760371081531048\n",
      "Epoch: 500, Loss: 0.0006912948447279632\n",
      "Epoch: 600, Loss: 0.0006192974396981299\n",
      "Epoch: 700, Loss: 0.0005576762487180531\n",
      "Epoch: 800, Loss: 0.0005042588454671204\n",
      "Epoch: 900, Loss: 0.0004578128573484719\n",
      "Epoch: 1000, Loss: 0.0004170317552052438\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \"\"\"\n",
    "    We train the model for 1000 epochs. In each epoch, we:\n",
    "    \n",
    "        - Reset the gradients by calling optimizer.zero_grad()\n",
    "        - Get the model's predictions for the input sentences by calling model(inputs.t()).squeeze(1)\n",
    "        - Calculate the loss between the predictions and the true labels using the criterion defined earlier\n",
    "        - Perform backpropagation by calling loss.backward()\n",
    "        - Update the model's parameters by calling optimizer.step()\n",
    "        - We also print the loss every 100 epochs for monitoring the training progress.\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(inputs.t()).squeeze(1)\n",
    "    loss = criterion(predictions, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch: {epoch + 1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T16:22:35.900667Z",
     "start_time": "2023-04-12T16:22:35.865321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions: tensor([0.9949, 0.0283])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \"\"\"\n",
    "    We use the model to make predictions on new sentences. In this example, we:\n",
    "    \n",
    "    - Disable gradient calculation by using torch.no_grad()\n",
    "    - Define a list of test sentences\n",
    "    - Tokenize and encode the test sentences\n",
    "    - Pad the sequences to match the maximum sequence length\n",
    "    - Convert the padded sequences to PyTorch tensors\n",
    "    - Get the model's predictions for the test sentences\n",
    "    - Apply the sigmoid function to convert the predictions to probabilities\n",
    "    - Print the test predictions\n",
    "    \"\"\"\n",
    "    test_sentences = [\"i love this film\", \"it was terrible\"]\n",
    "    encoded_test_sentences = [[vocab[word] for word in sentence.split()] for sentence in test_sentences]\n",
    "    padded_test_sentences = [sentence + [vocab[\"<PAD>\"]] * (max_length - len(sentence)) for sentence in encoded_test_sentences]\n",
    "    test_inputs = torch.LongTensor(padded_test_sentences)\n",
    "    test_predictions = torch.sigmoid(model(test_inputs.t()).squeeze(1))\n",
    "    \n",
    "    print(\"Test predictions:\", test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the model on two new sentences. First, we tokenize, encode, and pad the test sentences in the same way as we did for the training sentences. We then convert the test sentences to PyTorch tensors and pass them through the model. We apply the sigmoid function to the output logits to obtain the final predictions, which represent the probability of each sentence being positive.\n",
    "\n",
    "The resulting `test_predictions` tensor contains the model's sentiment predictions for the given test sentences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
